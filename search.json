[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis and Monitoring",
    "section": "",
    "text": "Welcome\nThe “Data Analysis and Monitoring” module provides an in-depth overview of the methodological skills required for hands-on research and development in any applied data-related or data-heavy project the Master’s level. Students will refine their methodological expertise by examining the different typical phases of data analysis and modelling, starting from data capture and preprocessing the data, through exploratory analysis and predictive modelling, to visualization and communication in the end. They will also acquire the methodological foundations that will underpin the subsequent modules in the MSc CEM programme. The module provides both general methodological skills that cut across disciplines (e.g., scientific theory, computer-aided data processing, and statistics) and specialised knowledge in the context of circular economy.\nThe materials required for the R exercises are available here, with demo files, exercises and solutions.\nThis website was last updated on 2025-03-25 15:26:58.970776.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preparation.html",
    "href": "preparation.html",
    "title": "Preparation Course",
    "section": "",
    "text": "Install R\nIn this course we will be using R and RStudio. We ask you to install and/or update these programs before the start of the course, so that we do not loose time once the course starts. In this chapter, we cover the course requirements and some tips on how you should change your RStudio settings.\nIf you haven’t installed R yet, do so now by getting the newest version from CRAN.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparation Course</span>"
    ]
  },
  {
    "objectID": "preparation.html#install-rstudio",
    "href": "preparation.html#install-rstudio",
    "title": "Preparation Course",
    "section": "Install RStudio",
    "text": "Install RStudio\nRStudio is the IDE (integrated development environment) we use in our course to interact with R. Download and install it on your computer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparation Course</span>"
    ]
  },
  {
    "objectID": "preparation.html#configure-rstudio",
    "href": "preparation.html#configure-rstudio",
    "title": "Preparation Course",
    "section": "Configure RStudio",
    "text": "Configure RStudio\nNow we will set some RStudio Global options. But first, close all instances of RStudio and restart it (!!!). Then go to Tools &gt; Global options.\n\nR General\n\nDeactivate the option “Restore .RData into workspace at startup”\nSet “Save workspace to .RData on exit” to “Never”\n\nCode\n\nActivate the option “Use native pipe operator, |&gt; (requires R 4.1+)”\n\nR Markdown\n\nDeactivate the option “Show output inline for all R Markdown documents”\n\n\nClick on “Ok” to apply the change and close the options menu.\n\nFolder structure for this course\nBy this point, you probably have created a folder for this course somewhere on your computer. In our example, we assume this folder is located here: C:/Users/yourname/semester2/Module_DAMO (mentally replace this with your actual path). Before we dive into the exercises, take a minute to think about how you are going to structure your files in this folder. This course will take place over several weeks, and in each week you will receive or produce various files. We recommend creating a separate folder for each week, and one folder for the case studies, like so:\nCourse Folder (C:\\\\Users\\\\yourname\\\\semester2\\\\Module_DAMO)\n ¦--week_1                                                \n ¦--week_2                                                \n ¦--week_3                                                \n |--...                                                \n °--case_studies \nFor the R-exercises we recommend that you create a new RStudio Project each week in subdirectory of the appropriate week. For example, this week your folder structure could look like this:\nFolder Week 1 (C:\\\\Users\\\\yourname\\\\semester2\\\\Module_DAMO\\\\week_1)\n ¦--slides.pdf                                                  \n ¦--my_notes.docx                                               \n ¦--seminar_screenshot.jpg                                      \n °---damo-week1-rexercise                                             \n     ¦--damo-week1-rexercise.Rproj                                   \n     ¦--test.csv                                      \n     °--my_solution.qmd   \nNote:\n\nthe RStudio Project is located in a subfolder of C:/Users/yourname/semester1/Module_DAMO/week_1 and named damo-week1-rexercise.\ndamo-week1-rexercise is the project’s directory name and the project name\nwe realize that damo and the week number is redundant, there is a reason1 for this\nthis means each week is a fresh start (which has pros and cons)\n\n\n\nCreate an RStudio project for the first week\nCreate a new RStudio Project (File &gt; New Project &gt; New Directory &gt; New Project).\n\nClick on “Browse” and switch to your equivalent of the folder C:/Users/yourname/semester1/Module_DAMO/week_1 (the project we are about to initiate will be be created in a subdirectory of this folder). Click on “open” to confirm the selection\nIn the field “Directory name”, type damo-week1-rexercise. This will be the name of your RStudio project and it’s parent directory.\nClick on “Create Project”\n\nYou are all set! You can start working on the tasks of exercise 1.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparation Course</span>"
    ]
  },
  {
    "objectID": "preparation.html#footnotes",
    "href": "preparation.html#footnotes",
    "title": "Preparation Course",
    "section": "",
    "text": "You will see the project names of all your RStudio Projects listed in RStudio. Having the week number in the project name keeps you from getting confused on which project you are working on.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparation Course</span>"
    ]
  },
  {
    "objectID": "PrePro.html",
    "href": "PrePro.html",
    "title": "Pre-Processing",
    "section": "",
    "text": "Data Science 2.0 equips students with the essential knowledge and practical skills needed to prepare and enhance their self-collected or sourced data for analysis (preprocessing). This unit focuses on fundamental data processing skills while tackling common challenges in the processing of environmental science data, all through a practical, ‘hands-on’ approach with R exercises. Students will learn to articulate the characteristics of their data sets using the appropriate technical terminology. They will also learn to interpret metadata and critically assess its implications for their own analysis projects. The lesson emphasises critical concepts such as scale levels, data types, time data, and type conversions.\nThis lesson focuses on the central skills required for preprocessing structured data, a fundamental aspect of environmental science research. It covers combining datasets (joins) and transforming them (“reshape”, “split-apply-combine”). Given that data seldom presents itself in a format ready for statistical analysis or information visualisation, students will master the key concepts and R tools required for these often intricate preprocessing tasks, enabling them to execute them effectively.",
    "crumbs": [
      "Pre-Processing"
    ]
  },
  {
    "objectID": "prepro/Prepro0_Vorbereitung.html",
    "href": "prepro/Prepro0_Vorbereitung.html",
    "title": "Preparation",
    "section": "",
    "text": "For Prepro 1 - 3, we will need the following packages: dplyr, ggplot2, lubridate, readr and tidyr. We recommend installing them before the first lesson. Individual packages cann be installed as follows:\n\ninstall.packages(c(\"dplyr\", \"ggplot2\", \"lubridate\", \"readr\", \"tidyr\"))\n\nYou can download the datasets for the exercises from Moodle:\n\nPrePro1\nPrePro2\nPrePro3",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preparation</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Demo.html",
    "href": "prepro/Prepro1_Demo.html",
    "title": "Prepro 1: Demo",
    "section": "",
    "text": "Data types\nThis demo’s source code can also be downloaded as an R Script (right click → Save Target As..)",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prepro 1: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Demo.html#data-types",
    "href": "prepro/Prepro1_Demo.html#data-types",
    "title": "Prepro 1: Demo",
    "section": "",
    "text": "Doubles\nThere are two different numeric data types in R:\n\ndouble: floating-point number (e.g. 10.3, 7.3)\ninteger (e.g. 10, 7)\n\nA double / floating point number is assigned to a variable as follows:\n\nx &lt;- 10.3\n\nx\n\n[1] 10.3\n\nclass(x)\n\n[1] \"numeric\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nEither &lt;- or = can be used. However, the latter is also easily confused with ==.\n\ny = 7.3\ny\n\n[1] 7.3\n\n\n\n\n\n\nInteger\nA number is only stored as an integer if it is explicitly defined as one (using as.integer() or L).\n\nd &lt;- 8L\n\nclass(d)\n\n[1] \"integer\"\n\n\n\n\nBoolean\n\nsunny &lt;- FALSE\ndry &lt;- TRUE\n\nsunny & dry\n\n[1] FALSE\n\n\n\ne &lt;- 3\nf &lt;- 6\n\ne &gt; f\n\n[1] FALSE\n\n\n\n\nCharacter\nCharacter strings contain text.\n\nfname &lt;- \"Andrea\"\nlname &lt;- \"Muster\"\nclass(fname)\n\n[1] \"character\"\n\n\nConnecting / concatenating character strings\n\npaste(fname, lname)\n\n[1] \"Andrea Muster\"\n\npaste(fname, lname, sep = \",\")\n\n[1] \"Andrea,Muster\"\n\n\n\n\n\nDate / time\nIn most parts of the world, we use the Gregorian Calendar to communicate a point in time. In this system, we track time as years, months, days, hours, minutes and seconds after a specific event (Anno Domini, “in the year of the Lord”).\nR, just as all other computer systems, do not store date / time information using years, months days etc. Instead, R stores the number of seconds after a given date (January 1st, 1970, which is also called unix epoch). This information is stored using the class POSIXct, which also helps us convert this number of seconds into more human readable information. On {r} now_pretty, {r} n_secs_pretty have passed since the unix epoch, so to store this timestamp, R stores the number {r} n_secs_pretty.\n\n# We may have a timestamp saved as a character string\ntoday_txt &lt;- \"2024-02-01 13:45:00\"\n\n# as.POSIXct converts the string to POSIXct:\ntoday_posixct &lt;- as.POSIXct(today_txt)\n\n# When printing a posixct date to the console, it is human readable\ntoday_posixct\n\n[1] \"2024-02-01 13:45:00 CET\"\n\n# To see the internally stored value (# of seconds), convert it to numeric:\nas.numeric(today_posixct)\n\n[1] 1706791500\n\n\nIf the character string is delivered in the above format (year-month-day hour:minute:second), as.POSIXct knows how to caluate the number of seconds since unix epoch. However, if the format is different, we have to tell R how to read our timestamp. This requires a special syntax, which is described in ?strptime.\n\ndate_txt &lt;- \"01.10.2017 15:15\"\n\n# converts character to POSIXct:\nas.POSIXct(date_txt)\n\nError in as.POSIXlt.character(x, tz, ...): character string is not in a standard unambiguous format\n\ndate_posix &lt;- as.POSIXct(date_txt, format = \"%d.%m.%Y %H:%M\")\n\ndate_posix\n\n[1] \"2017-10-01 15:15:00 CEST\"\n\n\nTheoretically, strftime can also be used to extract specific components from a date. However, the functions from lubridate are much simpler and we recommend you use these. Note how strftime always returns strings while lubridate returns more useful datatypes such as integers or factors.\n\n1strftime(date_posix, format = \"%m\")\n2strftime(date_posix, format = \"%b\")\n3strftime(date_posix, format = \"%B\")\n\n\n1\n\nextracts the month as a number\n\n2\n\nextracts the month by name (abbreviated)\n\n3\n\nextracts the month by name (full)\n\n\n\n\n\nlibrary(\"lubridate\")\n\n1month(date_posix)\n2month(date_posix, label = TRUE, abbr = TRUE)\n3month(date_posix, label = TRUE, abbr = FALSE)\n\n\n1\n\nextracts the month as a number\n\n2\n\nextracts the month by name (abbreviated)\n\n3\n\nextracts the month by name (full)\n\n\n\n\n\n\n\n\n\n\nTime is hard\n\n\n\nHandling date / time is tricky. We recommend the following practices to make life easier:\n\nAlways store time as POSIXct, not as text.\nAlways store time together with its according date, never separately.\nIf you must extract time (e.g. to analyse daily patterns), store it as decimal time (e.g. store 15:45 as 15.75) in a numeric data type.\nTry to be explicit about which timezone your data originates from\nIf your observation period is affected by switching to or from daylight saving time, think about converting time to UTC\nUse lubridate rather than strftime()",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prepro 1: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Demo.html#data-structures",
    "href": "prepro/Prepro1_Demo.html#data-structures",
    "title": "Prepro 1: Demo",
    "section": "Data structures",
    "text": "Data structures\n\nVectors\nUsing c(), a set of values of the same data type can be assigned to a variable (as a vector).\n\nvec &lt;- c(10, 20, 33, 42, 54, 66, 77)\nvec\n\n[1] 10 20 33 42 54 66 77\n\n# to extract the 5th element\nvec[5]\n\n[1] 54\n\n# to extract elements 2 to 4\nvec[2:4]\n\n[1] 20 33 42\n\n\n\n\nLists\nA list is a collection of objects that do not need to be the same data type.\n\nmylist &lt;- list(\"q\", TRUE, 3.14)\n\nThe individual elements in a list can also have assigned names.\n\nmylist2 &lt;- list(fav_letter = \"q\", fav_boolean = TRUE, fav_number = 3.14)\n\nmylist2\n\n$fav_letter\n[1] \"q\"\n\n$fav_boolean\n[1] TRUE\n\n$fav_number\n[1] 3.14\n\n\n\n\nData frames\nIf each entry in a list is the same length, this list can also be represented as a table, which is called a dataframe in R.\n\n# note how the names become column names\nas.data.frame(mylist2)\n\n  fav_letter fav_boolean fav_number\n1          q        TRUE       3.14\n\n\nThe data.frame function allows a table to be created without first having to create a list.\n\ndf &lt;- data.frame(\n  City = c(\"Zurich\", \"Geneva\", \"Basel\", \"Bern\", \"Lausanne\"),\n  Arrival = c(\n    \"1.1.2017 10:10\", \"5.1.2017 14:45\",\n    \"8.1.2017 13:15\", \"17.1.2017 18:30\", \"22.1.2017 21:05\"\n  )\n)\n\nstr(df)\n\n'data.frame':   5 obs. of  2 variables:\n $ City   : chr  \"Zurich\" \"Geneva\" \"Basel\" \"Bern\" ...\n $ Arrival: chr  \"1.1.2017 10:10\" \"5.1.2017 14:45\" \"8.1.2017 13:15\" \"17.1.2017 18:30\" ...\n\n\nThe $ symbol can be used to query data:\n\ndf$City\n\n[1] \"Zurich\"   \"Geneva\"   \"Basel\"    \"Bern\"     \"Lausanne\"\n\n\nNew columns can be added and existing ones can be changed:\n\ndf$Residents &lt;- c(400000, 200000, 175000, 14000, 130000)\n\n\n\n# A tibble: 5 × 3\n  City     Arrival         Residents\n  &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt;\n1 Zurich   1.1.2017 10:10     400000\n2 Geneva   5.1.2017 14:45     200000\n3 Basel    8.1.2017 13:15     175000\n4 Bern     17.1.2017 18:30     14000\n5 Lausanne 22.1.2017 21:05    130000\n\n\nWe need to convert the Arrival time to a time format (POSIXct).\n\n# first, test the output of the \"as.POSIXct\"-function\nas.POSIXct(df$Arrival, format = \"%d.%m.%Y %H:%M\")\n\n[1] \"2017-01-01 10:10:00 CET\" \"2017-01-05 14:45:00 CET\"\n[3] \"2017-01-08 13:15:00 CET\" \"2017-01-17 18:30:00 CET\"\n[5] \"2017-01-22 21:05:00 CET\"\n\n# if it works, we can save the output to a new column\ndf$Arrival_ct &lt;- as.POSIXct(df$Arrival, format = \"%d.%m.%Y %H:%M\")\n\n\n# We *could* overwrite the old column, but this is a destructive operation!\n\nThese columns can now help to create convenience variables. E.g., the arrival time can be derived from the Arrival column.\n\ndf$Arrival_day &lt;- wday(df$Arrival_ct, label = TRUE, week_start = 1)\n\ndf$Arrival_day\n\n[1] Sun Thu Sun Tue Sun\nLevels: Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat &lt; Sun",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prepro 1: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html",
    "href": "prepro/Prepro1_Uebung.html",
    "title": "PrePro 1: Exercise",
    "section": "",
    "text": "Working with RStudio “Project”\nWe recommend using “Projects” within RStudio. RStudio then creates a folder for each project in which the project file is stored (file extension .rproj). If Rscripts are loaded or generated within the project, they are then also stored in the project folder. You can find out more about RStudio Projects here.\nThere are several benefits to using Projects. You can:",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PrePro 1: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#working-with-rstudio-project",
    "href": "prepro/Prepro1_Uebung.html#working-with-rstudio-project",
    "title": "PrePro 1: Exercise",
    "section": "",
    "text": "specify the Working Directory without using an explicit path (setwd()). This is useful because the path can change (when collaborating with other users, or executing the script at a later date)\nautomatically cache open scripts and restore open scripts in the next session\nset different project-specific options\nuse version control systems (e.g., git)",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PrePro 1: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-1",
    "href": "prepro/Prepro1_Uebung.html#task-1",
    "title": "PrePro 1: Exercise",
    "section": "Task 1",
    "text": "Task 1\nCreate a data.frame with the following data. Tipp: Create a vector for each column first.\n\n\nSample Solution\ndf &lt;- data.frame(\n  Species = c(\"Fox\", \"Bear\", \"Rabbit\", \"Moose\"),\n  Number = c(2, 5, 1, 3),\n  Weight = c(4.4, 40.3, 1.1, 120),\n  Sex = c(\"m\", \"f\", \"m\", \"m\"),\n  Description = c(\"Reddish\", \"Brown, large\", \"Small, with long ears\", \"Long legs, shovel antlers\")\n)\n\n\n\n\n\n\n\nSpecies\nNumber\nWeight\nSex\nDescription\n\n\n\n\nFox\n2\n4.4\nm\nReddish\n\n\nBear\n5\n40.3\nf\nBrown, large\n\n\nRabbit\n1\n1.1\nm\nSmall, with long ears\n\n\nMoose\n3\n120.0\nm\nLong legs, shovel antlers",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PrePro 1: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-2",
    "href": "prepro/Prepro1_Uebung.html#task-2",
    "title": "PrePro 1: Exercise",
    "section": "Task 2",
    "text": "Task 2\nWhat types of data were automatically accepted in the last task? Check this using str(), see whether they make sense and convert where necessary.\n\n\nSample Solution\nstr(df)\n## 'data.frame':    4 obs. of  5 variables:\n##  $ Species    : chr  \"Fox\" \"Bear\" \"Rabbit\" \"Moose\"\n##  $ Number     : num  2 5 1 3\n##  $ Weight     : num  4.4 40.3 1.1 120\n##  $ Sex        : chr  \"m\" \"f\" \"m\" \"m\"\n##  $ Description: chr  \"Reddish\" \"Brown, large\" \"Small, with long ears\" \"Long legs, shovel antlers\"\ntypeof(df$Number)\n## [1] \"double\"\n# Number was interpreted as `double`, but it is actually an `integer`.\n\ndf$Number &lt;- as.integer(df$Number)\n\n# We know sex only has two options:\ndf$Sex &lt;- factor(df$Sex, levels = c(\"m\",\"f\"))",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PrePro 1: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#input-libraries-packages",
    "href": "prepro/Prepro1_Uebung.html#input-libraries-packages",
    "title": "PrePro 1: Exercise",
    "section": "Input: Libraries / packages",
    "text": "Input: Libraries / packages\nLibraries (aka packages) are are “extensions” to the basic R functionality. R packages have become indispensable to using R. The vast majority of packages are hosted on CRAN and can be easily installed using install.packages(\"packagename\"). This installation is done once. To use the library, you must load it into the current R session using library(packagename).\nE.g. To import data, we recommend using the readr package1. Install the package using the command install.package(\"readr\"). To use the package, load it into the current R session using library(\"readr\").",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PrePro 1: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-3",
    "href": "prepro/Prepro1_Uebung.html#task-3",
    "title": "PrePro 1: Exercise",
    "section": "Task 3",
    "text": "Task 3\nOn Moodle, you will find a folder called Datasets. Download the file and move it in your project folder. Import the weather.csv file. If you use the RStudio GUI for this, save the import command in your R-Script. Please use a relative path (i.e., not a path starting with C:\\, or similar).)\n\n\nSample Solution\n\nlibrary(\"readr\")\n\n\nweather &lt;- read_delim(\"datasets/prepro/weather.csv\", \",\")\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000010100\n-2.6\n\n\nABO\n2000010101\n-2.5\n\n\nABO\n2000010102\n-3.1\n\n\nABO\n2000010103\n-2.4\n\n\nABO\n2000010104\n-2.5\n\n\nABO\n2000010105\n-3.0\n\n\nABO\n2000010106\n-3.7\n\n\nABO\n2000010107\n-4.4\n\n\nABO\n2000010108\n-4.1\n\n\nABO\n2000010109\n-4.1",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PrePro 1: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-4",
    "href": "prepro/Prepro1_Uebung.html#task-4",
    "title": "PrePro 1: Exercise",
    "section": "Task 4",
    "text": "Task 4\nHave a look at your dataset in the console. Have the data been interpreted correctly?\n\n\nSample Solution\n# The 'time' column was interpreted as 'integer'. However, it is \n# obviously a time indication.",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PrePro 1: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-5",
    "href": "prepro/Prepro1_Uebung.html#task-5",
    "title": "PrePro 1: Exercise",
    "section": "Task 5",
    "text": "Task 5\nThe time column is a date/time with a format of YYYYMMDDHH. In order for R to recognise the data in this column as date/time, it must be in the correct format (POSIXct). Therefore, we must tell R what the current format is. Use as.POSIXct() to read the column into R, remembering to specify both format and tz.\n\n\n\n\n\n\nTip\n\n\n\n\nIf no time zone is set, as.POSIXct() sets a default (based on sys.timezone()). In our case, however, these are values in UTC (see metadata.csv)\nas.POSIXct requires a character input: If you receive the error message 'origin' must be supplied (or similar), you have probably tried to input a numeric into the function with.\n\n\n\n\n\nSample Solution\nweather$time &lt;- as.POSIXct(as.character(weather$time), format = \"%Y%m%d%H\", tz = \"UTC\")\n\n\n\n\n\nThe new table should look like this\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\n\n\nABO\n2000-01-01 01:00:00\n-2.5\n\n\nABO\n2000-01-01 02:00:00\n-3.1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\n\n\nABO\n2000-01-01 04:00:00\n-2.5\n\n\nABO\n2000-01-01 05:00:00\n-3.0\n\n\nABO\n2000-01-01 06:00:00\n-3.7\n\n\nABO\n2000-01-01 07:00:00\n-4.4\n\n\nABO\n2000-01-01 08:00:00\n-4.1\n\n\nABO\n2000-01-01 09:00:00\n-4.1",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PrePro 1: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-6",
    "href": "prepro/Prepro1_Uebung.html#task-6",
    "title": "PrePro 1: Exercise",
    "section": "Task 6",
    "text": "Task 6\nCreate two new columns for day of week (Monday, Tuesday, etc) and calendar week. Use the newly created POSIXct column and a suitable function from lubridate.\n\n\nSample Solution\n\nlibrary(\"lubridate\")\n\n\nweather$weekday &lt;- wday(weather$time, label = T)\nweather$week &lt;- week(weather$time)\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\nweekday\nweek\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSat\n1\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSat\n1\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSat\n1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSat\n1\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSat\n1\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSat\n1\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSat\n1\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSat\n1\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSat\n1\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSat\n1",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PrePro 1: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#footnotes",
    "href": "prepro/Prepro1_Uebung.html#footnotes",
    "title": "PrePro 1: Exercise",
    "section": "",
    "text": "Advantages of read_delim over read.csv: https://stackoverflow.com/a/60374974/4139249↩︎",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PrePro 1: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Demo.html",
    "href": "prepro/Prepro2_Demo.html",
    "title": "Prepro 2: Demo",
    "section": "",
    "text": "Piping\nThe code for this demo can also be downloaded as an R Script (right click → Save Target As..)\nWe want to extract the temperature data from a character string (temperature), and then convert the Kelvin value into Celsius according to the following formula, before finally calculating the mean of all the values:\n\\[°C = K - 273.15\\]\n# these temperature are in Kelvin:\ntemperature &lt;- c(\"310\",\"322\",\"348\")\n\ntemperature\n## [1] \"310\" \"322\" \"348\"\nTranslated into R-code, this results in the following operation:\nsubtract &lt;- function(x,y){x-y} # helperfunction to subtract y from x\n\noutput &lt;- mean(subtract(as.integer(temperature), 273.15))\n#                                 \\___1_____/\n#                       \\_____________2_______/\n#              \\______________________3________________/\n#         \\___________________________4_________________/\n\n# 1. Take temperature\n# 2. Convert \"character\" → \"integer\"\n# 4. Subtract 273.15\n# 5. Calculate the mean\nThe whole operation is easier to read if it is written down sequentially:\ntmp &lt;- as.integer(temperature)   # 2\ntmp &lt;- subtract(tmp, 273.15)     # 3\noutput &lt;- mean(tmp)              # 4\noutput\n## [1] 53.51667\nThe fact that the intermediate results must always be saved and retrieved again in the subsequent operation makes this somewhat cumbersome. This is where “piping” comes into play: It makes the output of one function the first parameter of the subsequent function.\ntemperature |&gt;        # 1\n  as.integer() |&gt;     # 2\n  subtract(273.15) |&gt; # 3\n  mean()              # 4\n## [1] 53.51667",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#piping",
    "href": "prepro/Prepro2_Demo.html#piping",
    "title": "Prepro 2: Demo",
    "section": "",
    "text": "Important\n\n\n\n\nthe |&gt; pipe operator was first introduced in R 4.1\nIn addition to the base R pipe operator, there is also a very similar1 pipe operator, %&gt;%, in the magrittr package.\nThe Ctrl +Shift+M keyboard shortcut in RStudio inserts a pipe operator.\nBy checking the Use native pipe operator setting in RStudio Settings Tools → Global Options → Code, you can control which pipe operator, |&gt; or %&gt;%, is inserted with the above key combination.\nWe recommend using the base-R pipe operator |&gt;",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#joins",
    "href": "prepro/Prepro2_Demo.html#joins",
    "title": "Prepro 2: Demo",
    "section": "Joins",
    "text": "Joins\n\nstudents &lt;- data.frame(\n  Matriculation_No = c(100002, 100003, 200003),\n  Student = c(\"Patrick\", \"Manuela\", \"Eva\"),\n  ZIP = c(8006, 8001, 8820)\n)\n\nstudents\n##   Matriculation_No Student  ZIP\n## 1           100002 Patrick 8006\n## 2           100003 Manuela 8001\n## 3           200003     Eva 8820\n\nlocalities &lt;- data.frame(\n  ZIP = c(8003, 8006, 8810, 8820),\n  LocalityName = c(\"Zurich\", \"Zurich\", \"Horgen\", \"Wadenswil\")\n)\n\nlocalities\n##    ZIP LocalityName\n## 1 8003       Zurich\n## 2 8006       Zurich\n## 3 8810       Horgen\n## 4 8820    Wadenswil\n\n\n# Load library\nlibrary(\"dplyr\")\n\ninner_join(students, localities, by = \"ZIP\")\n##   Matriculation_No Student  ZIP LocalityName\n## 1           100002 Patrick 8006       Zurich\n## 2           200003     Eva 8820    Wadenswil\n\nleft_join(students, localities, by = \"ZIP\")\n##   Matriculation_No Student  ZIP LocalityName\n## 1           100002 Patrick 8006       Zurich\n## 2           100003 Manuela 8001         &lt;NA&gt;\n## 3           200003     Eva 8820    Wadenswil\n\nright_join(students, localities, by = \"ZIP\")\n##   Matriculation_No Student  ZIP LocalityName\n## 1           100002 Patrick 8006       Zurich\n## 2           200003     Eva 8820    Wadenswil\n## 3               NA    &lt;NA&gt; 8003       Zurich\n## 4               NA    &lt;NA&gt; 8810       Horgen\n\nfull_join(students, localities, by = \"ZIP\")\n##   Matriculation_No Student  ZIP LocalityName\n## 1           100002 Patrick 8006       Zurich\n## 2           100003 Manuela 8001         &lt;NA&gt;\n## 3           200003     Eva 8820    Wadenswil\n## 4               NA    &lt;NA&gt; 8003       Zurich\n## 5               NA    &lt;NA&gt; 8810       Horgen\n\n\nstudents &lt;- data.frame(\n  Matriculation_No = c(100002, 100003, 200003),\n  Student = c(\"Patrick\", \"Manuela\", \"Pascal\"),\n  Residence = c(8006, 8001, 8006)\n)\n\nleft_join(students, localities, by = c(\"Residence\" = \"ZIP\"))\n##   Matriculation_No Student Residence LocalityName\n## 1           100002 Patrick      8006       Zurich\n## 2           100003 Manuela      8001         &lt;NA&gt;\n## 3           200003  Pascal      8006       Zurich",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#footnotes",
    "href": "prepro/Prepro2_Demo.html#footnotes",
    "title": "Prepro 2: Demo",
    "section": "",
    "text": "see https://stackoverflow.com/q/67633022/4139249↩︎",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Prepro 2: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html",
    "href": "prepro/Prepro2_Uebung_A.html",
    "title": "Prepro 2: Exercise A",
    "section": "",
    "text": "Task 1\nRead the weather data from last week weather.csv (source MeteoSchweiz) into R. Make sure that the columns are formatted correctly (stn as a factor, time as POSIXct, tre200h0 as a numeric).\nSample Solution\n\nlibrary(\"readr\")\n\nweather &lt;- read_delim(\"datasets/prepro/weather.csv\", \",\")\nweather$stn &lt;- as.factor(weather$stn)\nweather$time &lt;- as.POSIXct(as.character(weather$time), format = \"%Y%m%d%H\", tz = \"UTC\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Exercise A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-2",
    "href": "prepro/Prepro2_Uebung_A.html#task-2",
    "title": "Prepro 2: Exercise A",
    "section": "Task 2",
    "text": "Task 2\nRead in the metadata.csv dataset as a csv.\n\n\n\n\n\n\nTip\n\n\n\nIf umlauts and special characters are not displayed correctly (e.g. the è in Gèneve), this probably has something to do with the character encoding. The file is currently encoded in UTF-8. If special characters are not correctly displayed, R has not recognised this encoding and it must be specified in the import function. How this is done depends on the import function used:\n\nPackage functions: readr: locale = locale(encoding = \"UTF-8\")\nBase-R functions: fileEncoding = \"UTF-8\"\n\nNote: If you have a file where you do not know how a file is encoded, the following instructions for Windows, Mac and Linux will help.\n\n\n\n\nSample Solution\nmetadata &lt;- read_delim(\"datasets/prepro/metadata.csv\", \";\", locale = locale(encoding = \"UTF-8\"))",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Exercise A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-3",
    "href": "prepro/Prepro2_Uebung_A.html#task-3",
    "title": "Prepro 2: Exercise A",
    "section": "Task 3",
    "text": "Task 3\nNow we want to enrich the weather data set with information from metadata. However, we are only interested in the station abbreviation, the name, the x/y coordinates and the sea level. Select these columns.\n\n\nSample Solution\nmetadata &lt;- metadata[, c(\"stn\", \"Name\", \"x\", \"y\", \"Meereshoehe\")]",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Exercise A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-4",
    "href": "prepro/Prepro2_Uebung_A.html#task-4",
    "title": "Prepro 2: Exercise A",
    "section": "Task 4",
    "text": "Task 4\nNow the metadata can be connected to the weather data set. Which join should we use to do this? And, which attribute can we join?\nUse the join options in dplyr (help via? dplyr::join) to connect the weather data set and the metadata.\n\n\nSample Solution\n\nlibrary(\"dplyr\")\n\n\nweather &lt;- left_join(weather, metadata, by = \"stn\")\n\n# Join type: Left-Join on 'weather', as we are only interested in the stations in the 'weather' dataset.\n# Attribute: \"stn\"",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Exercise A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-5",
    "href": "prepro/Prepro2_Uebung_A.html#task-5",
    "title": "Prepro 2: Exercise A",
    "section": "Task 5",
    "text": "Task 5\nCreate a new month column (from time). To do this, use the lubridate::month() function.\n\n\nSample Solution\n\nlibrary(\"lubridate\")\n\n\nweather$month &lt;- month(weather$time)",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Exercise A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-6",
    "href": "prepro/Prepro2_Uebung_A.html#task-6",
    "title": "Prepro 2: Exercise A",
    "section": "Task 6",
    "text": "Task 6\nUse the month column to calculate the average temperature per month.\n\n\nSample Solution\nmean(weather$tre200h0[weather$month == 1])\n## [1] -1.963239\nmean(weather$tre200h0[weather$month == 2])\n## [1] 0.3552632\nmean(weather$tre200h0[weather$month == 3])\n## [1] 2.965054\n\n# etc. for all 12 months",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prepro 2: Exercise A</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html",
    "href": "prepro/Prepro2_Uebung_B.html",
    "title": "Prepro 2: Exercise B",
    "section": "",
    "text": "Task 1\nYou have data from three sensors (sensor1.csv, sensor2.csv, sensor3.csv). Read in the data sets using the library readr.\nSample Solution\n\nlibrary(\"readr\")\n\n\nsensor1 &lt;- read_delim(\"datasets/prepro/sensor1.csv\", \";\")\nsensor2 &lt;- read_delim(\"datasets/prepro/sensor2.csv\", \";\")\nsensor3 &lt;- read_delim(\"datasets/prepro/sensor3.csv\", \";\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prepro 2: Exercise B</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#task-2",
    "href": "prepro/Prepro2_Uebung_B.html#task-2",
    "title": "Prepro 2: Exercise B",
    "section": "Task 2",
    "text": "Task 2\nFrom the 3 data frames, create a single data frame that looks like the one shown below. Use two joins from dplyr to connect 3 data.frames. Then tidy up the column names (how can we do that?).\n\n\nSample Solution\n\nlibrary(\"dplyr\")\n\n\nsensor1_2 &lt;- full_join(sensor1, sensor2, \"Datetime\")\n\nsensor1_2 &lt;- rename(sensor1_2, sensor1 = Temp.x, sensor2 = Temp.y)\n\nsensor_all &lt;- full_join(sensor1_2, sensor3, by = \"Datetime\")\n\nsensor_all &lt;- rename(sensor_all, sensor3 = Temp)\n\n\n\n\n\n\n\nDatetime\nsensor1\nsensor2\nsensor3\n\n\n\n\n16102017_1800\n23.5\n13.5\n26.5\n\n\n17102017_1800\n25.4\n24.4\n24.4\n\n\n18102017_1800\n12.4\n22.4\n13.4\n\n\n19102017_1800\n5.4\n12.4\n7.4\n\n\n23102017_1800\n23.5\n13.5\nNA\n\n\n24102017_1800\n21.3\n11.3\nNA",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prepro 2: Exercise B</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#task-3",
    "href": "prepro/Prepro2_Uebung_B.html#task-3",
    "title": "Prepro 2: Exercise B",
    "section": "Task 3",
    "text": "Task 3\nImport the sensor_fail.csv file into R.\n\n\nSample Solution\nsensor_fail &lt;- read_delim(\"datasets/prepro/sensor_fail.csv\", delim = \";\")\n\n\nsensor_fail.csv has a variable SensorStatus: 1 means the sensor is measuring, 0 means the sensor is not measuring. If sensor status = 0, the Temp = 0 value is incorrect. It should be NA (not available). Correct the dataset accordingly.\n\n\n\n\n\nSensor\nTemp\nHum_%\nDatetime\nSensorStatus\n\n\n\n\nSen102\n0.6\n98\n16102017_1800\n1\n\n\nSen102\n0.3\n96\n17102017_1800\n1\n\n\nSen102\n0.0\n87\n18102017_1800\n1\n\n\nSen102\n0.0\n86\n19102017_1800\n0\n\n\nSen102\n0.0\n98\n23102017_1800\n0\n\n\nSen102\n0.0\n98\n24102017_1800\n0\n\n\nSen102\n0.0\n96\n25102017_1800\n1\n\n\nSen103\n-0.3\n87\n26102017_1800\n1\n\n\nSen103\n-0.7\n98\n27102017_1800\n1\n\n\nSen103\n-1.2\n98\n28102017_1800\n1\n\n\n\n\n\n\n\nSample Solution\n# with base-R:\nsensor_fail$Temp_correct[sensor_fail$SensorStatus == 0] &lt;- NA\nsensor_fail$Temp_correct[sensor_fail$SensorStatus != 0] &lt;- sensor_fail$Temp # Warning message can be ignored.\n\n# the same with dplyr:\nsensor_fail &lt;- sensor_fail |&gt;\n  mutate(Temp_correct = ifelse(SensorStatus == 0, NA, Temp))",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prepro 2: Exercise B</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#task-4",
    "href": "prepro/Prepro2_Uebung_B.html#task-4",
    "title": "Prepro 2: Exercise B",
    "section": "Task 4",
    "text": "Task 4\nWhy does it matter if 0 or NA is recorded? Calculate the mean of the temperature / humidity after you have corrected the dataset.\n\n\nSample Solution\n# Mean values of the incorrect sensor data: 0 flows into the calculation\n# and distorts the mean\nmean(sensor_fail$Temp)\n## [1] -0.13\n\n# Mean values of the corrected sensor data: with na.rm = TRUE,\n# NA values are removed from the calculation.\nmean(sensor_fail$Temp_correct, na.rm = TRUE)\n## [1] -0.1857143",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Prepro 2: Exercise B</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html",
    "href": "prepro/Prepro3_Demo.html",
    "title": "Prepro 3: Demo",
    "section": "",
    "text": "Load data\nIn this demo, we will introduce other tools from the Tidyverse and explain them using examples. The tidyverse tools make dealing with data much easier and have now become a must have when dealing with data in R.\nWe cannot show you all the possibilities of tidyverse. Therefore, we will focus on the most important components and also introduce additional functionalities that we often use but may not yet be known to you. If you want to delve deeper into the topic, you should read Wickham, Çetinkaya-Rundel, and Grolemund (2023), which is available online: https://r4ds.hadley.nz/\nLets load the weather data (source MeteoSchweiz) from the last exercise.\nlibrary(\"readr\")\n\nweather &lt;- read_delim(\"datasets/prepro/weather.csv\", \",\")\n\n\nweather$stn &lt;- as.factor(weather$stn)\n\n# only overwrite \"time\" if you are sure it worked!\nweather$time2 &lt;- as.POSIXct(as.character(weather$time), format = \"%Y%m%d%H\", tz = \"UTC\")\nTranslated into English, the above operation is as follows:\nThe translation from R → English looks different because we read the operation in a concatenated form in English (operation 1 → 2 → 3) while the computer reads it as a nested operation 3(2(1)). To make R closer to English, you can use the |&gt; operator (see Wickham, Çetinkaya-Rundel, and Grolemund 2023, chap. 4.3).\nweather$time2 &lt;- weather$time |&gt; \n  as.character() |&gt; \n  as.POSIXct(format = \"%Y%m%d%H\", tz = \"UTC\")\nOnce we are sure the conversion worked, we can overwrite time and remove time2\nweather$time &lt;- weather$time2\n\nweather$time2 &lt;- NULL\nNow, let’s look at another example.",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#load-data",
    "href": "prepro/Prepro3_Demo.html#load-data",
    "title": "Prepro 3: Demo",
    "section": "",
    "text": "Take the column time from the weather dataset\nConvert it to character\nConvert it to POSIXct using a specified format and timezone",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#calculate-values",
    "href": "prepro/Prepro3_Demo.html#calculate-values",
    "title": "Prepro 3: Demo",
    "section": "Calculate values",
    "text": "Calculate values\nWe would like to calculate the average of all measured temperature values. To do this, we could use the following command:\n\nmean(weather$tre200h0, na.rm = TRUE)\n## [1] 6.324744\n\nThe option na.rm = TRUE means that NA values should be excluded from the calculation.\nVarious values can be calculated using the same approach (e.g. the maximum (max()), minimum (min()), median (median()) and much more).\nThis approach only works well if we want to calculate values across all observations for a variable (column). As soon as we want to group the observations, it becomes difficult. For example, if we want to calculate the average temperature per month.",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#convenience-variables",
    "href": "prepro/Prepro3_Demo.html#convenience-variables",
    "title": "Prepro 3: Demo",
    "section": "Convenience Variables",
    "text": "Convenience Variables\nTo solve this task, the month must first be extracted (the month is the convenience variable). For this we need the lubridate::month() function.\nNow the month convenience variable can be created. Without using dplyr, a new column can be added as follows:\n\nlibrary(\"lubridate\")\n\nweather$month &lt;- month(weather$time)\n\nWith dplyr the same command looks like this:\n\nlibrary(\"dplyr\")\n\n\nweather &lt;- mutate(weather, month = month(time))\n\nThe main advantage of dplyr is not yet apparent at this point. However, this will become clear later.",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#calculate-values-from-groups",
    "href": "prepro/Prepro3_Demo.html#calculate-values-from-groups",
    "title": "Prepro 3: Demo",
    "section": "Calculate values from groups",
    "text": "Calculate values from groups\nTo calculate the average value per month with base R, you can first create a subset with [] and calculate the average value as follows:\n\nmean(weather$tre200h0[weather$month == 1], na.rm = TRUE)\n## [1] -1.963239\n\nWe have to repeat this every month, which of course is very cumbersome. That is why we use the dplyr package. This, allows us to complete the task (calculate temperature means per month) as follows:\n\nsummarise(group_by(weather, month), temp_average = mean(tre200h0, na.rm = TRUE))\n## # A tibble: 12 × 2\n##    month temp_average\n##    &lt;dbl&gt;        &lt;dbl&gt;\n##  1     1       -1.96 \n##  2     2        0.355\n##  3     3        2.97 \n##  4     4        4.20 \n##  5     5       11.0  \n##  6     6       12.4  \n##  7     7       13.0  \n##  8     8       15.0  \n##  9     9        9.49 \n## 10    10        8.79 \n## 11    11        1.21 \n## 12    12       -0.898",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#concatenate-vs.-nest",
    "href": "prepro/Prepro3_Demo.html#concatenate-vs.-nest",
    "title": "Prepro 3: Demo",
    "section": "Concatenate vs. Nest",
    "text": "Concatenate vs. Nest\nAgain, translated into English, the above operation is as follows:\n\nTake the weather dataset\nForm groups per year (group_by(weather, year))\nCalculate the mean temperature (mean(tre200h0))\n\nThe translation from R → English looks different because we read the operation in a concatenated form in English (operation 1 → 2 → 3) while the computer reads it as a nested operation 3(2(1)). To make R closer to English, you can use the |&gt; operator (see Wickham, Çetinkaya-Rundel, and Grolemund 2023, chap. 4.3).\n\n# 1 take the dataset \"weather\"\n# 2 form groups per month\n# 3 calculate the average temperature\n\nsummarise(group_by(weather, month), temp_average = mean(tre200h0))\n#                  \\__1__/\n#         \\___________2__________/\n# \\___________________3________________________________________/\n\n# becomes:\n\nweather |&gt;                                 # 1\n  group_by(month) |&gt;                       # 2\n  summarise(temp_average = mean(tre200h0)) # 3\n\nThis concatenation by means of |&gt; (called pipe) makes the code a lot easier to write and read, and we will use it in the following exercises. Pipe is provided as part of the magrittr package and installed with dplyr. There are several online tutorials about dplyr (see Wickham, Çetinkaya-Rundel, and Grolemund 2023, Part “Transform” or this youtube tutorial)\nTherefore, we will not explain all of these tools in full detail. Instead we will just focus on the important differences for two main functions in dpylr: mutate() and summarise().\n\nsummarise() summarises a data set. The number of observations (rows) is reduced to the number of groups (e.g., one summarised observation (row) per year). In addition, the number of variables (columns) is reduced to those specified in the “summarise” function (e.g., temp_mean).\nmutate adds additional variables (columns) to a data.frame (see example below).\n\n\n# Maximum and minimum temperature per calendar week\nweather_summary &lt;- weather |&gt;               # 1) take the dataset \"weather\"\n  filter(month == 1) |&gt;                     # 2) filter for the month of January\n  mutate(day = day(time)) |&gt;                # 3) create a new column \"day\"\n  group_by(day) |&gt;                          # 4) Use the new column to form groups\n  summarise(\n    temp_max = max(tre200h0, na.rm = TRUE), # 5) Calculate the maximum\n    temp_min = min(tre200h0, na.rm = TRUE)  # 6) Calculate the minimum\n  )\n\nweather_summary\n## # A tibble: 31 × 3\n##      day temp_max temp_min\n##    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n##  1     1      5.8     -4.4\n##  2     2      2.8     -4.3\n##  3     3      4.2     -3.1\n##  4     4      4.7     -2.8\n##  5     5     11.4     -0.6\n##  6     6      6.7     -1.6\n##  7     7      2.9     -2.8\n##  8     8      0.2     -3.6\n##  9     9      2.1     -8.8\n## 10    10      1.6     -2.4\n## # ℹ 21 more rows\n\n\n\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd Edition. O’Reilly. https://r4ds.hadley.nz/.",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prepro 3: Demo</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html",
    "href": "prepro/Prepro3_Uebung.html",
    "title": "Prepro 3: Exercise",
    "section": "",
    "text": "Task 1\nYou have a dataset, sensors_long.csv, with temperature values from three different sensors. Import it as a csv into R (as sensors_long).\nReformat the datetime column to POSIXct. Use the as.POSIXct function (read it in using?strftime()) to determine the specific format (the template).\nSample Solution\nlibrary(\"readr\")\n\nsensors_long &lt;- read_delim(\"datasets/prepro/sensors_long.csv\", \",\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Prepro 3: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-2",
    "href": "prepro/Prepro3_Uebung.html#task-2",
    "title": "Prepro 3: Exercise",
    "section": "Task 2",
    "text": "Task 2\nGroup sensors_long according to the column name where the sensor information is contained, using the function group_by, and calculate the average temperature for each sensor (summarise). Note: Both functions are part of the dplyr package.\nThe output will look like this:\n\n\nSample Solution\nlibrary(\"dplyr\")\n\nsensors_long |&gt;\n  group_by(name) |&gt;\n  summarise(temp_mean = mean(value, na.rm = TRUE))\n## # A tibble: 3 × 2\n##   name    temp_mean\n##   &lt;chr&gt;       &lt;dbl&gt;\n## 1 sensor1      14.7\n## 2 sensor2      12.0\n## 3 sensor3      14.4",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Prepro 3: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-3",
    "href": "prepro/Prepro3_Uebung.html#task-3",
    "title": "Prepro 3: Exercise",
    "section": "Task 3",
    "text": "Task 3\nCreate a new convenience variable, month, for sensors_long (Tip: use the month function from lubridate). Now group by month and sensor and calculate the mean temperature.\n\n\nSample Solution\nlibrary(\"lubridate\")\n\nsensors_long |&gt;\n  mutate(month = month(Datetime)) |&gt;\n  group_by(month, name) |&gt;\n  summarise(temp_mean = mean(value, na.rm = TRUE))\n## # A tibble: 6 × 3\n## # Groups:   month [2]\n##   month name    temp_mean\n##   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n## 1    10 sensor1     14.7 \n## 2    10 sensor2     12.7 \n## 3    10 sensor3     14.4 \n## 4    11 sensor1    NaN   \n## 5    11 sensor2      8.87\n## 6    11 sensor3    NaN",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Prepro 3: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-4",
    "href": "prepro/Prepro3_Uebung.html#task-4",
    "title": "Prepro 3: Exercise",
    "section": "Task 4",
    "text": "Task 4\nNow import the weather.csv dataset (source MeteoSwiss) with the correct column types (time as POSIXct, tre200h0 as double). You can download the file from moodle if you havent done so yet.\n\n\nSample Solution\nweather &lt;- read_delim(\"datasets/prepro/weather.csv\")\n\n\nweather$time2 &lt;- weather$time |&gt; \n  as.character() |&gt; \n  as.POSIXct(format = \"%Y%m%d%H\", tz = \"UTC\")\n  \n\nweather$time &lt;- weather$time2\nweather$time2 &lt;- NULL",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Prepro 3: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-5",
    "href": "prepro/Prepro3_Uebung.html#task-5",
    "title": "Prepro 3: Exercise",
    "section": "Task 5",
    "text": "Task 5\nNow create a convenience variable for the calendar week for each measurement (lubridate::isoweek). Then calculate the average temperature value for each calendar week.\n\n\nSample Solution\nweather_summary &lt;- weather |&gt;\n  mutate(week = isoweek(time)) |&gt;\n  group_by(week) |&gt;\n  summarise(\n    temp_mean = mean(tre200h0, na.rm = TRUE)\n  )\n\n\nNext, you can visualise the result using the following function:\nplot(weather_summary$week, weather_summary$temp_mean, type = \"l\")",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Prepro 3: Exercise</span>"
    ]
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-6",
    "href": "prepro/Prepro3_Uebung.html#task-6",
    "title": "Prepro 3: Exercise",
    "section": "Task 6",
    "text": "Task 6\nIn the previous task, we calculated the average temperature per calendar week over all years (2000 and 2001). However, if we want to compare the years with each other, we have to create the year as an additional convenience variable and group it accordingly. Try this with the weather data and then visualise the output.\n\n\nSample Solution\nweather_summary2 &lt;- weather |&gt;\n  mutate(\n    week = week(time),\n    year = year(time)\n    ) |&gt;\n  group_by(year, week) |&gt;\n  summarise(\n    temp_mean = mean(tre200h0, na.rm = TRUE)\n  )\n\n\n\n\nSample Solution\nplot(weather_summary2$week, weather_summary2$temp_mean, type = \"l\")\n\n\n\n\n\n\n\n\nFigure 9.1: Base plot does not like long tables and makes a continuous line out of the two years",
    "crumbs": [
      "Pre-Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Prepro 3: Exercise</span>"
    ]
  },
  {
    "objectID": "InfoVis.html",
    "href": "InfoVis.html",
    "title": "Information Visualisation",
    "section": "",
    "text": "InfoVis 1\nConventional inferential statistics are typically employed to confirm hypotheses. These hypotheses are derived from established theories and are then tested through experiments to determine whether they should be accepted or rejected. Conversely, Exploratory Data Analysis (EDA) takes an antagonistic approach, by first seeking out patterns and relationships within the data, which can subsequently inform the development of hypotheses for testing. This module presents the traditional five-step process of Exploratory Data Analysis as established by Tukey in 1980, culminating in a transition to its contemporary application through Visual Analytics.",
    "crumbs": [
      "Information Visualisation"
    ]
  },
  {
    "objectID": "InfoVis.html#infovis-2",
    "href": "InfoVis.html#infovis-2",
    "title": "Information Visualisation",
    "section": "InfoVis 2",
    "text": "InfoVis 2\nInformation visualisation stands out as a flexible, powerful, and efficient tool for exploratory data analysis. Beyond the well-known scatter plots and histograms, there are innovative visualisation techniques like parallel coordinate plots, tree maps, and chord diagrams that provide unique perspectives for analysing increasingly large and complex datasets. In this lesson, students get to know a number of information visualisation types, learn to design them in a targeted manner and to create them themselves.",
    "crumbs": [
      "Information Visualisation"
    ]
  },
  {
    "objectID": "infovis/Infovis0_Vorbereitung.html",
    "href": "infovis/Infovis0_Vorbereitung.html",
    "title": "Preparation",
    "section": "",
    "text": "As part of InfoVis 1 - 2, we will need several R packages. We recommend installing these before the first lesson. Individual packages are typically installed as follows:\n\ninstall.packages(c(\"dplyr\", \"ggplot2\", \"lubridate\", \"readr\", \"scales\", \"tidyr\"))\n\nHowever, this will re-install packages that you might have already installed. The package pacman helps with this: it checks if the package is already installed before downloading it:\n\n# install packman (this is necessary only once)\ninstall.packages(\"pacman\")\n\n\npacman::p_install(\n  \"dplyr\", \"ggplot2\", \"lubridate\", \"readr\", \"scales\", \"tidyr\",\n  force = FALSE,\n  character.only = TRUE\n  )\n\nYou can download the datasets for the exercises from Moodle:\n\nInfoVis1\nInfoVis2",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Preparation</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html",
    "href": "infovis/Infovis1_Demo.html",
    "title": "Infovis 1: Demo A",
    "section": "",
    "text": "Base-plot vs. ggplot\nIn this demonstration, we’ll start by loading the dataset temperature_SHA_ZER.csv, a refined version of the data from our previous lessons, PrePro1 and PrePro2. You can download this data from moodle:\nWe can create a scatterplot in “Base-R” to compare dates and temperatures as follows:\nplot(temperature$time, temperature$SHA, type = \"l\", col = \"red\")\nlines(temperature$time, temperature$ZER, col = \"blue\")\nIn ggplot, the approach is more nuanced. A plot begins with ggplot(). This command specifies the dataset (data =) and the variables within the dataset that influence the plot (mapping = aes()).\nlibrary(\"ggplot2\")\n\n\n# Dataset: \"temperature\" | Influencing variables: \"time\" and \"temp\"\nggplot(data = temperature, mapping = aes(time, SHA))\nIn ggplot, at least one “layer” is required to represent data, such as geom_point() for scatterplots, using the + operator. Unlike “piping” (|&gt;), a layer is added with +.\nggplot(data = temperature, mapping = aes(time, SHA)) +\n  # Layer: \"geom_point\" corresponds to points in a scatterplot\n  geom_point()\nSince inputs are expected in the order of data = followed by mapping = in ggplot, we can omit these specifications.\nggplot(temperature, aes(time, SHA)) +\n  geom_point()",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "href": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "title": "Infovis 1: Demo A",
    "section": "Long vs. wide",
    "text": "Long vs. wide\nAs mentioned in PrePro 2, ggplot2 is designed for long tables. Therefore, we need to transform the wide table into a long format:\n\nlibrary(\"tidyr\")\n\n\ntemperature_long &lt;- pivot_longer(temperature, -time, names_to = \"station\", values_to = \"temp\")\n\nTo colour-code different weather stations, we define variables that will influence the graphic, which are incorporated in the aes() function:\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe can also add additional layers with lines:\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_point() +\n  geom_line()",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#labels",
    "href": "infovis/Infovis1_Demo.html#labels",
    "title": "Infovis 1: Demo A",
    "section": "Labels",
    "text": "Labels\nNext, we’ll refine our plot by adding axis labels and a title. Additionally, we’ve chosen to remove the points (geom_point()) as they don’t align with my preferred visualisation style.\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in degrees C°\",\n    title = \"Temperature Data Switzerland\",\n    subtitle = \"2001 to 2002\",\n    colour = \"Station\"\n  )",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#split-apply-combine",
    "href": "infovis/Infovis1_Demo.html#split-apply-combine",
    "title": "Infovis 1: Demo A",
    "section": "Split Apply Combine",
    "text": "Split Apply Combine\nIn our plot, the hourly data points are too detailed for a two-year visualisation. Using the Split Apply Combine technique (covered in PrePro 3), we can adjust the data resolution:\n\nlibrary(\"dplyr\")\n\n\ntemperature_day &lt;- temperature_long |&gt;\n  mutate(time = as.Date(time))\n\ntemperature_day\n\n# A tibble: 35,088 × 3\n   time       station  temp\n   &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;\n 1 2000-01-01 SHA       0.2\n 2 2000-01-01 ZER      -8.8\n 3 2000-01-01 SHA       0.3\n 4 2000-01-01 ZER      -8.7\n 5 2000-01-01 SHA       0.3\n 6 2000-01-01 ZER      -9  \n 7 2000-01-01 SHA       0.3\n 8 2000-01-01 ZER      -8.7\n 9 2000-01-01 SHA       0.4\n10 2000-01-01 ZER      -8.5\n# ℹ 35,078 more rows\n\ntemperature_day &lt;- temperature_day |&gt;\n  group_by(station, time) |&gt;\n  summarise(temp = mean(temp))\n\ntemperature_day\n\n# A tibble: 1,462 × 3\n# Groups:   station [2]\n   station time        temp\n   &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt;\n 1 SHA     2000-01-01  1.25\n 2 SHA     2000-01-02  1.73\n 3 SHA     2000-01-03  1.59\n 4 SHA     2000-01-04  1.78\n 5 SHA     2000-01-05  4.66\n 6 SHA     2000-01-06  3.49\n 7 SHA     2000-01-07  3.87\n 8 SHA     2000-01-08  3.28\n 9 SHA     2000-01-09  3.24\n10 SHA     2000-01-10  3.24\n# ℹ 1,452 more rows",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#adjusting-the-xy-axes",
    "href": "infovis/Infovis1_Demo.html#adjusting-the-xy-axes",
    "title": "Infovis 1: Demo A",
    "section": "Adjusting the X/Y Axes",
    "text": "Adjusting the X/Y Axes\nYou can also influence the x/y axes. You first have to determine what type of axis the plot has (in its default setting, ggplot automatically selects the axis type based on the nature of the data).\nFor our y-axis, which consists of numerical data, ggplot uses scale_y_continuous(). Other axis types can be found at ggplot2.tidyverse.org (scale_x_something or scale_y_something).\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in degrees C\",\n    title = \"Temperature Data Switzerland\",\n    subtitle = \"2001 to 2002\",\n    color = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) # determine y-axis section\n\n\n\n\n\n\n\n\nThis can also be done for the x-axis. Our x-axis consists of date information. ggplot calls this: scale_x_date().\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in degrees C\",\n    title = \"Temperature Data Switzerland\",\n    subtitle = \"2001 to 2002\",\n    color = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  )",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#customising-themes",
    "href": "infovis/Infovis1_Demo.html#customising-themes",
    "title": "Infovis 1: Demo A",
    "section": "Customising Themes",
    "text": "Customising Themes\nThe theme function in ggplot allows us to alter the general layout of plots. For instance, theme_classic() changes the plot’s style to a more traditional look, which is ideal for formal reports or publications. This theme can be applied either to individual plots or set as a default for all plots within a session.\nApplying to a single Plot:\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  theme_classic()\n\nGlobal setting (for all subsequent plots in the current session):\n\ntheme_set(theme_classic())",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "href": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "title": "Infovis 1: Demo A",
    "section": "Facets / Small Multiples",
    "text": "Facets / Small Multiples\nggplot also offers powerful functions for creating “Small multiples” using facet_wrap() (or facet_grid(), more on this later). These functions divide the main plot into smaller subplots based on a specified variable, denoted by the tilde symbol “~”.\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in °C\",\n    title = \"Temperature Data of Switzerland\",\n    subtitle = \"2001 to 2002\",\n    colour = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~station)\n\n\n\n\n\n\n\n\nfacet_wrap can also be customised further, such as by setting the number of facets per row with ncol =.\nIn addition, since the station names are displayed above each facet, we no longer require the legend. This is achieved with theme(legend.position=\"none\").\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in °C\",\n    title = \"Temperature Data of Switzerland\",\n    subtitle = \"2001 to 2002\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~station, ncol = 1) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#storing-and-exporting-plots",
    "href": "infovis/Infovis1_Demo.html#storing-and-exporting-plots",
    "title": "Infovis 1: Demo A",
    "section": "Storing and Exporting Plots",
    "text": "Storing and Exporting Plots\nLike data.frames and other objects, a complete ggplot plot can be stored in a variable. This is useful for exporting the plot (as PNG, JPG, etc.) or for progressively enhancing it, as shown in this example.\n\np &lt;- ggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\",\n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~station, ncol = 1)\n# At this point, theme(legend.position=\"none\") was removed\n\nTo save the plot as a PNG file (without specifying “plot =”, the last plot is simply saved):\n\nggsave(filename = \"plot.png\", plot = p)\n\nTo add a layer or option to an existing plot stored in a variable:\n\np +\n  theme(legend.position = \"none\")\n\nAs is typical with R, the modification made to the plot is not automatically saved; it only shows the outcome of the change. To permanently incorporate this change into my plot stored in the variable, we need to overwrite the variable with the updated plot:\n\np &lt;- p +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#smoothing",
    "href": "infovis/Infovis1_Demo.html#smoothing",
    "title": "Infovis 1: Demo A",
    "section": "Smoothing",
    "text": "Smoothing\nThe geom_smooth() function in ggplot can add trend lines to scatter plots. It is possible to select the underlying statistical method that is applied, yet by default, for datasets with fewer than 1,000 observations, ggplot defaults to using the stats::loess method. For larger datasets, it switches to mgcv::gam.\n\np &lt;- p +\n  geom_smooth(colour = \"black\")\np",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Infovis 1: Demo A</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html",
    "href": "infovis/Infovis1_Uebung.html",
    "title": "Infovis 1: Exercise",
    "section": "",
    "text": "Task 1\nThis exercise involves recreating the graphics from the Kovic (2014) blog post. Since the original blog post is no longer available, we’ve hosted a copy on the following website:\nhttps://researchmethods-zhaw.github.io/blog.tagesanzeiger.ch/\nPlease review the graphics in the blog post. The default settings for ggplot2 were used in the blog post, which makes recreating the graphics easier. The links in the text refer to the original graphics, while the embedded plots have been recreated.\nFirst, let’s import the dataset tagi_data_kanton.csv.\nYour first task is to recreate the following plot from Kovic (2014) using ggplot and the tagi_data_kanton.csv dataset:\nHere’s are some tips to get you started:",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infovis 1: Exercise</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-1",
    "href": "infovis/Infovis1_Uebung.html#task-1",
    "title": "Infovis 1: Exercise",
    "section": "",
    "text": "Create a ggplot object with ggplot(canton, aes(auslanderanteil, ja_anteil)), then add a point layer with geom_point().\nUse coord_fixed() to set a fixed ratio (1:1) between the axes.\nOptionally, you can:\n\nSet the axis limits with scale_x_continuous (or scale_y_continuous).\nManually set the breaks (0.0, 0.1 0.3 etc) within scale_x_continuous (or scale_y_continuous)\nUse labs() to label the axes.",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infovis 1: Exercise</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-2",
    "href": "infovis/Infovis1_Uebung.html#task-2",
    "title": "Infovis 1: Exercise",
    "section": "Task 2",
    "text": "Task 2\nNext, replicate the following plot from Kovic (2014) using ggplot:\nHere’s a tip:\n\nUse geom_smooth.",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infovis 1: Exercise</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-3",
    "href": "infovis/Infovis1_Uebung.html#task-3",
    "title": "Infovis 1: Exercise",
    "section": "Task 3",
    "text": "Task 3\nNow, let’s import the municipal data tagi_data_gemeinden.csv.\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere are some tips:\n\nUse geom_point().\nUse labs().\nUse coord_fixed().",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infovis 1: Exercise</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-4",
    "href": "infovis/Infovis1_Uebung.html#task-4",
    "title": "Infovis 1: Exercise",
    "section": "Task 4",
    "text": "Task 4\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse geom_smooth.",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infovis 1: Exercise</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-5",
    "href": "infovis/Infovis1_Uebung.html#task-5",
    "title": "Infovis 1: Exercise",
    "section": "Task 5",
    "text": "Task 5\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse facet_wrap to display a separate plot for each canton.",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infovis 1: Exercise</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-6",
    "href": "infovis/Infovis1_Uebung.html#task-6",
    "title": "Infovis 1: Exercise",
    "section": "Task 6",
    "text": "Task 6\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse geom_smooth.",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infovis 1: Exercise</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-7",
    "href": "infovis/Infovis1_Uebung.html#task-7",
    "title": "Infovis 1: Exercise",
    "section": "Task 7",
    "text": "Task 7\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse facet_wrap",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infovis 1: Exercise</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-8",
    "href": "infovis/Infovis1_Uebung.html#task-8",
    "title": "Infovis 1: Exercise",
    "section": "Task 8",
    "text": "Task 8\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse geom_smooth.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKovic, Marko. 2014. “Je Weniger Ausländer, Desto Mehr Ja-Stimmen? Wirklich?” Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich.",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Infovis 1: Exercise</span>"
    ]
  },
  {
    "objectID": "infovis/Infovis1_Script_eda.html",
    "href": "infovis/Infovis1_Script_eda.html",
    "title": "Infovis 1: EDA Script",
    "section": "",
    "text": "library(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"scales\")\n\n# create some data about age and height of people\npeople &lt;- data.frame(\n  ID = c(1:30),\n  age = c(\n    5.0, 7.0, 6.5, 9.0, 8.0, 5.0, 8.6, 7.5, 9.0, 6.0,\n    63.5, 65.7, 57.6, 98.6, 76.5, 78.0, 93.4, 77.5, 256.6, 512.3,\n    15.5, 18.6, 18.5, 22.8, 28.5, 39.5, 55.9, 50.3, 31.9, 41.3\n  ),\n  height = c(\n    0.85, 0.93, 1.1, 1.25, 1.33, 1.17, 1.32, 0.82, 0.89, 1.13,\n    1.62, 1.87, 1.67, 1.76, 1.56, 1.71, 1.65, 1.55, 1.87, 1.69,\n    1.49, 1.68, 1.41, 1.55, 1.84, 1.69, 0.85, 1.65, 1.94, 1.80\n  ),\n  weight = c(\n    45.5, 54.3, 76.5, 60.4, 43.4, 36.4, 50.3, 27.8, 34.7, 47.6,\n    84.3, 90.4, 76.5, 55.6, 54.3, 83.2, 80.7, 55.6, 87.6, 69.5,\n    48.0, 55.6, 47.6, 60.5, 54.3, 59.5, 34.5, 55.4, 100.4, 110.3\n  )\n)\n\n# build a scatterplot for a first inspection\nggplot(people, aes(x = age, y = height)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0.75, 2))\n\n\n\n\n\n\n\n# Go to help page: http://docs.ggplot2.org/current/ -&gt; Search for icon of fit-line\n# http://docs.ggplot2.org/current/geom_smooth.html\n\n\n# build a scatterplot for a first inspection, with regression line\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n# stem and leaf plot\nstem(people$height)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 25593\n##   10 | 037\n##   12 | 523\n##   14 | 19556\n##   16 | 255789916\n##   18 | 04774\nstem(people$height, scale = 2)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 2559\n##    9 | 3\n##   10 | \n##   11 | 037\n##   12 | 5\n##   13 | 23\n##   14 | 19\n##   15 | 556\n##   16 | 2557899\n##   17 | 16\n##   18 | 0477\n##   19 | 4\n\n\n# explore the two variables with box-whiskerplots\nsummary(people$age)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    5.00    8.70   30.20   59.14   65.15  512.30\nboxplot(people$age)\n\n\n\n\n\n\n\n\n\nsummary(people$height)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.820   1.190   1.555   1.455   1.690   1.940\nboxplot(people$height)\n\n\n\n\n\n\n\n\n\n# explore data with a histogram\nggplot(people, aes(x = age)) +\n  geom_histogram(binwidth = 20)\n\n\n\n\n\n\n\n\n\ndensity(x = people$height)\n## \n## Call:\n##  density.default(x = people$height)\n## \n## Data: people$height (30 obs.);   Bandwidth 'bw' = 0.1576\n## \n##        x                y           \n##  Min.   :0.3472   Min.   :0.001593  \n##  1st Qu.:0.8636   1st Qu.:0.102953  \n##  Median :1.3800   Median :0.510601  \n##  Mean   :1.3800   Mean   :0.483553  \n##  3rd Qu.:1.8964   3rd Qu.:0.722660  \n##  Max.   :2.4128   Max.   :1.216350\n\n# re-expression: use log or sqrt axes\n#\n# Find here guideline about scaling axes\n# http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/\n# http://docs.ggplot2.org/0.9.3.1/scale_continuous.html\n\n\n# logarithmic axis: respond to skewness in the data, e.g. log10\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\n# outliers: Remove very small and very old people\n\npeopleClean &lt;- people |&gt;\n  filter(ID != 27) |&gt; # This person was too short.\n  filter(age &lt; 100) # Error in age recorded.\n\n\nggplot(peopleClean, aes(x = age)) +\n  geom_histogram(binwidth = 10)\n\n\n\n\n\n\n\n\n\nggplot(peopleClean, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\n# with custom binwidth\nggplot(peopleClean, aes(x = age)) +\n  geom_histogram(binwidth = 10) +\n  theme_bw() # specifying the theme\n\n\n\n\n\n\n\n\n\n# quadratic axis\nggplot(peopleClean, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5) +\n  scale_x_sqrt()\n\n\n\n\n\n\n\n\n\n# filter \"teenies\": No trend\nfilter(peopleClean, age &lt; 15) |&gt;\n  ggplot(aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n# filter \"teens\": No trend\npeopleClean |&gt;\n  filter(age &gt; 55) |&gt;\n  ggplot(aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n# Onwards towards multidimensional data\n\n# Finally, make a scatterplot matrix\npairs(peopleClean[, 2:4], panel = panel.smooth)\n\n\n\n\n\n\n\n\n\npairs(peopleClean[, 2:4], panel = panel.smooth)",
    "crumbs": [
      "Information Visualisation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Infovis 1: EDA Script</span>"
    ]
  },
  {
    "objectID": "Statistic.html",
    "href": "Statistic.html",
    "title": "Statistic",
    "section": "",
    "text": "Write Intro\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nLesson\n\n\nTopic\n\n\n\n\n\n\nPreparation\n\n\n2025-04-01\n\n\nstat1\n\n\nPreparation\n\n\n\n\nStatistics 1: The Basics of Statistics\n\n\n2025-04-01\n\n\nstat1\n\n\nStatistic 1\n\n\n\n\nStatistics 2: Advanced Topics: Inductive and Multivariate Statistics\n\n\n2025-04-08\n\n\nstat2\n\n\nStatistic 2\n\n\n\n\nStatistics 3: Cluster analysis and data classification approaches\n\n\n2025-04-15\n\n\nstat3\n\n\nStatistic 3\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Statistic"
    ]
  },
  {
    "objectID": "statistic/Stat0_Vorbereitung.html",
    "href": "statistic/Stat0_Vorbereitung.html",
    "title": "Preparation",
    "section": "",
    "text": "As part of Statistic 1 - 3, we will need some R packages. We recommend installing them before the first lesson. Similar to the preparation exercise in Prepro1 you can use the code below to automatically install all packages that have not yet been installed.\n\nipak &lt;- function(pkg) {\n  new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) {\n    install.packages(new.pkg, dependencies = TRUE)\n  }\n}\n\npackages &lt;- c('dplyr', 'wooldridge')\n\nipak(packages)",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Preparation</span>"
    ]
  },
  {
    "objectID": "statistic/Stat1_Uebung.html",
    "href": "statistic/Stat1_Uebung.html",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "",
    "text": "Exercise I: Descriptive Statistics",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Statistics 1: The Basics of Statistics</span>"
    ]
  },
  {
    "objectID": "statistic/Stat1_Uebung.html#exercise-i-descriptive-statistics",
    "href": "statistic/Stat1_Uebung.html#exercise-i-descriptive-statistics",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "",
    "text": "Exercise 1\nUse the data in WAGE1.RAW for this exercise.\n\nFind the average education level in the sample. What are the lowest and highest years of education?\nFind the average hourly wage in the sample. Does it seem high or low?\n\nThe wage data are reported in 1976 dollars. Using the Economic Report of the President (2011 or later), obtain and report the Consumer Price Index (CPI) for the years 1976 and 2010.\n\nUse the CPI values from 3) to find the average hourly wage in 2010 dollars. Now does the average hourly wage seem reasonable?\n\nHow many women are in the sample? How many non-women?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('wage1')\n?wage1\n\n#1)\nmean(wage1$educ)\nmin(wage1$educ)\nmax(wage1$educ)\n\n#2)\nmean(wage1$wage)\n#This seems low\n\n#3)\n#page 259 of the report\n#CPI in 1976 is 56.9\n#CPI in 2010 is 218.056\n\n#4)\nmean(wage1$wage)*218.056/56.9\n\n#5)\nnrow(wage1[wage1$female == 1, ])\nnrow(wage1[wage1$female == 0, ])\n\n\n\n\nExercise 2\nUse the data in BWGHT.RAW to answer this question.\n\nHow many women are in the sample, and how many report smoking during pregnancy?\n\nWhat is the average number of cigarettes smoked per day? Is the average a good measure of the “typical” woman in this case? Explain.\n\nAmong women who smoked during pregnancy, what is the average number of cigarettes smoked per day? How does this compare with your answer from 2), and why?\n\nFind the average of fatheduc in the sample. Why are only 1,192 observations used to compute this average?\n\nReport the average family income and its standard deviation in dollars.\n\n\n\nSample Solution\nlibrary('wooldridge')\nlibrary('dplyr')\ndata('bwght')\n?bwght\n\n#1)\nnrow(bwght[is.na(bwght$mothereduc), ])\n#there is no missing information so we have 1388 women\n1388-nrow(bwght[(bwght$cigs==0), ])\n\n#2)\nmean(bwght$cigs)\n#No, because there is a lot of non-smokers\n\n#3)\nbwght %&gt;% filter(cigs&gt;0) %&gt;% summarise(mean(cigs))\n\n#4)\nmean(bwght$fatheduc, na.rm = TRUE)\n\n#5)\nmean(bwght$faminc)\nsd(bwght$faminc)\n\n\n\n\nExercise 3\nThe data in MEAP01.RAW are for the state of Michigan in the year 2001. Use these data to answer the following questions.\n\nFind the largest and smallest values of math4. Does the range make sense? Explain.\n\nHow many schools have a perfect pass rate on the math test? What percentage is this of the total sample?\n\nHow many schools have math pass rates of exactly 50%?\n\nCompare the average pass rates for the math and reading scores. Which test is harder to pass?\n\nFind the correlation between math4 and read4. What do you conclude?\n\nThe variable exppp is expenditure per pupil. Find the average of exppp along with its standard deviation. Would you say there is wide variation in per pupil spending?\n\nSuppose School A spends $6,000 per student and School B spends $5,500 per student. By what percentage does School A’s spending exceed School B’s?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('meap01')\n?meap01\n\n#1)\nmin(meap01$math4)\nmax(meap01$math4)\n\n#2)\nnrow(meap01[(meap01$math4==100), ])\nnrow(meap01[(meap01$math4==100), ])/1823*100\n\n#3)\nnrow(meap01[(meap01$math4==50), ])\n\n#4)\nmean(meap01$math4)\nmean(meap01$read4)\n\n#5)\ncorr=lm(math4 ~ read4, data = meap01)\nsummary(corr)\n\n#6)\nmean(meap01$exppp)\nsd(meap01$exppp)\n\n#7)\n(6000-5500)/5500*100\n\n\n\n\nExercise 4\nThe data in JTRAIN2.RAW come from a job training experiment conducted for low-income men during 1976–1977.\n\nUse the indicator variable train to determine the fraction of men receiving job training.\n\nThe variable re78 is earnings from 1978, measured in thousands of 1982 dollars. Find the averages of re78 for the sample of men receiving job training and the sample not receiving job training. Is the difference economically large?\n\nThe variable unem78 is an indicator of whether a man is unemployed or not in 1978. What fraction of the men who received job training are unemployed? What about for men who did not receive job training? Comment on the difference.\n\nFrom questions 2 and 3, does it appear that the job training program was effective? What would make our conclusions more convincing?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('jtrain2')\n?jtrain2\n\n#1)\nnrow(jtrain2[(jtrain2$train==1), ])/445\n\n#2)\njtrain2 %&gt;% filter(train==0) %&gt;% summarise(mean(re78))\njtrain2 %&gt;% filter(train==1) %&gt;% summarise(mean(re78))\n\n#3)\n(nrow(jtrain2[(jtrain2$train==1), ]) - (jtrain2 %&gt;% filter(train==1) %&gt;% summarise(sum(unem78))))/nrow(jtrain2[(jtrain2$train==1), ])\n\n(nrow(jtrain2[(jtrain2$train==0), ]) - (jtrain2 %&gt;% filter(train==0) %&gt;% summarise(sum(unem78))))/nrow(jtrain2[(jtrain2$train==0), ])\n\n#4)\n#No\n#To make a linear regression to find if there is evidence on the fact that the training does not have any effects on the employement rate\n\n\n\n\nExercise 5\nThe data in FERTIL2.DTA were collected on women living in the Republic of Botswana in 1988. The variable children refers to the number of living children. The variable electric is a binary indicator equal to one if the woman’s home has electricity, and zero if not.\n\nFind the smallest and largest values of children in the sample. What is the average of children?\n\nWhat percentage of women have electricity in the home?\n\nCompute the average of children for those without electricity and do the same for those with electricity. Comment on what you find.\n\nFrom question 3), can you infer that having electricity “causes” women to have fewer children? Explain.\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('fertil2')\n?fertil2\n\n#1)\nmin(fertil2$children)\nmax(fertil2$children)\nmean(fertil2$children)\n\n#2)\nnrow(fertil2[(fertil2$electric==1), ])/4361*100\n\n#3)\nfertil2 %&gt;% filter(electric==0) %&gt;% summarise(mean(children))\nfertil2 %&gt;% filter(electric==1) %&gt;% summarise(mean(children))\n\n#4)\n#No",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Statistics 1: The Basics of Statistics</span>"
    ]
  },
  {
    "objectID": "statistic/Stat1_Uebung.html#exercise-ii-the-simple-linear-regression-model",
    "href": "statistic/Stat1_Uebung.html#exercise-ii-the-simple-linear-regression-model",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "Exercise II: The Simple Linear Regression Model",
    "text": "Exercise II: The Simple Linear Regression Model\n\nExercise 6\nThe data in 401K.RAW are a subset of data analyzed by Papke (1995) to study the relationship between participation in a 401(k) pension plan and the generosity of the plan. The variable prate is the percentage of eligible workers with an active account; this is the variable we would like to explain. The measure of generosity is the plan match rate, mrate. This variable gives the average amount the firm contributes to each worker’s plan for each $1 contribution by the worker. For example, if mrate = 0.50, then a $1 contribution by the worker is matched by a 50¢ contribution by the firm.\n\nFind the average participation rate and the average match rate in the sample of plans.\nNow, estimate the simple regression equation \\(\\widehat{prate} = \\hat{\\beta_0} + \\hat{\\beta_1}mrate,\\) and report the results along with the sample size and R-squared.\nInterpret the intercept in your equation. Interpret the coefficient on mrate.\nFind the predicted prate when mrate = 3.5. Is this a reasonable prediction? Explain what is happening here.\nHow much of the variation in prate is explained by mrate? Is this a lot in your opinion?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('k401k')\n?k401k\n\n#1)\nmean(k401k$prate)\nmean(k401k$mrate)\n\n#2)\nLinMod &lt;- lm(prate ~ mrate, data=k401k)\nsummary(LinMod)\n\n#3)\n#Intercept is 83.0755\n#if mrate increase by 1% then prate increases by 5.86%\n\n#4)\n#No\n\n#5)\n#R-squared=0.075. No\n\n\n\n\nExercise 7\nThe data set in CEOSAL2.RAW contains information on chief executive officers for U.S. corporations. The variable salary is annual compensation, in thousands of dollars, and ceoten is prior number of years as company CEO.\n\nFind the average salary and the average tenure in the sample.\nHow many CEOs are in their first year as CEO (that is, ceoten = 0)? What is the longest tenure as a CEO?\nEstimate the simple regression model \\(log(salary) = \\beta_0 + \\beta_1ceoten + u,\\) and report your results in the usual form. What is the (approximate) predicted percentage increase in salary given one more year as a CEO?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('ceosal2')\n?ceosal2\n\n#1)\nmean(ceosal2$salary)\nmean(ceosal2$ceoten)\n\n#2)\nmax(ceosal2$ceoten)\nnrow(ceosal2[(ceosal2$ceoten==0), ])\n\n#3)\nLinMod &lt;- lm(log(salary) ~ ceoten, data=ceosal2)\nsummary(LinMod)\n\n\n\n\nExercise 8\nUse the data in SLEEP75.RAW from Biddle and Hamermesh (1990) to study whether there is a tradeoff between the time spent sleeping per week and the time spent in paid work. We could use either variable as the dependent variable. For concreteness, estimate the model \\(sleep = \\beta_0 + \\beta_1totwrk + u,\\) where sleep is minutes spent sleeping at night per week and totwrk is total minutes worked during the week.\n\nReport your results in equation form along with the number of observations and R-squared. What does the intercept in this equation mean?\nIf totwrk increases by 2 hours, by how much is sleep estimated to fall? Do you find this to be a large effect?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('sleep75')\n?sleep75\n\n#1)\nLinMod &lt;- lm(sleep ~ totwrk, data=sleep75)\nsummary(LinMod)\n\n#2)\n#120*0.15075=18.09 minutes\n\n\n\n\nExercise 9\nFor the population of firms in the chemical industry, let rd denote annual expenditures on research and development, and let sales denote annual sales (both are in millions of dollars).\n\nWrite down a model (not an estimated equation) that implies a constant elasticity between rd and sales. Which parameter is the elasticity?\nNow, estimate the model using the data in RDCHEM.RAW. Write out the estimated equation in the usual form. What is the estimated elasticity of rd with respect to sales? Explain in words what this elasticity means.\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('rdchem')\n?rdchem\n\n#1) Constant elasticity -&gt; log ~ log regression, the elasticity is the slope parameter\n\n#2)\nLinMod &lt;- lm(log(rd) ~ log(sales), data=rdchem)\nsummary(LinMod)\n#when sales increase by 1%, R&D increase by 1.08%",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Statistics 1: The Basics of Statistics</span>"
    ]
  },
  {
    "objectID": "statistic/Stat2_Uebung.html",
    "href": "statistic/Stat2_Uebung.html",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "",
    "text": "Exercise I: Estimation of Multiple Regression Analysis",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistics 2: Advanced Topics: Inductive and Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-1",
    "href": "statistic/Stat2_Uebung.html#exercise-1",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise 1",
    "text": "Exercise 1\nA problem of interest to health officials (and others) is to determine the effects of smoking during pregnancy on infant health. One measure of infant health is birth weight; a birth weight that is too low can put an infant at risk for contracting various illnesses. Since factors other than cigarette smoking that affect birth weight are likely to be correlated with smoking, we should take those factors into account. For example, higher income generally results in access to better prenatal care, as well as better nutrition forthe mother. An equation that recognizes this is \\(bwght = \\beta_0 + \\beta_1cigs + \\beta_2 faminc + u.\\)\n\nWhat is the most likely sign for \\beta_2?\nDo you think cigs and faminc are likely to be correlated? Explain why the correlation might be positive or negative.\nNow, estimate the equation with and without faminc, using the data in BWGHT.RAW. Report the results in equation form, including the sample size and R-squared. Discuss your results, focusing on whether adding faminc substantially changes the estimated effect of cigs on bwght.\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('bwght')\n?bwght\n\n#1)\n#likely to be positive\n\n#2)\n#Yes\n#negative correlation\nlinearBwght0 &lt;- lm(cigs ~ faminc, data=bwght)\nsummary(linearBwght0)\n\n#3)\nlinearBwght1 &lt;- lm(bwght ~ cigs + faminc, data=bwght)\nlinearBwght2 &lt;- lm(bwght ~ cigs, data=bwght)\n\nsummary(linearBwght1)\nsummary(linearBwght2)",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistics 2: Advanced Topics: Inductive and Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-2",
    "href": "statistic/Stat2_Uebung.html#exercise-2",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise 2",
    "text": "Exercise 2\nUse the data in HPRICE1.RAW to estimate the model \\(price = \\beta_0 + \\beta_1sqrft + \\beta_2bdrms + u,\\) where price is the house price measured in thousands of dollars.\n\nWrite out the results in equation form.\nWhat is the estimated increase in price for a house with one more bedroom, holding square footage constant?\nWhat is the estimated increase in price for a house with an additional bedroom that is 140 square feet in size? Compare this to your answer in question 2.\nWhat percentage of the variation in price is explained by square footage and number of bedrooms?\nThe first house in the sample has sqrft = 2,438 and bdrms = 4. Find the predicted selling price for this house from the OLS regression line.\nThe actual selling price of the first house in the sample was $300,000 (so price = 300). Find the residual for this house. Does it suggest that the buyer underpaid or overpaid for the house?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('hprice1')\n?hprice1\n\n#1)\nlinearHprice &lt;- lm(price ~ sqrft + bdrms, data=hprice1)\nsummary(linearHprice)\n\n#2)\n#+15.2 thousand dollars\n\n#3)\n140*0.128+15.2\n#33.12 thousand dollars\n\n#4)\n#R2 = 0.63\n\n#5)\n2438*0.128+4*15.2-19.31\n#354 thousand dollars\n\n#6)\n354-300\n#residual of 54 thousand dollars\n#the buyer underpaid",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistics 2: Advanced Topics: Inductive and Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-3",
    "href": "statistic/Stat2_Uebung.html#exercise-3",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise 3",
    "text": "Exercise 3\nThe file CEOSAL2.RAW contains data on 177 chief executive officers and can be used to examine the effects of firm performance on CEO salary.\n\nEstimate a model relating annual salary to firm sales and market value. Make the model of the constant elasticity variety for both independent variables. Write the results out in equation form.\nAdd profits to the model from question 1. Why can this variable not be included in logarithmic form? Would you say that these firm performance variables explain most of the variation in CEO salaries?\nAdd the variable ceoten to the model in question 2. What is the estimated percentage return for another year of CEO tenure, holding other factors fixed?\nFind the sample correlation coefficient between the variables log(mktval) and profits. Are these variables highly correlated?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('ceosal2')\n?ceosal2\n\n#1)\nlinearCEO &lt;- lm(log(salary) ~ log(sales) + log(mktval), data=ceosal2)\nsummary(linearCEO)\n\n#2)\nlinearCEO2 &lt;- lm(log(salary) ~ log(sales) + log(mktval) + profits, data=ceosal2)\nsummary(linearCEO2)\n\nmin(ceosal2$profits)\n#because it contains negative values\n#Yes, R2 is important\n\n#3)\nlinearCEO3 &lt;- lm(log(salary) ~ log(sales) + log(mktval) + profits + ceoten, data=ceosal2)\nsummary(linearCEO3)\n\n#4)\nlinearCEO4 &lt;- lm(log(mktval) ~ profits, data=ceosal2)\nsummary(linearCEO4)\n#Yes, R2 is important",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistics 2: Advanced Topics: Inductive and Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-ii-multiple-regression-analysis-inference",
    "href": "statistic/Stat2_Uebung.html#exercise-ii-multiple-regression-analysis-inference",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise II: Multiple Regression Analysis: Inference",
    "text": "Exercise II: Multiple Regression Analysis: Inference",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistics 2: Advanced Topics: Inductive and Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-4",
    "href": "statistic/Stat2_Uebung.html#exercise-4",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise 4",
    "text": "Exercise 4\nThe following model can be used to study whether campaign expenditures affect election outcomes: \\(voteA = \\beta_0 + \\beta_1log(expendA) + \\beta_2log(expendB) + \\beta_3 prtystrA + u,\\) where voteA is the percentage of the vote received by Candidate A, expendA and expendB are campaign expenditures by Candidates A and B, and prtystrA is a measure of party strength for Candidate A (the percentage of the most recent presidential vote that went to A’s party).\n\nWhat is the interpretation of \\(\\beta_1\\)?\nIn terms of the parameters, state the null hypothesis that a 1% increase in A’s expenditures is offset by a 1% increase in B’s expenditures.\nEstimate the given model using the data in VOTE1.RAW and report the results in usual form. Do A’s expenditures affect the outcome? What about B’s expenditures? Can you use these results to test the hypothesis in question 2?\nEstimate a model that directly gives the t statistic for testing the hypothesis in question 2. What do you conclude? (Use a two-sided alternative knowing that the 10% critical value against a two-side alternative with 169 df is 1.645)\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('vote1')\n?vote1\n\n#1)\n#beta_1 measures the percentage point candidate A votes increases when candidate A expenditures increase by 1%\n\n#2)\n#H_0: \\beta_1 + \\beta_2 = 0\n\n#3)\nlinearVote &lt;- lm(voteA ~ log(expendA) + log(expendB) + prtystrA, data=vote1)\nsummary(linearVote)\n#No (linear combination of two estimates)\n\n#4)\nlinearVote2 &lt;- lm(voteA ~ log(expendA) + log(expendB/expendA) + prtystrA, data=vote1)\nsummary(linearVote2)\n#We fail to reject H_0",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistics 2: Advanced Topics: Inductive and Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-5",
    "href": "statistic/Stat2_Uebung.html#exercise-5",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise 5",
    "text": "Exercise 5\nUse the data in WAGE2.RAW for this exercise.\n\nConsider the standard wage equation \\(log(wage) = \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 tenure + u\\). State the null hypothesis that another year of general workforce experience has no effect on log(wage).\nTest the null hypothesis in question 1. against a two-sided alternative, at the 5% significance level. What do you conclude?\nState the null hypothesis that another year of general workforce experience has the same effect on log(wage) as another year of tenure with the current employer.\nTest the null hypothesis in question 3. against a two-sided alternative, at the 5% significance level.\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('wage2')\n?wage2\n\n#1)\n#H_0: \\beta_2 = 0\n\n#2)\nlinearWage &lt;- lm(log(wage) ~ educ + exper + tenure, data=wage2)\nsummary(linearWage)\n#Significant. a 1-year experience increases wage by 1.53%\n\n#3)\n#H_0: \\beta_2 - \\beta_3 = 0\n\n#4)\nwage_new &lt;- wage2 %&gt;% mutate(var=exper+tenure)\nlinearWage2 &lt;- lm(log(wage) ~ educ + exper + var, data=wage_new)\nsummary(linearWage2)\n#We fail to reject H_0",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistics 2: Advanced Topics: Inductive and Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-6",
    "href": "statistic/Stat2_Uebung.html#exercise-6",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise 6",
    "text": "Exercise 6\nThe data set 401KSUBS.RAW contains information on net financial wealth nettfa, age of the survey respondent age, annual family income inc, family size fsize, and participation in certain pension plans for people in the United States. The wealth and income variables are both recorded in thousands of dollars. For this question, use only the data for single-person households (so fsize = 1).\n\nHow many single-person households are there in the data set?\nUse OLS to estimate the model \\(nettfa = \\beta_0 + \\beta_1inc + \\beta_2age + u,\\) and report the results using the usual format. Be sure to use only the single-person households in the sample. Interpret the slope coefficients. Are there any surprises in the slope estimates?\nDoes the intercept from the regression in question 2 have an interesting meaning? Explain.\nFind the p-value for the test \\(H_0: \\beta_2 = 0\\) against a two-side alternative. Do you reject \\(H_0\\) at the 1% significance level?\nIf you do a simple regression of nettfa on inc, is the estimated coefficient on inc much different from the estimate in question 2? Why or why not?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('k401ksubs')\n?k401ksubs\n\n#1)\nk401ksubs %&gt;% filter(fsize==1) %&gt;% summarise(n())\n\n#2)\ndata_single &lt;- k401ksubs %&gt;% filter(fsize==1)\nlinear_single &lt;- lm(nettfa ~ inc + age, data=data_single)\nsummary(linear_single)\n#Not very surprising\n\n#3)\n#Not an interesting meaning\n\n#4)\n#Yes\n\n#5)\nsimple_linear &lt;- lm(nettfa ~ inc, data=data_single)\nsummary(simple_linear)\n#Not so different",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistics 2: Advanced Topics: Inductive and Multivariate Statistics</span>"
    ]
  },
  {
    "objectID": "statistic/Stat3_Uebung.html",
    "href": "statistic/Stat3_Uebung.html",
    "title": "Statistics 3: Cluster analysis and data classification approaches",
    "section": "",
    "text": "K-means Clustering\ninstall.packages(\"factoextra\")\ninstall.packages(\"mclust\")\nlibrary(factoextra)\nConsider the \\(iris\\) dataset. Suppose we are given the petal and sepal length and widths, but not told which species each iris belongs to.\nOur goal is to find clusters within the data. These are groups of irises that are more similar to each other than to those in other groups (clusters). These clusters may correspond to the Species label (which we aren’t given), or they may not. The goal of cluster analysis is not to predict the species, but simply to group the data into similar clusters.\nLet’s look at finding 3 clusters. We can do this using the kmeans command in R.\niris2 &lt;- iris[,1:4]\n# nstart gives the number of random initialisations to try \nset.seed(123)\n(iris.k &lt;- kmeans(iris2, centers = 3, nstart=25))\nFrom this output we can read off the final cluster means. Also given is the final within-cluster sum of squares for each cluster.\nWe can visualise the output of K-means using the fviz_cluster command from the factoextra package. This first projects the points into two dimensions using PCA, and then shows the classification in 2D, and so some caution is needed in interpreting these plots.\nfviz_cluster(iris.k, data = iris2,\n             geom = \"point\")\nFinally, in this case we know that there really are three clusters in the data (the three species). We can compare the clusters found using K-means with the species label to see if they are similar. The easiest way to do this is with the table command.\ntable(iris$Species, iris.k$cluster)\nIn the iris data, we know there are 3 distinct species. But suppose we didn’t know this. What happens if we try other values for \\(K\\)?\nFor the iris data, we can create an elbow plot using the fviz_nbclust command from the factoextra package.\nfviz_nbclust(iris2, kmeans, method = \"wss\")\nIn this case, we would probably decide there most likely three natural clusters in the data, as there is a reasonable decrease in W when moving from 2 to 3 clusters, but moving to 3 clusters only yields a minor improvement. Note here the slight increase in W in moving from 9 to 10 clusters. This is due to only using a greed search, rather than an exhaustive one (we know the best 10-group cluster must be better than the best 9-group cluster, we just have found it).",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistics 3: Cluster analysis and data classification approaches</span>"
    ]
  },
  {
    "objectID": "statistic/Stat3_Uebung.html#hierarchical-clustering",
    "href": "statistic/Stat3_Uebung.html#hierarchical-clustering",
    "title": "Statistics 3: Cluster analysis and data classification approaches",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nSuppose we are given 5 observations with distance matrix\n\n(D &lt;- as.dist(matrix(c(0,0,0,0,0,\n                      2,0,0,0,0,\n                      11,9,0,0,0,\n                      15,13,10,0,0,\n                      7,5,4,8,0), nr=5, byrow=T)))\n\nThe hclust command does agglomerative clustering: we just have to specify the method to use.\n\nD.sl &lt;-hclust(D, method=\"single\")\n\nTo display the dendrogram\n\nplot(D.sl)\n\nChanging the method\n\nplot(hclust(D, method=\"complete\"))\n\nGroup average clustering produces the same hierarchy of clusters as single linkage, but the nodes (points where clusters join) are at different heights in the dendrogram.\n\nD.ga &lt;- hclust(D, method=\"average\")\nplot(D.ga)",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistics 3: Cluster analysis and data classification approaches</span>"
    ]
  },
  {
    "objectID": "statistic/Stat3_Uebung.html#model-based-clustering",
    "href": "statistic/Stat3_Uebung.html#model-based-clustering",
    "title": "Statistics 3: Cluster analysis and data classification approaches",
    "section": "Model-Based Clustering",
    "text": "Model-Based Clustering\nModel-based clustering is similar to K-means clustering, in that we want to allocate each case to a cluster. The difference is that we will now assume a probability distribution for the observations within each cluster.\nThe mclust library can be used to perform model-based clustering with Gaussian clusters. We just have to specify the number of required clusters.\n\nlibrary(mclust)\niris.m &lt;- Mclust(iris2,G=3)\n\nPairs plots of the classification of each point can easily be obtained, as can the estimated probability density of each cluster.\n\nplot(iris.m, what = c(\"classification\"))\n\nChanging the representation\n\nplot(iris.m, what = c(\"density\"))",
    "crumbs": [
      "Statistic",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistics 3: Cluster analysis and data classification approaches</span>"
    ]
  },
  {
    "objectID": "SpatAn.html",
    "href": "SpatAn.html",
    "title": "Spatial Analysis",
    "section": "",
    "text": "Part 1\nThe first exercise introduces the basics of loading and displaying geospatial data in both vector and raster formats. It also covers the fundamentals of coordinate systems and vector-to-raster conversion. Initial analyses demonstrate the use of Spatial Joins and the annotation of points with attributes from enclosing vector data. Finally, the issue of spatial data aggregation dependency is addressed, illustrated by the Modifiable Areal Unit Problem (MAUP).",
    "crumbs": [
      "Spatial Analysis"
    ]
  },
  {
    "objectID": "SpatAn.html#part-2",
    "href": "SpatAn.html#part-2",
    "title": "Spatial Analysis",
    "section": "Part 2",
    "text": "Part 2\nThe second exercise focuses on processing and visualising geospatial datasets, starting with a point dataset on air quality measurement in Switzerland (specifically nitrogen dioxide (NO2) levels). Unlike the point dataset on water availability from the previous exercise, the air quality monitoring sites have an irregular spatial distribution. Nevertheless, the goal is to interpolate a continuous layer of air quality values across Switzerland. We begin with the Inverse Distance Weighting (IDW) interpolation method, followed by constructing Thiessen Polygons using a nearest-neighbor approach. The latter part of the exercise examines density distribution, utilising a dataset on the movement of red kites in Switzerland. A Kernel Density Estimation (KDE) will be used to calculate a continuous density distribution, providing an approximation of the habitat of this bird of prey species.",
    "crumbs": [
      "Spatial Analysis"
    ]
  },
  {
    "objectID": "spatan/Spatan0_Vorbereitung.html",
    "href": "spatan/Spatan0_Vorbereitung.html",
    "title": "Preparation",
    "section": "",
    "text": "As part of SpatAn 1 - 3, we will need some R packages. We recommend installing them before the first lesson. Similar to the preparation exercise in InfoVis1 you can use pacman to automatically install all packages that have not yet been installed.\n\npacman::p_install(\n  \"sf\", \"dplyr\", \"ggplot2\", \"spatstat.geom\", \n  \"spatstat.explore\",\"spatstat.geom\",\n  \"gstat\", \"tidyr\", \"terra\", \"tmap\",\n  force = FALSE,\n  character.only = TRUE\n  )\n\nYou can download the datasets for the exercises from Moodle.",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Preparation</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan1_Uebung_A.html",
    "href": "spatan/Spatan1_Uebung_A.html",
    "title": "SpatAn 1: Exercise A",
    "section": "",
    "text": "Task 1: Import vector data\nYou can download the datasets for the exercises from Moodle-\nImport the kantone.gpkg and gemeinden.gpkg records as follows. These are geodatasets in the geopackage (\\* .gpkg) format.\nlibrary(\"sf\")\n\ncantons &lt;- read_sf(\"datasets/rauman/kantone.gpkg\")\nmunicipalities &lt;- read_sf(\"datasets/rauman/gemeinden.gpkg\")\nLook at the imported records in your terminal (see Note 19.1).",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>SpatAn 1: Exercise A</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan1_Uebung_A.html#task-1-import-vector-data",
    "href": "spatan/Spatan1_Uebung_A.html#task-1-import-vector-data",
    "title": "SpatAn 1: Exercise A",
    "section": "",
    "text": "Note 19.1\n\n\n\nYou will get the most information about sf objects if you look at the record in the console (by typing the variable name in the console). When using the RStudio Viewer, sf objects load very slowly and metadata is not displayed.",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>SpatAn 1: Exercise A</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan1_Uebung_A.html#task-2-visualise-data",
    "href": "spatan/Spatan1_Uebung_A.html#task-2-visualise-data",
    "title": "SpatAn 1: Exercise A",
    "section": "Task 2: Visualise data",
    "text": "Task 2: Visualise data\nA very simple way of visualising sf objects is to use the plot() function in base-R. Execute the specified R commands and study the resulting plots. What differences can you see? How do you explain these differences?\n\n# without max.plot = 1 will result in R per plot per column\nplot(municipalities, max.plot = 1)\n\n\n\n\n\n\n\n\n# Alternatively, you can also plot a specific column\nplot(cantons[\"KANTONSFLA\"])",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>SpatAn 1: Exercise A</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan1_Uebung_A.html#input-coordinate-systems",
    "href": "spatan/Spatan1_Uebung_A.html#input-coordinate-systems",
    "title": "SpatAn 1: Exercise A",
    "section": "Input: Coordinate systems",
    "text": "Input: Coordinate systems\nIn the above visualisation, the following is noticeable:\n\nthe X/Y axes have two very different number ranges (see the axis labels)\nthe outline of Switzerland looks different in the two datasets (cantons are compressed against municipalities)\n\nOf course, this has to do with the fact that the two data sets were recorded in different coordinate systems. Coordinate systems are abbreviated to CRS (Coordinate Reference System). The assigned coordinate systems can be queried with st_crs().\n\nst_crs(cantons)\n## Coordinate Reference System:\n##   User input: Undefined Cartesian SRS \n##   wkt:\n## ENGCRS[\"Undefined Cartesian SRS\",\n##     EDATUM[\"Unknown engineering datum\"],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"Meter\",1]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"Meter\",1]]]\nst_crs(municipalities)\n## Coordinate Reference System:\n##   User input: Undefined Cartesian SRS \n##   wkt:\n## ENGCRS[\"Undefined Cartesian SRS\",\n##     EDATUM[\"Unknown engineering datum\"],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"Meter\",1]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"Meter\",1]]]\n\nUnfortunately, no coordinate systems are assigned in our case. With a little experience, however, you can guess which coordinate system is used, because a lot of them can be ruled out. The three most common coordinate systems in Switzerland are as follows:\n\nCH1903 LV03: the old coordinate system of Switzerland\nCH1903+ LV95: the new coordinate system of Switzerland\nWGS84: a frequently used, global geodetic coordinate system, i.e., the coordinates are given in length and width (lat/lon).\n\nIt is important to determine the correct coordinate system on the basis of the coordinates shown in the geometry column. If you select a location by right clicking on map.geo.admin.ch, you can find the coordinates of this location in various coordinate reference systems (see Figure 19.1).\n\n\n\n\n\n\nFigure 19.1: The sampe location in different coordinate systems, displayed on the website map.geo.admin.ch\n\n\n\nIf you compare these coordinates with the coordinates of our data sets, it quickly becomes clear that the cantons dataset is the coordinate reference system (CRS) WGS84. We can use this information to set the CRS of our dataset with st_set_crs().\n\n# Assign with st_set_crs()...\ncantons &lt;- st_set_crs(cantons, \"WGS84\")\n\nIf we now retrieve the CRS information, we should see that this task has been successfully completed.\n\n# ... query with st_crs()\nst_crs(cantons)\n## Coordinate Reference System:\n##   User input: WGS84 \n##   wkt:\n## GEOGCRS[\"WGS 84\",\n##     DATUM[\"World Geodetic System 1984\",\n##         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n##             LENGTHUNIT[\"metre\",1]]],\n##     PRIMEM[\"Greenwich\",0,\n##         ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     CS[ellipsoidal,2],\n##         AXIS[\"geodetic latitude (Lat)\",north,\n##             ORDER[1],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         AXIS[\"geodetic longitude (Lon)\",east,\n##             ORDER[2],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     ID[\"EPSG\",4326]]\n\nIt is a bit more complicated if we want to set the CRS of the municipalities dataset. In comparison with map.geo.admin.ch, we can see that this must be the CRS CH1903+ LV95. Using this name for our CRS assignment won’t work:\n\n# Assign with st_set_crs()...\nmunicipalities &lt;- st_set_crs(municipalities, \"CH1903+ LV95\")\n\n# ... query with st_crs()\nst_crs(municipalities)\n## Coordinate Reference System: NA\n\nThe advertised names of these CRS are prone to errors. Therefore, it is better to work with the respective EPSG codes of the reference systems. These EPSG codes can be found on the following website: epsg.io/map. It is worth noting the EPSG codes of the relevant CRS:\n\nCH1903 LV03: EPSG:21781\nCH1903+ LV95: EPSG:2056\nWGS84: EPSG:4326\n\nWe can use this code to set the CRS of the municipalities dataset:\n\n# Assign with st_set_crs()...\nmunicipalities &lt;- st_set_crs(municipalities, 2056)\n\n# ... query with st_crs()\nst_crs(municipalities)\n## Coordinate Reference System:\n##   User input: EPSG:2056 \n##   wkt:\n## PROJCRS[\"CH1903+ / LV95\",\n##     BASEGEOGCRS[\"CH1903+\",\n##         DATUM[\"CH1903+\",\n##             ELLIPSOID[\"Bessel 1841\",6377397.155,299.1528128,\n##                 LENGTHUNIT[\"metre\",1]]],\n##         PRIMEM[\"Greenwich\",0,\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         ID[\"EPSG\",4150]],\n##     CONVERSION[\"Swiss Oblique Mercator 1995\",\n##         METHOD[\"Hotine Oblique Mercator (variant B)\",\n##             ID[\"EPSG\",9815]],\n##         PARAMETER[\"Latitude of projection centre\",46.9524055555556,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8811]],\n##         PARAMETER[\"Longitude of projection centre\",7.43958333333333,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8812]],\n##         PARAMETER[\"Azimuth of initial line\",90,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8813]],\n##         PARAMETER[\"Angle from Rectified to Skew Grid\",90,\n##             ANGLEUNIT[\"degree\",0.0174532925199433],\n##             ID[\"EPSG\",8814]],\n##         PARAMETER[\"Scale factor on initial line\",1,\n##             SCALEUNIT[\"unity\",1],\n##             ID[\"EPSG\",8815]],\n##         PARAMETER[\"Easting at projection centre\",2600000,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8816]],\n##         PARAMETER[\"Northing at projection centre\",1200000,\n##             LENGTHUNIT[\"metre\",1],\n##             ID[\"EPSG\",8817]]],\n##     CS[Cartesian,2],\n##         AXIS[\"(E)\",east,\n##             ORDER[1],\n##             LENGTHUNIT[\"metre\",1]],\n##         AXIS[\"(N)\",north,\n##             ORDER[2],\n##             LENGTHUNIT[\"metre\",1]],\n##     USAGE[\n##         SCOPE[\"Cadastre, engineering survey, topographic mapping (large and medium scale).\"],\n##         AREA[\"Liechtenstein; Switzerland.\"],\n##         BBOX[45.82,5.96,47.81,10.49]],\n##     ID[\"EPSG\",2056]]\n\nNow that the CRS of the datasets is known, we can use ggplot2 to visualise our data. In InfoVis 1 & 2, we worked intensively with ggplot2 and got to know the geom_point() and geom_line() layers. ggplot() is also able to very easily plot vector data with geom_sf().\n\n\nSample Solution\nlibrary(\"ggplot2\") # Add this line to the top of you script\n\nggplot() +\n  # In geom_sf neither x nor y axes need to be defined\n  geom_sf(data = municipalities)",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>SpatAn 1: Exercise A</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan1_Uebung_A.html#task-3-transform-coordinate-systems",
    "href": "spatan/Spatan1_Uebung_A.html#task-3-transform-coordinate-systems",
    "title": "SpatAn 1: Exercise A",
    "section": "Task 3: Transform coordinate systems",
    "text": "Task 3: Transform coordinate systems\nIn the previous exercise, we assigned a coordinate system but we did not manipulate the existing coordinates (in the geom column). It is quite different to transfer the data from one coordinate system to the other. In the process of transforming the system, the coordinates are converted and thus manipulated. For practical reasons,  we will transfer all our data into the new Swiss coordinate system CH1903+ LV95. Transform the cantons record with st_transform() into CH1903+ LV95, using the correct EPSG code.\nBefore transforming the data (consider the attributes Bounding box, Projected CRS as well as the values in the geomcolumn):\n\ncantons\n## Simple feature collection with 51 features and 6 fields\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 5.955902 ymin: 45.81796 xmax: 10.49217 ymax: 47.80845\n## Geodetic CRS:  WGS 84\n## # A tibble: 51 × 7\n##    NAME       KANTONSNUM SEE_FLAECH KANTONSFLA KT_TEIL EINWOHNERZ\n##  * &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;\n##  1 Graubünden         18         NA     710530 0           197888\n##  2 Bern                2      11897     595952 1          1031126\n##  3 Valais             23       1060     522463 0           341463\n##  4 Vaud               22      39097     321201 1           793129\n##  5 Ticino             21       7147     281216 0           353709\n##  6 St. Gallen         17       7720     202820 1           504686\n##  7 Zürich              1       6811     172894 0          1504346\n##  8 Fribourg           10       7818     167142 1           315074\n##  9 Luzern              3       6438     149352 0           406506\n## 10 Aargau             19        870     140380 1           670988\n## # ℹ 41 more rows\n## # ℹ 1 more variable: geom &lt;POLYGON [°]&gt;\n\n\n\nSample Solution\ncantons &lt;- st_transform(cantons, 2056)\n\n\nAfter transferring the data (consider the Bounding box and Projected CRS attributes as well as the values in the geom column):\n\ncantons\n## Simple feature collection with 51 features and 6 fields\n## Geometry type: POLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 2485410 ymin: 1075268 xmax: 2833858 ymax: 1295934\n## Projected CRS: CH1903+ / LV95\n## # A tibble: 51 × 7\n##    NAME       KANTONSNUM SEE_FLAECH KANTONSFLA KT_TEIL EINWOHNERZ\n##  * &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;\n##  1 Graubünden         18         NA     710530 0           197888\n##  2 Bern                2      11897     595952 1          1031126\n##  3 Valais             23       1060     522463 0           341463\n##  4 Vaud               22      39097     321201 1           793129\n##  5 Ticino             21       7147     281216 0           353709\n##  6 St. Gallen         17       7720     202820 1           504686\n##  7 Zürich              1       6811     172894 0          1504346\n##  8 Fribourg           10       7818     167142 1           315074\n##  9 Luzern              3       6438     149352 0           406506\n## 10 Aargau             19        870     140380 1           670988\n## # ℹ 41 more rows\n## # ℹ 1 more variable: geom &lt;POLYGON [m]&gt;",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>SpatAn 1: Exercise A</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan1_Uebung_A.html#task-4-tidyverse-functions",
    "href": "spatan/Spatan1_Uebung_A.html#task-4-tidyverse-functions",
    "title": "SpatAn 1: Exercise A",
    "section": "Task 4: Tidyverse functions",
    "text": "Task 4: Tidyverse functions\nsf objects are essentially data.frames with a few metadata and a special geometry column. We can perform the same operations as with data.frames. For example, we can calculate the population density from the columns EINWOHNERZ and KANTONSFLA:\n\nlibrary(\"dplyr\")  # Add this line to the top of you script\n\ncantons &lt;- cantons |&gt;\n  mutate(\n    # convert hectares to km2\n    area_km2 = KANTONSFLA / 100,\n    # calculate population density per km2\n    population_density = EINWOHNERZ / area_km2\n  )\n\nNow calculate the population density at the level of the municipalities.\n\nmunicipalities &lt;- municipalities |&gt;\n  mutate(\n    area_km2 = GEM_FLAECH / 100,\n    population_density = EINWOHNERZ / area_km2\n  )",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>SpatAn 1: Exercise A</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan1_Uebung_A.html#task-5-chloropleth-maps",
    "href": "spatan/Spatan1_Uebung_A.html#task-5-chloropleth-maps",
    "title": "SpatAn 1: Exercise A",
    "section": "Task 5: Chloropleth Maps",
    "text": "Task 5: Chloropleth Maps\nNow we want to colour the municipalities or the cantons according to their population density. As usual, we use the aes(fill = ...) method from ggplot().\n\n\nSample Solution\nggplot(cantons) +\n  geom_sf(aes(fill = population_density))\n\n\n\n\n\n\n\n\n\nThere are hardly any differences in colour, because the extremely high population density of Basel-Stadt (&gt;5,000 inhabitants per km2!) dominates the entire colour scale. Switzerland’s Statistical Atlas solves the problem by using classes with irregular thresholds and grouping all numbers &gt;2,000. We can reproduce this procedure with cut().\n\n# Threshold is the same as BFS \"Statistical Atlas of Switzerland\"\nbreaks = c(0, 50, 100, 150, 200, 300, 500, 750, 1000, 2000, Inf)\n\n# show classes based on thresholds\ncantons &lt;- cantons |&gt;\n    mutate(population_density_classes = cut(population_density, breaks))\n\np_cantons &lt;- ggplot(cantons, aes(fill = population_density_classes)) +\n  geom_sf(colour = NA) +\n  scale_fill_brewer(palette = \"RdYlGn\",direction = -1) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\nCreate the same classes for the population density of the communities and compare the plots.\n\n\nSample Solution\nmunicipalities &lt;- municipalities |&gt;\n  mutate(population_density_classes = cut(population_density, breaks))\n\np_municipalities &lt;- ggplot(municipalities, aes(fill = population_density_classes)) +\n  geom_sf(colour = NA) +\n  scale_fill_brewer(palette = \"RdYlGn\",direction = -1) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cantons\n\n\n\n\n\n\n\n\n\n\n\n(b) Municipalities\n\n\n\n\n\n\n\nFigure 19.2: Comparing these depictions clearly shows the problems of MAUP",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>SpatAn 1: Exercise A</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan1_Uebung_B.html",
    "href": "spatan/Spatan1_Uebung_B.html",
    "title": "SpatAn 1: Exercise B",
    "section": "",
    "text": "Task 1: Geopackage “Layers”\nFor the upcoming exercise, we will work with the gruental.gpkg. data set: Import it into R.\nYou may have noticed the following warning message when importing the gruental.gpkg geopackage:\nWarning message:\nIn evalq((function (..., call. = TRUE, immediate. = FALSE, noBreaks. = FALSE,  :\n  automatically selected the first layer in a data source containing more than one.\nThis warning message indicates that the geopackage gruental.gpkg has several layers (rep. records) and only the first layer has been imported. Use the st_layers command to find out the layer names and then use them in st_read (as argument layer =) to import the layers individually and store them in variables (e.g., such as in the variables wiesen and baeume).\n\n\nSample Solution\nst_layers(\"datasets/rauman/gruental.gpkg\")\n## Driver: GPKG \n## Available layers:\n##   layer_name geometry_type features fields       crs_name\n## 1     wiesen       Polygon       37      1 CH1903+ / LV95\n## 2     baeume         Point      185      2 CH1903+ / LV95\n\nmeadows &lt;- read_sf(\"datasets/rauman/gruental.gpkg\", \"wiesen\")\ntrees &lt;- read_sf(\"datasets/rauman/gruental.gpkg\", \"baeume\")\n\n\n\n\nTask 2: Understanding the data sets\nTake some time to explore the two datasets. Use the visualisation options of ggplot (especially geom_sf). You can superimpose multiple geom_sf to represent multiple records at the same time.\n\nSample Solution\nlibrary(\"ggplot2\")\n\nggplot(meadows) +\n  geom_sf(aes(fill = flaechen_typ)) +\n  geom_sf(data = trees) +\n  theme_void()\n\nggplot(meadows) +\n  geom_sf() +\n  geom_sf(data = trees, aes(colour = baum_typ)) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nFigure 20.1: Meadow areas are shown in different colours depending on type\n\n\n\n\n\n\n\n\n\n\n\nFigure 20.2: Trees are shown in different colours depending on type\n\n\n\n\n\n\n\n\nTask 3: Spatial join with points\nWe now want to know whether each tree is in a meadow or not. To do this, we use the GIS technique spatial join as described in the lecture. Using sf, we can perform spatial joins with st_join. There are only left and innerjoins (see PrePro 1 & 2). The points must be listed first, since we want to attach attributes to the points.\nNote that the output has a new column: flaechen_typ. This is empty (NA) if the corresponding tree is not in a meadow. How many trees are in a meadow and how many are not?\n\n\nSample Solution\ntrees_join &lt;- st_join(trees, meadows)\n\nnumber_in_meadow &lt;- sum(!is.na(trees_join$flaechen_typ))\nnumber_not_in_meadow &lt;- sum(is.na(trees_join$flaechen_typ))",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>SpatAn 1: Exercise B</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan2_Uebung_A.html",
    "href": "spatan/Spatan2_Uebung_A.html",
    "title": "SpatAn 2: Exercise A",
    "section": "",
    "text": "Task 1\nPlease download the data for todays exercise here: SpatAn2.\nIn the last exercise, we performed a spatial join between trees and meadows to find out whether a tree is in a meadow or not using data from the Grüental campus (gruental.gpkg)\nToday we will go a step further and try to answer the following question: How much meadow is there within 20 metres of each tree?\nTo do this, load the required libraries and records into your session. Explore the data and visualize it spatially.\nIn order to simplify the exercise a little, we will only work with 10 trees at first. Use the code below to randomly select 10 trees. If you use the same “seed” as we have(set.seed(100)), you should also “coincidentally” have the same trees as me.\nAs a first step, we need to apply a 20m buffer to each tree. Use st_buffer to save the output as trees_20m. Now take a close look at trees_20m. What type of geometry does it now represent?\nFigure 21.1: Trees are displayed as points with a 20m buffer. Meadows are displayed in the background.",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>SpatAn 2: Exercise A</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan2_Uebung_A.html#task-2",
    "href": "spatan/Spatan2_Uebung_A.html#task-2",
    "title": "SpatAn 2: Exercise A",
    "section": "Task 2",
    "text": "Task 2\nNow calculate the intersection of trees_20m and meadows with the st_intersection function and save the output as trees_meadows. Next explore trees_meadows. What happened? Check the number of rows per record. Have they changed? If so, why?",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>SpatAn 2: Exercise A</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan2_Uebung_A.html#task-3",
    "href": "spatan/Spatan2_Uebung_A.html#task-3",
    "title": "SpatAn 2: Exercise A",
    "section": "Task 3",
    "text": "Task 3\nNow calculate the area per geometry with the st_area() function. Save the output in a new column called trees_meadows (e.g. with the name meadow_area). Tip: Convert the output from st_area() to a numeric vector with as.numeric().",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>SpatAn 2: Exercise A</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan2_Uebung_A.html#task-4-optional",
    "href": "spatan/Spatan2_Uebung_A.html#task-4-optional",
    "title": "SpatAn 2: Exercise A",
    "section": "Task 4 (Optional)",
    "text": "Task 4 (Optional)\nNow calculate the meadow_share from meadow_area. Tip: The circular area of \\(r^2\\times \\pi\\) is 100%, whereas in our case, \\(r = 20\\).\nThen transfer the calculated proportional values to the trees dataset with a left_ join() between trees and trees_meadows. Which column would be suitable for this join? Note: Use st_drop_geometry() to remove the geometry column in trees_meadows before joining.\n\n\n\n\n\n\n\n\nFigure 21.2: After this exercise, you can visualise the results like this.",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>SpatAn 2: Exercise A</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan2_Uebung_A.html#sec-raster-intro1",
    "href": "spatan/Spatan2_Uebung_A.html#sec-raster-intro1",
    "title": "SpatAn 2: Exercise A",
    "section": "Task 5",
    "text": "Task 5\nBy now you have performed a few vector operations such as st_buffer() and st_intersection() and st_area(). However, certain questions are better answered using the raster format. For example, if we want to know how far the nearest tree is for each point in the room, this can be better represented in a raster.\nHowever, before we can answer that question, we have to convert the vector data set into a raster data set. To do this, a raster “template” is needed so that R knows roughly what the raster output should look like.\n\n\nSample Solution\n# Use the \"terra\" package to work with the raster format.\nlibrary(\"terra\")\n\n# We need a template to vectorise data \n# We will use \"meadows\" as a template and set the cell size (resolution)\ntemplate &lt;- rast(meadows, resolution = 20)\n\n# When we rasterise, we convert \"trees\" into a raster format\n# Use all trees, not trees_sample\ntrees_rast &lt;- terra::rasterize(trees, template)\n\n\nThe difference between raster and vector can be shown very vividly if the two data sets are stored one on top of the other.\n\n\nSample Solution\nplot(trees_rast, col = \"grey\")\nplot(trees, add = TRUE, col = \"red\", pch = \"x\")\n\n\n\n\n\n\n\n\n\nwe can now use the function distance() with trees_rast to calculate the distance to each tree:\n\n\nSample Solution\ntrees_dist &lt;- distance(trees_rast)\nplot(trees_dist)\nplot(trees, add = TRUE, pch = \"x\")",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>SpatAn 2: Exercise A</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan2_Uebung_B.html",
    "href": "spatan/Spatan2_Uebung_B.html",
    "title": "SpatAn 2: Exercise B",
    "section": "",
    "text": "Task 1\nIn the last exercise, you had your first experience with raster data. We first rasterised a vector dataset. However, we often work with spatial data that has already been captured in raster format.\nIn this exercise, we will continue to work with terra to show how we can import, visualise and further process a raster dataset. In your data you will find a dataset called dhm250m.tif, which represents the “Digital Elevation Model” (DHM) of the Canton of Schwyz. Execute the specified code.\nlibrary(\"terra\")\nImport your raster with the rast() function\ndhm_schwyz &lt;- rast(\"datasets/rauman/dhm250m.tif\")\nYou will get some important metadata about the raster data when you enter the variable name in the console.\n## class       : SpatRaster \n## dimensions  : 150, 186, 1  (nrow, ncol, nlyr)\n## resolution  : 250, 250  (x, y)\n## extent      : 2672175, 2718675, 1193658, 1231158  (xmin, xmax, ymin, ymax)\n## coord. ref. : CH1903+ / LV95 (EPSG:2056) \n## source      : dhm250m.tif \n## name        :   dhm250m \n## min value   :  389.1618 \n## max value   : 2850.0203\nTo get a quick overview of a raster record, we can simply use the plot() function.\nUnfortunately, using raster data in ggplot is not very easy. Since ggplot() is a universal plot framework, we quickly reach the limits of what is possible when we create something as special as a map. Plot allows us to work very quickly, but again, it has its limits.\nFor this reason, we will introduce a new plot framework that specialises in maps and is built in a very similar design to ggplot: tmap. Load this package into your session now:\nlibrary(\"tmap\")\nJust like ggplot(), tmap is based on the idea of “layers” connected by a +. Each level has two components:\nNote that tm_shape() and tm_raster() (in this case) cannot exist without each other.\nIf you consult?tm_raster, you will see a variety of options that you can how your data is visualised. For example, the default style of tm_raster() creates “bins” with a discrete colour gamut. We can override this with style = \"cont\".\nThat should look appropriate, but maybe we want to change the default colour palette. Fortunately, this is much easier in tmap than in ggplot2. To view the available palettes, enter tmaptools ::palette_explorer() or RColorBrewer:: display.brewer.all() in the console (for the former, you may need to install additional packages, e.g. shinyjs).\nOne of tmap’s great strengths is the fact that both static and interactive plots can be created with the same command. For this, you need to change the mode from static to interactive.",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>SpatAn 2: Exercise B</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan2_Uebung_B.html#task-1",
    "href": "spatan/Spatan2_Uebung_B.html#task-1",
    "title": "SpatAn 2: Exercise B",
    "section": "",
    "text": "a dataset component that is always tm_shape(dataset) (replace dataset with your variable)\na geometry component that describes how the previous tm_shape() should be visualised. This can be tm_dots() for points, tm_polygons() for polygons, tm_lines() for lines, etc. For single band raster (which is the case with dhm_ schwyz), use tm_raster()",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>SpatAn 2: Exercise B</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan2_Uebung_B.html#sec-raster-slope",
    "href": "spatan/Spatan2_Uebung_B.html#sec-raster-slope",
    "title": "SpatAn 2: Exercise B",
    "section": "Task 2",
    "text": "Task 2\nUsing terra, we can run a variety of raster operations on our elevation model. A classic raster operation is the calculation of a slope’s incline (“slope”) or its orientation (“aspect”). Use the terrain() function from terra to calculate the slope inclination and orientation. Visualise the results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n“aspect” is a value that ranges from 0 to 360. In classic palettes, the two extreme values (in this case 0 and 360) are far apart in terms of colour. In aspect, however, these should be close together (since an orientation of 1° is only 2 degrees away from an orientation of 359°). To take this fact into account, we can create our own colour palette, where the first colour is repeated.\n\n\n\n\nSample Solution\ntm_shape(schwyz_aspect) +\n  tm_raster(\n    palette = c(\"#EF476F\", \"#FFD166\", \"#06D6A0\", \"#118AB2\", \"#EF476F\"),\n    style = \"cont\", breaks = seq(0, 360, 90)\n  )",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>SpatAn 2: Exercise B</span>"
    ]
  },
  {
    "objectID": "spatan/Spatan2_Uebung_B.html#task-3",
    "href": "spatan/Spatan2_Uebung_B.html#task-3",
    "title": "SpatAn 2: Exercise B",
    "section": "Task 3",
    "text": "Task 3\nUsing slope incline and orientation, we can calculate a hill shading effect. Hill shading refers to the shadow cast on the surface model and is calculated at a given angle of the sun (height and azimuth). The typical angle is 45° above the horizon and 315° from the northwest.\nTo create a hill shading effect, first calculate slope and aspect of dhm_schwyz, just like in the previous task, but make sure that the unit corresponds to the radians. Use these two objects in the shade() function to calculate the hill shade. Then visualise the output with plot or tmap.\n\n\n\n\n\nUse tmap for this visualisation along side the cividis colour palette",
    "crumbs": [
      "Spatial Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>SpatAn 2: Exercise B</span>"
    ]
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "Kovic, Marko. 2014. “Je Weniger Ausländer, Desto Mehr Ja-Stimmen?\nWirklich?” Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualize, and Model\nData. 2nd Edition. O’Reilly. https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>References</span>"
    ]
  }
]