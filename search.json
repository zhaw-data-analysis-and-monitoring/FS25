[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis and Monitoring",
    "section": "",
    "text": "Welcome\n\n\n\nThe “Data Analysis and Monitoring” module provides an in-depth overview of the methodological skills required for hands-on research and development in any applied data-related or data-heavy project the Master’s level. Students will refine their methodological expertise by examining the different typical phases of data analysis and modelling, starting from data capture and preprocessing the data, through exploratory analysis and predictive modelling, to visualization and communication in the end. They will also acquire the methodological foundations that will underpin the subsequent modules in the MSc CEM programme. The module provides both general methodological skills that cut across disciplines (e.g., scientific theory, computer-aided data processing, and statistics) and specialised knowledge in the context of circular economy.\nThe materials required for the R exercises are available here, with demo files, exercises and solutions.\nThis website was last updated on 2025-03-25 15:26:58.970776."
  },
  {
    "objectID": "preparation.html#install-r",
    "href": "preparation.html#install-r",
    "title": "Preparation Course",
    "section": "Install R",
    "text": "Install R\nIf you haven’t installed R yet, do so now by getting the newest version from CRAN."
  },
  {
    "objectID": "preparation.html#install-rstudio",
    "href": "preparation.html#install-rstudio",
    "title": "Preparation Course",
    "section": "Install RStudio",
    "text": "Install RStudio\nRStudio is the IDE (integrated development environment) we use in our course to interact with R. Download and install it on your computer."
  },
  {
    "objectID": "preparation.html#configure-rstudio",
    "href": "preparation.html#configure-rstudio",
    "title": "Preparation Course",
    "section": "Configure RStudio",
    "text": "Configure RStudio\nNow we will set some RStudio Global options. But first, close all instances of RStudio and restart it (!!!). Then go to Tools > Global options.\n\nR General\n\nDeactivate the option “Restore .RData into workspace at startup”\nSet “Save workspace to .RData on exit” to “Never”\n\nCode\n\nActivate the option “Use native pipe operator, |> (requires R 4.1+)”\n\nR Markdown\n\nDeactivate the option “Show output inline for all R Markdown documents”\n\n\nClick on “Ok” to apply the change and close the options menu.\n\nFolder structure for this course\nBy this point, you probably have created a folder for this course somewhere on your computer. In our example, we assume this folder is located here: C:/Users/yourname/semester2/Module_DAMO (mentally replace this with your actual path). Before we dive into the exercises, take a minute to think about how you are going to structure your files in this folder. This course will take place over several weeks, and in each week you will receive or produce various files. We recommend creating a separate folder for each week, and one folder for the case studies, like so:\nCourse Folder (C:\\\\Users\\\\yourname\\\\semester2\\\\Module_DAMO)\n ¦--week_1                                                \n ¦--week_2                                                \n ¦--week_3                                                \n |--...                                                \n °--case_studies \nFor the R-exercises we recommend that you create a new RStudio Project each week in subdirectory of the appropriate week. For example, this week your folder structure could look like this:\nFolder Week 1 (C:\\\\Users\\\\yourname\\\\semester2\\\\Module_DAMO\\\\week_1)\n ¦--slides.pdf                                                  \n ¦--my_notes.docx                                               \n ¦--seminar_screenshot.jpg                                      \n °---damo-week1-rexercise                                             \n     ¦--damo-week1-rexercise.Rproj                                   \n     ¦--test.csv                                      \n     °--my_solution.qmd   \nNote:\n\nthe RStudio Project is located in a subfolder of C:/Users/yourname/semester1/Module_DAMO/week_1 and named damo-week1-rexercise.\ndamo-week1-rexercise is the project’s directory name and the project name\nwe realize that damo and the week number is redundant, there is a reason1 for this\nthis means each week is a fresh start (which has pros and cons)\n\n\n\nCreate an RStudio project for the first week\nCreate a new RStudio Project (File > New Project > New Directory > New Project).\n\nClick on “Browse” and switch to your equivalent of the folder C:/Users/yourname/semester1/Module_DAMO/week_1 (the project we are about to initiate will be be created in a subdirectory of this folder). Click on “open” to confirm the selection\nIn the field “Directory name”, type damo-week1-rexercise. This will be the name of your RStudio project and it’s parent directory.\nClick on “Create Project”\n\nYou are all set! You can start working on the tasks of exercise 1."
  },
  {
    "objectID": "PrePro.html",
    "href": "PrePro.html",
    "title": "Pre-Processing",
    "section": "",
    "text": "Data Science 2.0 equips students with the essential knowledge and practical skills needed to prepare and enhance their self-collected or sourced data for analysis (preprocessing). This unit focuses on fundamental data processing skills while tackling common challenges in the processing of environmental science data, all through a practical, ‘hands-on’ approach with R exercises. Students will learn to articulate the characteristics of their data sets using the appropriate technical terminology. They will also learn to interpret metadata and critically assess its implications for their own analysis projects. The lesson emphasises critical concepts such as scale levels, data types, time data, and type conversions.\nThis lesson focuses on the central skills required for preprocessing structured data, a fundamental aspect of environmental science research. It covers combining datasets (joins) and transforming them (“reshape”, “split-apply-combine”). Given that data seldom presents itself in a format ready for statistical analysis or information visualisation, students will master the key concepts and R tools required for these often intricate preprocessing tasks, enabling them to execute them effectively."
  },
  {
    "objectID": "prepro/Prepro0_Vorbereitung.html",
    "href": "prepro/Prepro0_Vorbereitung.html",
    "title": "Preparation",
    "section": "",
    "text": "For Prepro 1 - 3, we will need the following packages: dplyr, ggplot2, lubridate, readr and tidyr. We recommend installing them before the first lesson. Individual packages cann be installed as follows:\n\ninstall.packages(c(\"dplyr\", \"ggplot2\", \"lubridate\", \"readr\", \"tidyr\"))\n\nYou can download the datasets for the exercises from Moodle:\n\n{{<var datasets.PrePro1>}}\n{{<var datasets.PrePro2>}}\n{{<var datasets.PrePro3>}}"
  },
  {
    "objectID": "prepro/Prepro1_Demo.html#data-types",
    "href": "prepro/Prepro1_Demo.html#data-types",
    "title": "Prepro 1: Demo",
    "section": "Data types",
    "text": "Data types\n\nDoubles\nThere are two different numeric data types in R:\n\ndouble: floating-point number (e.g. 10.3, 7.3)\ninteger (e.g. 10, 7)\n\nA double / floating point number is assigned to a variable as follows:\n\nx <- 10.3\n\nx\n\n[1] 10.3\n\nclass(x)\n\n[1] \"numeric\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nEither <- or = can be used. However, the latter is also easily confused with ==.\n\ny = 7.3\ny\n\n[1] 7.3\n\n\n\n\n\n\nInteger\nA number is only stored as an integer if it is explicitly defined as one (using as.integer() or L).\n\nd <- 8L\n\nclass(d)\n\n[1] \"integer\"\n\n\n\n\nBoolean\n\nsunny <- FALSE\ndry <- TRUE\n\nsunny & dry\n\n[1] FALSE\n\n\n\ne <- 3\nf <- 6\n\ne > f\n\n[1] FALSE\n\n\n\n\nCharacter\nCharacter strings contain text.\n\nfname <- \"Andrea\"\nlname <- \"Muster\"\nclass(fname)\n\n[1] \"character\"\n\n\nConnecting / concatenating character strings\n\npaste(fname, lname)\n\n[1] \"Andrea Muster\"\n\npaste(fname, lname, sep = \",\")\n\n[1] \"Andrea,Muster\"\n\n\n\n\n\nDate / time\n\n\n\nIn most parts of the world, we use the Gregorian Calendar to communicate a point in time. In this system, we track time as years, months, days, hours, minutes and seconds after a specific event (Anno Domini, “in the year of the Lord”).\nR, just as all other computer systems, do not store date / time information using years, months days etc. Instead, R stores the number of seconds after a given date (January 1st, 1970, which is also called unix epoch). This information is stored using the class POSIXct, which also helps us convert this number of seconds into more human readable information. On {r} now_pretty, {r} n_secs_pretty have passed since the unix epoch, so to store this timestamp, R stores the number {r} n_secs_pretty.\n\n# We may have a timestamp saved as a character string\ntoday_txt <- \"2024-02-01 13:45:00\"\n\n# as.POSIXct converts the string to POSIXct:\ntoday_posixct <- as.POSIXct(today_txt)\n\n# When printing a posixct date to the console, it is human readable\ntoday_posixct\n\n[1] \"2024-02-01 13:45:00 CET\"\n\n# To see the internally stored value (# of seconds), convert it to numeric:\nas.numeric(today_posixct)\n\n[1] 1706791500\n\n\nIf the character string is delivered in the above format (year-month-day hour:minute:second), as.POSIXct knows how to caluate the number of seconds since unix epoch. However, if the format is different, we have to tell R how to read our timestamp. This requires a special syntax, which is described in ?strptime.\n\ndate_txt <- \"01.10.2017 15:15\"\n\n# converts character to POSIXct:\nas.POSIXct(date_txt)\n\nError in as.POSIXlt.character(x, tz, ...): character string is not in a standard unambiguous format\n\ndate_posix <- as.POSIXct(date_txt, format = \"%d.%m.%Y %H:%M\")\n\ndate_posix\n\n[1] \"2017-10-01 15:15:00 CEST\"\n\n\nTheoretically, strftime can also be used to extract specific components from a date. However, the functions from lubridate are much simpler and we recommend you use these. Note how strftime always returns strings while lubridate returns more useful datatypes such as integers or factors.\n\nstrftime(date_posix, format = \"%m\")           # <1>\nstrftime(date_posix, format = \"%b\")           # <2>\nstrftime(date_posix, format = \"%B\")           # <3>\n\n\nextracts the month as a number\nextracts the month by name (abbreviated)\nextracts the month by name (full)\n\n\nlibrary(\"lubridate\")\n\nmonth(date_posix)                             # <1>\nmonth(date_posix, label = TRUE, abbr = TRUE)  # <2>\nmonth(date_posix, label = TRUE, abbr = FALSE) # <3>\n\n\nextracts the month as a number\nextracts the month by name (abbreviated)\nextracts the month by name (full)\n\n\n\n\n\n\n\nTime is hard\n\n\n\nHandling date / time is tricky. We recommend the following practices to make life easier:\n\nAlways store time as POSIXct, not as text.\nAlways store time together with its according date, never separately.\nIf you must extract time (e.g. to analyse daily patterns), store it as decimal time (e.g. store 15:45 as 15.75) in a numeric data type.\nTry to be explicit about which timezone your data originates from\nIf your observation period is affected by switching to or from daylight saving time, think about converting time to UTC\nUse lubridate rather than strftime()"
  },
  {
    "objectID": "prepro/Prepro1_Demo.html#data-structures",
    "href": "prepro/Prepro1_Demo.html#data-structures",
    "title": "Prepro 1: Demo",
    "section": "Data structures",
    "text": "Data structures\n\nVectors\nUsing c(), a set of values of the same data type can be assigned to a variable (as a vector).\n\nvec <- c(10, 20, 33, 42, 54, 66, 77)\nvec\n\n[1] 10 20 33 42 54 66 77\n\n# to extract the 5th element\nvec[5]\n\n[1] 54\n\n# to extract elements 2 to 4\nvec[2:4]\n\n[1] 20 33 42\n\n\n\n\nLists\nA list is a collection of objects that do not need to be the same data type.\n\nmylist <- list(\"q\", TRUE, 3.14)\n\nThe individual elements in a list can also have assigned names.\n\nmylist2 <- list(fav_letter = \"q\", fav_boolean = TRUE, fav_number = 3.14)\n\nmylist2\n\n$fav_letter\n[1] \"q\"\n\n$fav_boolean\n[1] TRUE\n\n$fav_number\n[1] 3.14\n\n\n\n\nData frames\nIf each entry in a list is the same length, this list can also be represented as a table, which is called a dataframe in R.\n\n# note how the names become column names\nas.data.frame(mylist2)\n\n  fav_letter fav_boolean fav_number\n1          q        TRUE       3.14\n\n\nThe data.frame function allows a table to be created without first having to create a list.\n\ndf <- data.frame(\n  City = c(\"Zurich\", \"Geneva\", \"Basel\", \"Bern\", \"Lausanne\"),\n  Arrival = c(\n    \"1.1.2017 10:10\", \"5.1.2017 14:45\",\n    \"8.1.2017 13:15\", \"17.1.2017 18:30\", \"22.1.2017 21:05\"\n  )\n)\n\nstr(df)\n\n'data.frame':   5 obs. of  2 variables:\n $ City   : chr  \"Zurich\" \"Geneva\" \"Basel\" \"Bern\" ...\n $ Arrival: chr  \"1.1.2017 10:10\" \"5.1.2017 14:45\" \"8.1.2017 13:15\" \"17.1.2017 18:30\" ...\n\n\nThe $ symbol can be used to query data:\n\ndf$City\n\n[1] \"Zurich\"   \"Geneva\"   \"Basel\"    \"Bern\"     \"Lausanne\"\n\n\nNew columns can be added and existing ones can be changed:\n\ndf$Residents <- c(400000, 200000, 175000, 14000, 130000)\n\n\n\n# A tibble: 5 × 3\n  City     Arrival         Residents\n  <chr>    <chr>               <dbl>\n1 Zurich   1.1.2017 10:10     400000\n2 Geneva   5.1.2017 14:45     200000\n3 Basel    8.1.2017 13:15     175000\n4 Bern     17.1.2017 18:30     14000\n5 Lausanne 22.1.2017 21:05    130000\n\n\nWe need to convert the Arrival time to a time format (POSIXct).\n\n# first, test the output of the \"as.POSIXct\"-function\nas.POSIXct(df$Arrival, format = \"%d.%m.%Y %H:%M\")\n\n[1] \"2017-01-01 10:10:00 CET\" \"2017-01-05 14:45:00 CET\"\n[3] \"2017-01-08 13:15:00 CET\" \"2017-01-17 18:30:00 CET\"\n[5] \"2017-01-22 21:05:00 CET\"\n\n# if it works, we can save the output to a new column\ndf$Arrival_ct <- as.POSIXct(df$Arrival, format = \"%d.%m.%Y %H:%M\")\n\n\n# We *could* overwrite the old column, but this is a destructive operation!\n\nThese columns can now help to create convenience variables. E.g., the arrival time can be derived from the Arrival column.\n\ndf$Arrival_day <- wday(df$Arrival_ct, label = TRUE, week_start = 1)\n\ndf$Arrival_day\n\n[1] Sun Thu Sun Tue Sun\nLevels: Mon < Tue < Wed < Thu < Fri < Sat < Sun"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#working-with-rstudio-project",
    "href": "prepro/Prepro1_Uebung.html#working-with-rstudio-project",
    "title": "PrePro 1: Exercise",
    "section": "Working with RStudio “Project”",
    "text": "Working with RStudio “Project”\nWe recommend using “Projects” within RStudio. RStudio then creates a folder for each project in which the project file is stored (file extension .rproj). If Rscripts are loaded or generated within the project, they are then also stored in the project folder. You can find out more about RStudio Projects here.\nThere are several benefits to using Projects. You can:\n\nspecify the Working Directory without using an explicit path (setwd()). This is useful because the path can change (when collaborating with other users, or executing the script at a later date)\nautomatically cache open scripts and restore open scripts in the next session\nset different project-specific options\nuse version control systems (e.g., git)"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-1",
    "href": "prepro/Prepro1_Uebung.html#task-1",
    "title": "PrePro 1: Exercise",
    "section": "Task 1",
    "text": "Task 1\nCreate a data.frame with the following data. Tipp: Create a vector for each column first.\n\n\nSample Solution\ndf <- data.frame(\n  Species = c(\"Fox\", \"Bear\", \"Rabbit\", \"Moose\"),\n  Number = c(2, 5, 1, 3),\n  Weight = c(4.4, 40.3, 1.1, 120),\n  Sex = c(\"m\", \"f\", \"m\", \"m\"),\n  Description = c(\"Reddish\", \"Brown, large\", \"Small, with long ears\", \"Long legs, shovel antlers\")\n)\n\n\n\n\n\n\n\nSpecies\nNumber\nWeight\nSex\nDescription\n\n\n\n\nFox\n2\n4.4\nm\nReddish\n\n\nBear\n5\n40.3\nf\nBrown, large\n\n\nRabbit\n1\n1.1\nm\nSmall, with long ears\n\n\nMoose\n3\n120.0\nm\nLong legs, shovel antlers"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-2",
    "href": "prepro/Prepro1_Uebung.html#task-2",
    "title": "PrePro 1: Exercise",
    "section": "Task 2",
    "text": "Task 2\nWhat types of data were automatically accepted in the last task? Check this using str(), see whether they make sense and convert where necessary.\n\n\nSample Solution\nstr(df)\n## 'data.frame':    4 obs. of  5 variables:\n##  $ Species    : chr  \"Fox\" \"Bear\" \"Rabbit\" \"Moose\"\n##  $ Number     : num  2 5 1 3\n##  $ Weight     : num  4.4 40.3 1.1 120\n##  $ Sex        : chr  \"m\" \"f\" \"m\" \"m\"\n##  $ Description: chr  \"Reddish\" \"Brown, large\" \"Small, with long ears\" \"Long legs, shovel antlers\"\ntypeof(df$Number)\n## [1] \"double\"\n# Number was interpreted as `double`, but it is actually an `integer`.\n\ndf$Number <- as.integer(df$Number)\n\n# We know sex only has two options:\ndf$Sex <- factor(df$Sex, levels = c(\"m\",\"f\"))"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#input-libraries-packages",
    "href": "prepro/Prepro1_Uebung.html#input-libraries-packages",
    "title": "PrePro 1: Exercise",
    "section": "Input: Libraries / packages",
    "text": "Input: Libraries / packages\nLibraries (aka packages) are are “extensions” to the basic R functionality. R packages have become indispensable to using R. The vast majority of packages are hosted on CRAN and can be easily installed using install.packages(\"packagename\"). This installation is done once. To use the library, you must load it into the current R session using library(packagename).\nE.g. To import data, we recommend using the readr package1. Install the package using the command install.package(\"readr\"). To use the package, load it into the current R session using library(\"readr\")."
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-3",
    "href": "prepro/Prepro1_Uebung.html#task-3",
    "title": "PrePro 1: Exercise",
    "section": "Task 3",
    "text": "Task 3\nOn Moodle, you will find a folder called Datasets. Download the file and move it in your project folder. Import the weather.csv file. If you use the RStudio GUI for this, save the import command in your R-Script. Please use a relative path (i.e., not a path starting with C:\\, or similar).)\n\n\nSample Solution\n\nlibrary(\"readr\")\n\n\nweather <- read_delim(\"datasets/prepro/weather.csv\", \",\")\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000010100\n-2.6\n\n\nABO\n2000010101\n-2.5\n\n\nABO\n2000010102\n-3.1\n\n\nABO\n2000010103\n-2.4\n\n\nABO\n2000010104\n-2.5\n\n\nABO\n2000010105\n-3.0\n\n\nABO\n2000010106\n-3.7\n\n\nABO\n2000010107\n-4.4\n\n\nABO\n2000010108\n-4.1\n\n\nABO\n2000010109\n-4.1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-4",
    "href": "prepro/Prepro1_Uebung.html#task-4",
    "title": "PrePro 1: Exercise",
    "section": "Task 4",
    "text": "Task 4\nHave a look at your dataset in the console. Have the data been interpreted correctly?\n\n\nSample Solution\n# The 'time' column was interpreted as 'integer'. However, it is \n# obviously a time indication."
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-5",
    "href": "prepro/Prepro1_Uebung.html#task-5",
    "title": "PrePro 1: Exercise",
    "section": "Task 5",
    "text": "Task 5\nThe time column is a date/time with a format of YYYYMMDDHH. In order for R to recognise the data in this column as date/time, it must be in the correct format (POSIXct). Therefore, we must tell R what the current format is. Use as.POSIXct() to read the column into R, remembering to specify both format and tz.\n\n\n\n\n\n\nTip\n\n\n\n\nIf no time zone is set, as.POSIXct() sets a default (based on sys.timezone()). In our case, however, these are values in UTC (see metadata.csv)\nas.POSIXct requires a character input: If you receive the error message 'origin' must be supplied (or similar), you have probably tried to input a numeric into the function with.\n\n\n\n\n\nSample Solution\nweather$time <- as.POSIXct(as.character(weather$time), format = \"%Y%m%d%H\", tz = \"UTC\")\n\n\n\n\n\nThe new table should look like this\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\n\n\nABO\n2000-01-01 01:00:00\n-2.5\n\n\nABO\n2000-01-01 02:00:00\n-3.1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\n\n\nABO\n2000-01-01 04:00:00\n-2.5\n\n\nABO\n2000-01-01 05:00:00\n-3.0\n\n\nABO\n2000-01-01 06:00:00\n-3.7\n\n\nABO\n2000-01-01 07:00:00\n-4.4\n\n\nABO\n2000-01-01 08:00:00\n-4.1\n\n\nABO\n2000-01-01 09:00:00\n-4.1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#task-6",
    "href": "prepro/Prepro1_Uebung.html#task-6",
    "title": "PrePro 1: Exercise",
    "section": "Task 6",
    "text": "Task 6\nCreate two new columns for day of week (Monday, Tuesday, etc) and calendar week. Use the newly created POSIXct column and a suitable function from lubridate.\n\n\nSample Solution\n\nlibrary(\"lubridate\")\n\n\nweather$weekday <- wday(weather$time, label = T)\nweather$week <- week(weather$time)\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\nweekday\nweek\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSat\n1\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSat\n1\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSat\n1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSat\n1\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSat\n1\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSat\n1\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSat\n1\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSat\n1\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSat\n1\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSat\n1"
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#piping",
    "href": "prepro/Prepro2_Demo.html#piping",
    "title": "Prepro 2: Demo",
    "section": "Piping",
    "text": "Piping\nWe want to extract the temperature data from a character string (temperature), and then convert the Kelvin value into Celsius according to the following formula, before finally calculating the mean of all the values:\n\\[°C = K - 273.15\\]\n\n# these temperature are in Kelvin:\ntemperature <- c(\"310\",\"322\",\"348\")\n\ntemperature\n## [1] \"310\" \"322\" \"348\"\n\nTranslated into R-code, this results in the following operation:\n\n\nsubtract <- function(x,y){x-y} # helperfunction to subtract y from x\n\noutput <- mean(subtract(as.integer(temperature), 273.15))\n#                                 \\___1_____/\n#                       \\_____________2_______/\n#              \\______________________3________________/\n#         \\___________________________4_________________/\n\n# 1. Take temperature\n# 2. Convert \"character\" → \"integer\"\n# 4. Subtract 273.15\n# 5. Calculate the mean\n\nThe whole operation is easier to read if it is written down sequentially:\n\ntmp <- as.integer(temperature)   # 2\ntmp <- subtract(tmp, 273.15)     # 3\noutput <- mean(tmp)              # 4\noutput\n## [1] 53.51667\n\nThe fact that the intermediate results must always be saved and retrieved again in the subsequent operation makes this somewhat cumbersome. This is where “piping” comes into play: It makes the output of one function the first parameter of the subsequent function.\n\ntemperature |>        # 1\n  as.integer() |>     # 2\n  subtract(273.15) |> # 3\n  mean()              # 4\n## [1] 53.51667\n\n\n\n\n\n\n\nImportant\n\n\n\n\nthe |> pipe operator was first introduced in R 4.1\nIn addition to the base R pipe operator, there is also a very similar1 pipe operator, %>%, in the magrittr package.\nThe Ctrl +Shift+M keyboard shortcut in RStudio inserts a pipe operator.\nBy checking the Use native pipe operator setting in RStudio Settings Tools → Global Options → Code, you can control which pipe operator, |> or %>%, is inserted with the above key combination.\nWe recommend using the base-R pipe operator |>"
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#joins",
    "href": "prepro/Prepro2_Demo.html#joins",
    "title": "Prepro 2: Demo",
    "section": "Joins",
    "text": "Joins\n\nstudents <- data.frame(\n  Matriculation_No = c(100002, 100003, 200003),\n  Student = c(\"Patrick\", \"Manuela\", \"Eva\"),\n  ZIP = c(8006, 8001, 8820)\n)\n\nstudents\n##   Matriculation_No Student  ZIP\n## 1           100002 Patrick 8006\n## 2           100003 Manuela 8001\n## 3           200003     Eva 8820\n\nlocalities <- data.frame(\n  ZIP = c(8003, 8006, 8810, 8820),\n  LocalityName = c(\"Zurich\", \"Zurich\", \"Horgen\", \"Wadenswil\")\n)\n\nlocalities\n##    ZIP LocalityName\n## 1 8003       Zurich\n## 2 8006       Zurich\n## 3 8810       Horgen\n## 4 8820    Wadenswil\n\n\n# Load library\nlibrary(\"dplyr\")\n\ninner_join(students, localities, by = \"ZIP\")\n##   Matriculation_No Student  ZIP LocalityName\n## 1           100002 Patrick 8006       Zurich\n## 2           200003     Eva 8820    Wadenswil\n\nleft_join(students, localities, by = \"ZIP\")\n##   Matriculation_No Student  ZIP LocalityName\n## 1           100002 Patrick 8006       Zurich\n## 2           100003 Manuela 8001         <NA>\n## 3           200003     Eva 8820    Wadenswil\n\nright_join(students, localities, by = \"ZIP\")\n##   Matriculation_No Student  ZIP LocalityName\n## 1           100002 Patrick 8006       Zurich\n## 2           200003     Eva 8820    Wadenswil\n## 3               NA    <NA> 8003       Zurich\n## 4               NA    <NA> 8810       Horgen\n\nfull_join(students, localities, by = \"ZIP\")\n##   Matriculation_No Student  ZIP LocalityName\n## 1           100002 Patrick 8006       Zurich\n## 2           100003 Manuela 8001         <NA>\n## 3           200003     Eva 8820    Wadenswil\n## 4               NA    <NA> 8003       Zurich\n## 5               NA    <NA> 8810       Horgen\n\n\nstudents <- data.frame(\n  Matriculation_No = c(100002, 100003, 200003),\n  Student = c(\"Patrick\", \"Manuela\", \"Pascal\"),\n  Residence = c(8006, 8001, 8006)\n)\n\nleft_join(students, localities, by = c(\"Residence\" = \"ZIP\"))\n##   Matriculation_No Student Residence LocalityName\n## 1           100002 Patrick      8006       Zurich\n## 2           100003 Manuela      8001         <NA>\n## 3           200003  Pascal      8006       Zurich"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-1",
    "href": "prepro/Prepro2_Uebung_A.html#task-1",
    "title": "Prepro 2: Exercise A",
    "section": "Task 1",
    "text": "Task 1\nRead the weather data from last week weather.csv (source MeteoSchweiz) into R. Make sure that the columns are formatted correctly (stn as a factor, time as POSIXct, tre200h0 as a numeric).\n\n\nSample Solution\n\nlibrary(\"readr\")\n\nweather <- read_delim(\"datasets/prepro/weather.csv\", \",\")\nweather$stn <- as.factor(weather$stn)\nweather$time <- as.POSIXct(as.character(weather$time), format = \"%Y%m%d%H\", tz = \"UTC\")"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-2",
    "href": "prepro/Prepro2_Uebung_A.html#task-2",
    "title": "Prepro 2: Exercise A",
    "section": "Task 2",
    "text": "Task 2\nRead in the metadata.csv dataset as a csv.\n\n\n\n\n\n\nTip\n\n\n\nIf umlauts and special characters are not displayed correctly (e.g. the è in Gèneve), this probably has something to do with the character encoding. The file is currently encoded in UTF-8. If special characters are not correctly displayed, R has not recognised this encoding and it must be specified in the import function. How this is done depends on the import function used:\n\nPackage functions: readr: locale = locale(encoding = \"UTF-8\")\nBase-R functions: fileEncoding = \"UTF-8\"\n\nNote: If you have a file where you do not know how a file is encoded, the following instructions for Windows, Mac and Linux will help.\n\n\n\n\nSample Solution\nmetadata <- read_delim(\"datasets/prepro/metadata.csv\", \";\", locale = locale(encoding = \"UTF-8\"))"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-3",
    "href": "prepro/Prepro2_Uebung_A.html#task-3",
    "title": "Prepro 2: Exercise A",
    "section": "Task 3",
    "text": "Task 3\nNow we want to enrich the weather data set with information from metadata. However, we are only interested in the station abbreviation, the name, the x/y coordinates and the sea level. Select these columns.\n\n\nSample Solution\nmetadata <- metadata[, c(\"stn\", \"Name\", \"x\", \"y\", \"Meereshoehe\")]"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-4",
    "href": "prepro/Prepro2_Uebung_A.html#task-4",
    "title": "Prepro 2: Exercise A",
    "section": "Task 4",
    "text": "Task 4\nNow the metadata can be connected to the weather data set. Which join should we use to do this? And, which attribute can we join?\nUse the join options in dplyr (help via? dplyr::join) to connect the weather data set and the metadata.\n\n\nSample Solution\n\nlibrary(\"dplyr\")\n\n\nweather <- left_join(weather, metadata, by = \"stn\")\n\n# Join type: Left-Join on 'weather', as we are only interested in the stations in the 'weather' dataset.\n# Attribute: \"stn\""
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-5",
    "href": "prepro/Prepro2_Uebung_A.html#task-5",
    "title": "Prepro 2: Exercise A",
    "section": "Task 5",
    "text": "Task 5\nCreate a new month column (from time). To do this, use the lubridate::month() function.\n\n\nSample Solution\n\nlibrary(\"lubridate\")\n\n\nweather$month <- month(weather$time)"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#task-6",
    "href": "prepro/Prepro2_Uebung_A.html#task-6",
    "title": "Prepro 2: Exercise A",
    "section": "Task 6",
    "text": "Task 6\nUse the month column to calculate the average temperature per month.\n\n\nSample Solution\nmean(weather$tre200h0[weather$month == 1])\n## [1] -1.963239\nmean(weather$tre200h0[weather$month == 2])\n## [1] 0.3552632\nmean(weather$tre200h0[weather$month == 3])\n## [1] 2.965054\n\n# etc. for all 12 months"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#task-1",
    "href": "prepro/Prepro2_Uebung_B.html#task-1",
    "title": "Prepro 2: Exercise B",
    "section": "Task 1",
    "text": "Task 1\nYou have data from three sensors (sensor1.csv, sensor2.csv, sensor3.csv). Read in the data sets using the library readr.\n\n\nSample Solution\n\nlibrary(\"readr\")\n\n\nsensor1 <- read_delim(\"datasets/prepro/sensor1.csv\", \";\")\nsensor2 <- read_delim(\"datasets/prepro/sensor2.csv\", \";\")\nsensor3 <- read_delim(\"datasets/prepro/sensor3.csv\", \";\")"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#task-2",
    "href": "prepro/Prepro2_Uebung_B.html#task-2",
    "title": "Prepro 2: Exercise B",
    "section": "Task 2",
    "text": "Task 2\nFrom the 3 data frames, create a single data frame that looks like the one shown below. Use two joins from dplyr to connect 3 data.frames. Then tidy up the column names (how can we do that?).\n\n\nSample Solution\n\nlibrary(\"dplyr\")\n\n\nsensor1_2 <- full_join(sensor1, sensor2, \"Datetime\")\n\nsensor1_2 <- rename(sensor1_2, sensor1 = Temp.x, sensor2 = Temp.y)\n\nsensor_all <- full_join(sensor1_2, sensor3, by = \"Datetime\")\n\nsensor_all <- rename(sensor_all, sensor3 = Temp)\n\n\n\n\n\n\n\nDatetime\nsensor1\nsensor2\nsensor3\n\n\n\n\n16102017_1800\n23.5\n13.5\n26.5\n\n\n17102017_1800\n25.4\n24.4\n24.4\n\n\n18102017_1800\n12.4\n22.4\n13.4\n\n\n19102017_1800\n5.4\n12.4\n7.4\n\n\n23102017_1800\n23.5\n13.5\nNA\n\n\n24102017_1800\n21.3\n11.3\nNA"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#task-3",
    "href": "prepro/Prepro2_Uebung_B.html#task-3",
    "title": "Prepro 2: Exercise B",
    "section": "Task 3",
    "text": "Task 3\nImport the sensor_fail.csv file into R.\n\n\nSample Solution\nsensor_fail <- read_delim(\"datasets/prepro/sensor_fail.csv\", delim = \";\")\n\n\nsensor_fail.csv has a variable SensorStatus: 1 means the sensor is measuring, 0 means the sensor is not measuring. If sensor status = 0, the Temp = 0 value is incorrect. It should be NA (not available). Correct the dataset accordingly.\n\n\n\n\n\nSensor\nTemp\nHum_%\nDatetime\nSensorStatus\n\n\n\n\nSen102\n0.6\n98\n16102017_1800\n1\n\n\nSen102\n0.3\n96\n17102017_1800\n1\n\n\nSen102\n0.0\n87\n18102017_1800\n1\n\n\nSen102\n0.0\n86\n19102017_1800\n0\n\n\nSen102\n0.0\n98\n23102017_1800\n0\n\n\nSen102\n0.0\n98\n24102017_1800\n0\n\n\nSen102\n0.0\n96\n25102017_1800\n1\n\n\nSen103\n-0.3\n87\n26102017_1800\n1\n\n\nSen103\n-0.7\n98\n27102017_1800\n1\n\n\nSen103\n-1.2\n98\n28102017_1800\n1\n\n\n\n\n\n\n\nSample Solution\n# with base-R:\nsensor_fail$Temp_correct[sensor_fail$SensorStatus == 0] <- NA\nsensor_fail$Temp_correct[sensor_fail$SensorStatus != 0] <- sensor_fail$Temp # Warning message can be ignored.\n\n# the same with dplyr:\nsensor_fail <- sensor_fail |>\n  mutate(Temp_correct = ifelse(SensorStatus == 0, NA, Temp))"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#task-4",
    "href": "prepro/Prepro2_Uebung_B.html#task-4",
    "title": "Prepro 2: Exercise B",
    "section": "Task 4",
    "text": "Task 4\nWhy does it matter if 0 or NA is recorded? Calculate the mean of the temperature / humidity after you have corrected the dataset.\n\n\nSample Solution\n# Mean values of the incorrect sensor data: 0 flows into the calculation\n# and distorts the mean\nmean(sensor_fail$Temp)\n## [1] -0.13\n\n# Mean values of the corrected sensor data: with na.rm = TRUE,\n# NA values are removed from the calculation.\nmean(sensor_fail$Temp_correct, na.rm = TRUE)\n## [1] -0.1857143"
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#load-data",
    "href": "prepro/Prepro3_Demo.html#load-data",
    "title": "Prepro 3: Demo",
    "section": "Load data",
    "text": "Load data\nLets load the weather data (source MeteoSchweiz) from the last exercise.\n\n\nlibrary(\"readr\")\n\nweather <- read_delim(\"datasets/prepro/weather.csv\", \",\")\n\n\nweather$stn <- as.factor(weather$stn)\n\n# only overwrite \"time\" if you are sure it worked!\nweather$time2 <- as.POSIXct(as.character(weather$time), format = \"%Y%m%d%H\", tz = \"UTC\")\n\nTranslated into English, the above operation is as follows:\n\nTake the column time from the weather dataset\nConvert it to character\nConvert it to POSIXct using a specified format and timezone\n\nThe translation from R → English looks different because we read the operation in a concatenated form in English (operation 1 → 2 → 3) while the computer reads it as a nested operation 3(2(1)). To make R closer to English, you can use the |> operator (see Wickham, Çetinkaya-Rundel, and Grolemund 2023, chap. 4.3).\n\n\nweather$time2 <- weather$time |> \n  as.character() |> \n  as.POSIXct(format = \"%Y%m%d%H\", tz = \"UTC\")\n\nOnce we are sure the conversion worked, we can overwrite time and remove time2\n\nweather$time <- weather$time2\n\nweather$time2 <- NULL\n\nNow, let’s look at another example."
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#calculate-values",
    "href": "prepro/Prepro3_Demo.html#calculate-values",
    "title": "Prepro 3: Demo",
    "section": "Calculate values",
    "text": "Calculate values\nWe would like to calculate the average of all measured temperature values. To do this, we could use the following command:\n\nmean(weather$tre200h0, na.rm = TRUE)\n## [1] 6.324744\n\nThe option na.rm = TRUE means that NA values should be excluded from the calculation.\nVarious values can be calculated using the same approach (e.g. the maximum (max()), minimum (min()), median (median()) and much more).\nThis approach only works well if we want to calculate values across all observations for a variable (column). As soon as we want to group the observations, it becomes difficult. For example, if we want to calculate the average temperature per month."
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#convenience-variables",
    "href": "prepro/Prepro3_Demo.html#convenience-variables",
    "title": "Prepro 3: Demo",
    "section": "Convenience Variables",
    "text": "Convenience Variables\nTo solve this task, the month must first be extracted (the month is the convenience variable). For this we need the lubridate::month() function.\nNow the month convenience variable can be created. Without using dplyr, a new column can be added as follows:\n\nlibrary(\"lubridate\")\n\nweather$month <- month(weather$time)\n\nWith dplyr the same command looks like this:\n\nlibrary(\"dplyr\")\n\n\nweather <- mutate(weather, month = month(time))\n\nThe main advantage of dplyr is not yet apparent at this point. However, this will become clear later."
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#calculate-values-from-groups",
    "href": "prepro/Prepro3_Demo.html#calculate-values-from-groups",
    "title": "Prepro 3: Demo",
    "section": "Calculate values from groups",
    "text": "Calculate values from groups\nTo calculate the average value per month with base R, you can first create a subset with [] and calculate the average value as follows:\n\nmean(weather$tre200h0[weather$month == 1], na.rm = TRUE)\n## [1] -1.963239\n\nWe have to repeat this every month, which of course is very cumbersome. That is why we use the dplyr package. This, allows us to complete the task (calculate temperature means per month) as follows:\n\nsummarise(group_by(weather, month), temp_average = mean(tre200h0, na.rm = TRUE))\n## # A tibble: 12 × 2\n##    month temp_average\n##    <dbl>        <dbl>\n##  1     1       -1.96 \n##  2     2        0.355\n##  3     3        2.97 \n##  4     4        4.20 \n##  5     5       11.0  \n##  6     6       12.4  \n##  7     7       13.0  \n##  8     8       15.0  \n##  9     9        9.49 \n## 10    10        8.79 \n## 11    11        1.21 \n## 12    12       -0.898"
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#concatenate-vs.-nest",
    "href": "prepro/Prepro3_Demo.html#concatenate-vs.-nest",
    "title": "Prepro 3: Demo",
    "section": "Concatenate vs. Nest",
    "text": "Concatenate vs. Nest\nAgain, translated into English, the above operation is as follows:\n\nTake the weather dataset\nForm groups per year (group_by(weather, year))\nCalculate the mean temperature (mean(tre200h0))\n\nThe translation from R → English looks different because we read the operation in a concatenated form in English (operation 1 → 2 → 3) while the computer reads it as a nested operation 3(2(1)). To make R closer to English, you can use the |> operator (see Wickham, Çetinkaya-Rundel, and Grolemund 2023, chap. 4.3).\n\n# 1 take the dataset \"weather\"\n# 2 form groups per month\n# 3 calculate the average temperature\n\nsummarise(group_by(weather, month), temp_average = mean(tre200h0))\n#                  \\__1__/\n#         \\___________2__________/\n# \\___________________3________________________________________/\n\n# becomes:\n\nweather |>                                 # 1\n  group_by(month) |>                       # 2\n  summarise(temp_average = mean(tre200h0)) # 3\n\nThis concatenation by means of |> (called pipe) makes the code a lot easier to write and read, and we will use it in the following exercises. Pipe is provided as part of the magrittr package and installed with dplyr. There are several online tutorials about dplyr (see Wickham, Çetinkaya-Rundel, and Grolemund 2023, Part “Transform” or this youtube tutorial)\nTherefore, we will not explain all of these tools in full detail. Instead we will just focus on the important differences for two main functions in dpylr: mutate() and summarise().\n\nsummarise() summarises a data set. The number of observations (rows) is reduced to the number of groups (e.g., one summarised observation (row) per year). In addition, the number of variables (columns) is reduced to those specified in the “summarise” function (e.g., temp_mean).\nmutate adds additional variables (columns) to a data.frame (see example below).\n\n\n# Maximum and minimum temperature per calendar week\nweather_summary <- weather |>               # 1) take the dataset \"weather\"\n  filter(month == 1) |>                     # 2) filter for the month of January\n  mutate(day = day(time)) |>                # 3) create a new column \"day\"\n  group_by(day) |>                          # 4) Use the new column to form groups\n  summarise(\n    temp_max = max(tre200h0, na.rm = TRUE), # 5) Calculate the maximum\n    temp_min = min(tre200h0, na.rm = TRUE)  # 6) Calculate the minimum\n  )\n\nweather_summary\n## # A tibble: 31 × 3\n##      day temp_max temp_min\n##    <int>    <dbl>    <dbl>\n##  1     1      5.8     -4.4\n##  2     2      2.8     -4.3\n##  3     3      4.2     -3.1\n##  4     4      4.7     -2.8\n##  5     5     11.4     -0.6\n##  6     6      6.7     -1.6\n##  7     7      2.9     -2.8\n##  8     8      0.2     -3.6\n##  9     9      2.1     -8.8\n## 10    10      1.6     -2.4\n## # ℹ 21 more rows\n\n\n\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd Edition. O’Reilly. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-1",
    "href": "prepro/Prepro3_Uebung.html#task-1",
    "title": "Prepro 3: Exercise",
    "section": "Task 1",
    "text": "Task 1\nYou have a dataset, sensors_long.csv, with temperature values from three different sensors. Import it as a csv into R (as sensors_long).\nReformat the datetime column to POSIXct. Use the as.POSIXct function (read it in using?strftime()) to determine the specific format (the template).\n\n\nSample Solution\nlibrary(\"readr\")\n\nsensors_long <- read_delim(\"datasets/prepro/sensors_long.csv\", \",\")"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-2",
    "href": "prepro/Prepro3_Uebung.html#task-2",
    "title": "Prepro 3: Exercise",
    "section": "Task 2",
    "text": "Task 2\nGroup sensors_long according to the column name where the sensor information is contained, using the function group_by, and calculate the average temperature for each sensor (summarise). Note: Both functions are part of the dplyr package.\nThe output will look like this:\n\n\nSample Solution\nlibrary(\"dplyr\")\n\nsensors_long |>\n  group_by(name) |>\n  summarise(temp_mean = mean(value, na.rm = TRUE))\n## # A tibble: 3 × 2\n##   name    temp_mean\n##   <chr>       <dbl>\n## 1 sensor1      14.7\n## 2 sensor2      12.0\n## 3 sensor3      14.4"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-3",
    "href": "prepro/Prepro3_Uebung.html#task-3",
    "title": "Prepro 3: Exercise",
    "section": "Task 3",
    "text": "Task 3\nCreate a new convenience variable, month, for sensors_long (Tip: use the month function from lubridate). Now group by month and sensor and calculate the mean temperature.\n\n\nSample Solution\nlibrary(\"lubridate\")\n\nsensors_long |>\n  mutate(month = month(Datetime)) |>\n  group_by(month, name) |>\n  summarise(temp_mean = mean(value, na.rm = TRUE))\n## # A tibble: 6 × 3\n## # Groups:   month [2]\n##   month name    temp_mean\n##   <dbl> <chr>       <dbl>\n## 1    10 sensor1     14.7 \n## 2    10 sensor2     12.7 \n## 3    10 sensor3     14.4 \n## 4    11 sensor1    NaN   \n## 5    11 sensor2      8.87\n## 6    11 sensor3    NaN"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-4",
    "href": "prepro/Prepro3_Uebung.html#task-4",
    "title": "Prepro 3: Exercise",
    "section": "Task 4",
    "text": "Task 4\nNow import the weather.csv dataset (source MeteoSwiss) with the correct column types (time as POSIXct, tre200h0 as double). You can download the file from moodle if you havent done so yet.\n\n\nSample Solution\nweather <- read_delim(\"datasets/prepro/weather.csv\")\n\n\nweather$time2 <- weather$time |> \n  as.character() |> \n  as.POSIXct(format = \"%Y%m%d%H\", tz = \"UTC\")\n  \n\nweather$time <- weather$time2\nweather$time2 <- NULL"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-5",
    "href": "prepro/Prepro3_Uebung.html#task-5",
    "title": "Prepro 3: Exercise",
    "section": "Task 5",
    "text": "Task 5\nNow create a convenience variable for the calendar week for each measurement (lubridate::isoweek). Then calculate the average temperature value for each calendar week.\n\n\nSample Solution\nweather_summary <- weather |>\n  mutate(week = isoweek(time)) |>\n  group_by(week) |>\n  summarise(\n    temp_mean = mean(tre200h0, na.rm = TRUE)\n  )\n\n\nNext, you can visualise the result using the following function:\nplot(weather_summary$week, weather_summary$temp_mean, type = \"l\")"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#task-6",
    "href": "prepro/Prepro3_Uebung.html#task-6",
    "title": "Prepro 3: Exercise",
    "section": "Task 6",
    "text": "Task 6\nIn the previous task, we calculated the average temperature per calendar week over all years (2000 and 2001). However, if we want to compare the years with each other, we have to create the year as an additional convenience variable and group it accordingly. Try this with the weather data and then visualise the output.\n\n\nSample Solution\nweather_summary2 <- weather |>\n  mutate(\n    week = week(time),\n    year = year(time)\n    ) |>\n  group_by(year, week) |>\n  summarise(\n    temp_mean = mean(tre200h0, na.rm = TRUE)\n  )\n\n\n\n\nSample Solution\nplot(weather_summary2$week, weather_summary2$temp_mean, type = \"l\")\n\n\n\n\n\nFigure 9.1: Base plot does not like long tables and makes a continuous line out of the two years"
  },
  {
    "objectID": "InfoVis.html#infovis-1",
    "href": "InfoVis.html#infovis-1",
    "title": "Information Visualisation",
    "section": "InfoVis 1",
    "text": "InfoVis 1\nConventional inferential statistics are typically employed to confirm hypotheses. These hypotheses are derived from established theories and are then tested through experiments to determine whether they should be accepted or rejected. Conversely, Exploratory Data Analysis (EDA) takes an antagonistic approach, by first seeking out patterns and relationships within the data, which can subsequently inform the development of hypotheses for testing. This module presents the traditional five-step process of Exploratory Data Analysis as established by Tukey in 1980, culminating in a transition to its contemporary application through Visual Analytics."
  },
  {
    "objectID": "InfoVis.html#infovis-2",
    "href": "InfoVis.html#infovis-2",
    "title": "Information Visualisation",
    "section": "InfoVis 2",
    "text": "InfoVis 2\nInformation visualisation stands out as a flexible, powerful, and efficient tool for exploratory data analysis. Beyond the well-known scatter plots and histograms, there are innovative visualisation techniques like parallel coordinate plots, tree maps, and chord diagrams that provide unique perspectives for analysing increasingly large and complex datasets. In this lesson, students get to know a number of information visualisation types, learn to design them in a targeted manner and to create them themselves."
  },
  {
    "objectID": "infovis/Infovis0_Vorbereitung.html",
    "href": "infovis/Infovis0_Vorbereitung.html",
    "title": "Preparation",
    "section": "",
    "text": "As part of InfoVis 1 - 2, we will need several R packages. We recommend installing these before the first lesson. Individual packages are typically installed as follows:\n\ninstall.packages(c(\"dplyr\", \"ggplot2\", \"lubridate\", \"readr\", \"scales\", \"tidyr\"))\n\nHowever, this will re-install packages that you might have already installed. The package pacman helps with this: it checks if the package is already installed before downloading it:\n\n# install packman (this is necessary only once)\ninstall.packages(\"pacman\")\n\n\npacman::p_install(\n  \"dplyr\", \"ggplot2\", \"lubridate\", \"readr\", \"scales\", \"tidyr\",\n  force = FALSE,\n  character.only = TRUE\n  )\n\nYou can download the datasets for the exercises from Moodle:\n\n{{<var datasets.InfoVis1>}}\n{{<var datasets.InfoVis2>}}"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#base-plot-vs.-ggplot",
    "href": "infovis/Infovis1_Demo.html#base-plot-vs.-ggplot",
    "title": "Infovis 1: Demo A",
    "section": "Base-plot vs. ggplot",
    "text": "Base-plot vs. ggplot\nWe can create a scatterplot in “Base-R” to compare dates and temperatures as follows:\n\nplot(temperature$time, temperature$SHA, type = \"l\", col = \"red\")\nlines(temperature$time, temperature$ZER, col = \"blue\")\n\n\n\n\nIn ggplot, the approach is more nuanced. A plot begins with ggplot(). This command specifies the dataset (data =) and the variables within the dataset that influence the plot (mapping = aes()).\n\nlibrary(\"ggplot2\")\n\n\n# Dataset: \"temperature\" | Influencing variables: \"time\" and \"temp\"\nggplot(data = temperature, mapping = aes(time, SHA))\n\n\n\n\nIn ggplot, at least one “layer” is required to represent data, such as geom_point() for scatterplots, using the + operator. Unlike “piping” (|>), a layer is added with +.\n\nggplot(data = temperature, mapping = aes(time, SHA)) +\n  # Layer: \"geom_point\" corresponds to points in a scatterplot\n  geom_point()\n\n\n\n\nSince inputs are expected in the order of data = followed by mapping = in ggplot, we can omit these specifications.\n\nggplot(temperature, aes(time, SHA)) +\n  geom_point()"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "href": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "title": "Infovis 1: Demo A",
    "section": "Long vs. wide",
    "text": "Long vs. wide\nAs mentioned in PrePro 2, ggplot2 is designed for long tables. Therefore, we need to transform the wide table into a long format:\n\nlibrary(\"tidyr\")\n\n\ntemperature_long <- pivot_longer(temperature, -time, names_to = \"station\", values_to = \"temp\")\n\nTo colour-code different weather stations, we define variables that will influence the graphic, which are incorporated in the aes() function:\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_point()\n\n\n\n\nWe can also add additional layers with lines:\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_point() +\n  geom_line()"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#labels",
    "href": "infovis/Infovis1_Demo.html#labels",
    "title": "Infovis 1: Demo A",
    "section": "Labels",
    "text": "Labels\nNext, we’ll refine our plot by adding axis labels and a title. Additionally, we’ve chosen to remove the points (geom_point()) as they don’t align with my preferred visualisation style.\n\nggplot(temperature_long, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in degrees C°\",\n    title = \"Temperature Data Switzerland\",\n    subtitle = \"2001 to 2002\",\n    colour = \"Station\"\n  )"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#split-apply-combine",
    "href": "infovis/Infovis1_Demo.html#split-apply-combine",
    "title": "Infovis 1: Demo A",
    "section": "Split Apply Combine",
    "text": "Split Apply Combine\nIn our plot, the hourly data points are too detailed for a two-year visualisation. Using the Split Apply Combine technique (covered in PrePro 3), we can adjust the data resolution:\n\nlibrary(\"dplyr\")\n\n\ntemperature_day <- temperature_long |>\n  mutate(time = as.Date(time))\n\ntemperature_day\n\n# A tibble: 35,088 × 3\n   time       station  temp\n   <date>     <chr>   <dbl>\n 1 2000-01-01 SHA       0.2\n 2 2000-01-01 ZER      -8.8\n 3 2000-01-01 SHA       0.3\n 4 2000-01-01 ZER      -8.7\n 5 2000-01-01 SHA       0.3\n 6 2000-01-01 ZER      -9  \n 7 2000-01-01 SHA       0.3\n 8 2000-01-01 ZER      -8.7\n 9 2000-01-01 SHA       0.4\n10 2000-01-01 ZER      -8.5\n# ℹ 35,078 more rows\n\ntemperature_day <- temperature_day |>\n  group_by(station, time) |>\n  summarise(temp = mean(temp))\n\ntemperature_day\n\n# A tibble: 1,462 × 3\n# Groups:   station [2]\n   station time        temp\n   <chr>   <date>     <dbl>\n 1 SHA     2000-01-01  1.25\n 2 SHA     2000-01-02  1.73\n 3 SHA     2000-01-03  1.59\n 4 SHA     2000-01-04  1.78\n 5 SHA     2000-01-05  4.66\n 6 SHA     2000-01-06  3.49\n 7 SHA     2000-01-07  3.87\n 8 SHA     2000-01-08  3.28\n 9 SHA     2000-01-09  3.24\n10 SHA     2000-01-10  3.24\n# ℹ 1,452 more rows"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#adjusting-the-xy-axes",
    "href": "infovis/Infovis1_Demo.html#adjusting-the-xy-axes",
    "title": "Infovis 1: Demo A",
    "section": "Adjusting the X/Y Axes",
    "text": "Adjusting the X/Y Axes\nYou can also influence the x/y axes. You first have to determine what type of axis the plot has (in its default setting, ggplot automatically selects the axis type based on the nature of the data).\nFor our y-axis, which consists of numerical data, ggplot uses scale_y_continuous(). Other axis types can be found at ggplot2.tidyverse.org (scale_x_something or scale_y_something).\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in degrees C\",\n    title = \"Temperature Data Switzerland\",\n    subtitle = \"2001 to 2002\",\n    color = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) # determine y-axis section\n\n\n\n\nThis can also be done for the x-axis. Our x-axis consists of date information. ggplot calls this: scale_x_date().\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in degrees C\",\n    title = \"Temperature Data Switzerland\",\n    subtitle = \"2001 to 2002\",\n    color = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  )"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#customising-themes",
    "href": "infovis/Infovis1_Demo.html#customising-themes",
    "title": "Infovis 1: Demo A",
    "section": "Customising Themes",
    "text": "Customising Themes\nThe theme function in ggplot allows us to alter the general layout of plots. For instance, theme_classic() changes the plot’s style to a more traditional look, which is ideal for formal reports or publications. This theme can be applied either to individual plots or set as a default for all plots within a session.\nApplying to a single Plot:\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  theme_classic()\n\nGlobal setting (for all subsequent plots in the current session):\n\ntheme_set(theme_classic())"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "href": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "title": "Infovis 1: Demo A",
    "section": "Facets / Small Multiples",
    "text": "Facets / Small Multiples\nggplot also offers powerful functions for creating “Small multiples” using facet_wrap() (or facet_grid(), more on this later). These functions divide the main plot into smaller subplots based on a specified variable, denoted by the tilde symbol “~”.\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in °C\",\n    title = \"Temperature Data of Switzerland\",\n    subtitle = \"2001 to 2002\",\n    colour = \"Station\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~station)\n\n\n\n\nfacet_wrap can also be customised further, such as by setting the number of facets per row with ncol =.\nIn addition, since the station names are displayed above each facet, we no longer require the legend. This is achieved with theme(legend.position=\"none\").\n\nggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Time\",\n    y = \"Temperature in °C\",\n    title = \"Temperature Data of Switzerland\",\n    subtitle = \"2001 to 2002\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~station, ncol = 1) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#storing-and-exporting-plots",
    "href": "infovis/Infovis1_Demo.html#storing-and-exporting-plots",
    "title": "Infovis 1: Demo A",
    "section": "Storing and Exporting Plots",
    "text": "Storing and Exporting Plots\nLike data.frames and other objects, a complete ggplot plot can be stored in a variable. This is useful for exporting the plot (as PNG, JPG, etc.) or for progressively enhancing it, as shown in this example.\n\np <- ggplot(temperature_day, aes(time, temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\",\n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\"\n  ) +\n  scale_y_continuous(limits = c(-30, 30)) +\n  scale_x_date(\n    date_breaks = \"3 months\",\n    date_labels = \"%b\"\n  ) +\n  facet_wrap(~station, ncol = 1)\n# At this point, theme(legend.position=\"none\") was removed\n\nTo save the plot as a PNG file (without specifying “plot =”, the last plot is simply saved):\n\nggsave(filename = \"plot.png\", plot = p)\n\nTo add a layer or option to an existing plot stored in a variable:\n\np +\n  theme(legend.position = \"none\")\n\nAs is typical with R, the modification made to the plot is not automatically saved; it only shows the outcome of the change. To permanently incorporate this change into my plot stored in the variable, we need to overwrite the variable with the updated plot:\n\np <- p +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#smoothing",
    "href": "infovis/Infovis1_Demo.html#smoothing",
    "title": "Infovis 1: Demo A",
    "section": "Smoothing",
    "text": "Smoothing\nThe geom_smooth() function in ggplot can add trend lines to scatter plots. It is possible to select the underlying statistical method that is applied, yet by default, for datasets with fewer than 1,000 observations, ggplot defaults to using the stats::loess method. For larger datasets, it switches to mgcv::gam.\n\np <- p +\n  geom_smooth(colour = \"black\")\np"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-1",
    "href": "infovis/Infovis1_Uebung.html#task-1",
    "title": "Infovis 1: Exercise",
    "section": "Task 1",
    "text": "Task 1\nYour first task is to recreate the following plot from Kovic (2014) using ggplot and the tagi_data_kanton.csv dataset:\nHere’s are some tips to get you started:\n\nCreate a ggplot object with ggplot(canton, aes(auslanderanteil, ja_anteil)), then add a point layer with geom_point().\nUse coord_fixed() to set a fixed ratio (1:1) between the axes.\nOptionally, you can:\n\nSet the axis limits with scale_x_continuous (or scale_y_continuous).\nManually set the breaks (0.0, 0.1 0.3 etc) within scale_x_continuous (or scale_y_continuous)\nUse labs() to label the axes."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-2",
    "href": "infovis/Infovis1_Uebung.html#task-2",
    "title": "Infovis 1: Exercise",
    "section": "Task 2",
    "text": "Task 2\nNext, replicate the following plot from Kovic (2014) using ggplot:\nHere’s a tip:\n\nUse geom_smooth."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-3",
    "href": "infovis/Infovis1_Uebung.html#task-3",
    "title": "Infovis 1: Exercise",
    "section": "Task 3",
    "text": "Task 3\nNow, let’s import the municipal data tagi_data_gemeinden.csv.\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere are some tips:\n\nUse geom_point().\nUse labs().\nUse coord_fixed()."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-4",
    "href": "infovis/Infovis1_Uebung.html#task-4",
    "title": "Infovis 1: Exercise",
    "section": "Task 4",
    "text": "Task 4\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse geom_smooth."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-5",
    "href": "infovis/Infovis1_Uebung.html#task-5",
    "title": "Infovis 1: Exercise",
    "section": "Task 5",
    "text": "Task 5\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse facet_wrap to display a separate plot for each canton."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-6",
    "href": "infovis/Infovis1_Uebung.html#task-6",
    "title": "Infovis 1: Exercise",
    "section": "Task 6",
    "text": "Task 6\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse geom_smooth."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-7",
    "href": "infovis/Infovis1_Uebung.html#task-7",
    "title": "Infovis 1: Exercise",
    "section": "Task 7",
    "text": "Task 7\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse facet_wrap"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#task-8",
    "href": "infovis/Infovis1_Uebung.html#task-8",
    "title": "Infovis 1: Exercise",
    "section": "Task 8",
    "text": "Task 8\nReplicate the following plot from Kovic (2014) using ggplot and the tagi_data_gemeinden.csv dataset:\nHere’s a tip:\n\nUse geom_smooth.\n\n\n\n\n\n\n\n\n\n\nKovic, Marko. 2014. “Je Weniger Ausländer, Desto Mehr Ja-Stimmen? Wirklich?” Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich."
  },
  {
    "objectID": "infovis/Infovis1_Script_eda.html",
    "href": "infovis/Infovis1_Script_eda.html",
    "title": "Infovis 1: EDA Script",
    "section": "",
    "text": "library(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"scales\")\n\n# create some data about age and height of people\npeople <- data.frame(\n  ID = c(1:30),\n  age = c(\n    5.0, 7.0, 6.5, 9.0, 8.0, 5.0, 8.6, 7.5, 9.0, 6.0,\n    63.5, 65.7, 57.6, 98.6, 76.5, 78.0, 93.4, 77.5, 256.6, 512.3,\n    15.5, 18.6, 18.5, 22.8, 28.5, 39.5, 55.9, 50.3, 31.9, 41.3\n  ),\n  height = c(\n    0.85, 0.93, 1.1, 1.25, 1.33, 1.17, 1.32, 0.82, 0.89, 1.13,\n    1.62, 1.87, 1.67, 1.76, 1.56, 1.71, 1.65, 1.55, 1.87, 1.69,\n    1.49, 1.68, 1.41, 1.55, 1.84, 1.69, 0.85, 1.65, 1.94, 1.80\n  ),\n  weight = c(\n    45.5, 54.3, 76.5, 60.4, 43.4, 36.4, 50.3, 27.8, 34.7, 47.6,\n    84.3, 90.4, 76.5, 55.6, 54.3, 83.2, 80.7, 55.6, 87.6, 69.5,\n    48.0, 55.6, 47.6, 60.5, 54.3, 59.5, 34.5, 55.4, 100.4, 110.3\n  )\n)\n\n# build a scatterplot for a first inspection\nggplot(people, aes(x = age, y = height)) +\n  geom_point()\n\n\n\n\n\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0.75, 2))\n\n\n\n# Go to help page: http://docs.ggplot2.org/current/ -> Search for icon of fit-line\n# http://docs.ggplot2.org/current/geom_smooth.html\n\n\n# build a scatterplot for a first inspection, with regression line\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n# stem and leaf plot\nstem(people$height)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 25593\n##   10 | 037\n##   12 | 523\n##   14 | 19556\n##   16 | 255789916\n##   18 | 04774\nstem(people$height, scale = 2)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 2559\n##    9 | 3\n##   10 | \n##   11 | 037\n##   12 | 5\n##   13 | 23\n##   14 | 19\n##   15 | 556\n##   16 | 2557899\n##   17 | 16\n##   18 | 0477\n##   19 | 4\n\n\n# explore the two variables with box-whiskerplots\nsummary(people$age)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    5.00    8.70   30.20   59.14   65.15  512.30\nboxplot(people$age)\n\n\n\n\n\nsummary(people$height)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.820   1.190   1.555   1.455   1.690   1.940\nboxplot(people$height)\n\n\n\n\n\n# explore data with a histogram\nggplot(people, aes(x = age)) +\n  geom_histogram(binwidth = 20)\n\n\n\n\n\ndensity(x = people$height)\n## \n## Call:\n##  density.default(x = people$height)\n## \n## Data: people$height (30 obs.);   Bandwidth 'bw' = 0.1576\n## \n##        x                y           \n##  Min.   :0.3472   Min.   :0.001593  \n##  1st Qu.:0.8636   1st Qu.:0.102953  \n##  Median :1.3800   Median :0.510601  \n##  Mean   :1.3800   Mean   :0.483553  \n##  3rd Qu.:1.8964   3rd Qu.:0.722660  \n##  Max.   :2.4128   Max.   :1.216350\n\n# re-expression: use log or sqrt axes\n#\n# Find here guideline about scaling axes\n# http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/\n# http://docs.ggplot2.org/0.9.3.1/scale_continuous.html\n\n\n# logarithmic axis: respond to skewness in the data, e.g. log10\nggplot(people, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth() +\n  scale_x_log10()\n\n\n\n\n\n# outliers: Remove very small and very old people\n\npeopleClean <- people |>\n  filter(ID != 27) |> # This person was too short.\n  filter(age < 100) # Error in age recorded.\n\n\nggplot(peopleClean, aes(x = age)) +\n  geom_histogram(binwidth = 10)\n\n\n\n\n\nggplot(peopleClean, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n# with custom binwidth\nggplot(peopleClean, aes(x = age)) +\n  geom_histogram(binwidth = 10) +\n  theme_bw() # specifying the theme\n\n\n\n\n\n# quadratic axis\nggplot(peopleClean, aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5) +\n  scale_x_sqrt()\n\n\n\n\n\n# filter \"teenies\": No trend\nfilter(peopleClean, age < 15) |>\n  ggplot(aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5)\n\n\n\n\n\n# filter \"teens\": No trend\npeopleClean |>\n  filter(age > 55) |>\n  ggplot(aes(x = age, y = height)) +\n  geom_point() +\n  scale_y_continuous(limits = c(0, 2.0)) +\n  geom_smooth(method = \"lm\", fill = \"lightblue\", size = 0.5, alpha = 0.5)\n\n\n\n\n\n# Onwards towards multidimensional data\n\n# Finally, make a scatterplot matrix\npairs(peopleClean[, 2:4], panel = panel.smooth)\n\n\n\n\n\npairs(peopleClean[, 2:4], panel = panel.smooth)"
  },
  {
    "objectID": "Statistic.html",
    "href": "Statistic.html",
    "title": "Statistic",
    "section": "",
    "text": "Write Intro\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nLesson\n\n\nTopic\n\n\n\n\n\n\nStatistics 1: The Basics of Statistics\n\n\n2024-04-02\n\n\nstat1\n\n\nStatistic 1\n\n\n\n\nStatistics 2: Advanced Topics: Inductive and Multivariate Statistics\n\n\n2024-04-09\n\n\nstat2\n\n\nStatistic 2\n\n\n\n\nStatistics 3: Cluster analysis and data classification approaches\n\n\n2024-04-16\n\n\nstat3\n\n\nStatistic 3\n\n\n\n\nPreparation\n\n\n2025-04-01\n\n\nstat1\n\n\nPreparation\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "statistic/Stat0_Vorbereitung.html",
    "href": "statistic/Stat0_Vorbereitung.html",
    "title": "Preparation",
    "section": "",
    "text": "As part of Statistic 1 - 3, we will need some R packages. We recommend installing them before the first lesson. Similar to the preparation exercise in Prepro1 you can use the code below to automatically install all packages that have not yet been installed.\n\nipak <- function(pkg) {\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) {\n    install.packages(new.pkg, dependencies = TRUE)\n  }\n}\n\npackages <- c('dplyr', 'wooldridge')\n\nipak(packages)"
  },
  {
    "objectID": "statistic/Stat1_Uebung.html#exercise-i-descriptive-statistics",
    "href": "statistic/Stat1_Uebung.html#exercise-i-descriptive-statistics",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "Exercise I: Descriptive Statistics",
    "text": "Exercise I: Descriptive Statistics"
  },
  {
    "objectID": "statistic/Stat1_Uebung.html#exercise-1",
    "href": "statistic/Stat1_Uebung.html#exercise-1",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "Exercise 1",
    "text": "Exercise 1\nUse the data in WAGE1.RAW for this exercise.\n\nFind the average education level in the sample. What are the lowest and highest years of education?\nFind the average hourly wage in the sample. Does it seem high or low?\n\nThe wage data are reported in 1976 dollars. Using the Economic Report of the President (2011 or later), obtain and report the Consumer Price Index (CPI) for the years 1976 and 2010.\n\nUse the CPI values from 3) to find the average hourly wage in 2010 dollars. Now does the average hourly wage seem reasonable?\n\nHow many women are in the sample? How many non-women?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('wage1')\n?wage1\n\n#1)\nmean(wage1$educ)\nmin(wage1$educ)\nmax(wage1$educ)\n\n#2)\nmean(wage1$wage)\n#This seems low\n\n#3)\n#page 259 of the report\n#CPI in 1976 is 56.9\n#CPI in 2010 is 218.056\n\n#4)\nmean(wage1$wage)*218.056/56.9\n\n#5)\nnrow(wage1[wage1$female == 1, ])\nnrow(wage1[wage1$female == 0, ])"
  },
  {
    "objectID": "statistic/Stat1_Uebung.html#exercise-2",
    "href": "statistic/Stat1_Uebung.html#exercise-2",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "Exercise 2",
    "text": "Exercise 2\nUse the data in BWGHT.RAW to answer this question.\n\nHow many women are in the sample, and how many report smoking during pregnancy?\n\nWhat is the average number of cigarettes smoked per day? Is the average a good measure of the “typical” woman in this case? Explain.\n\nAmong women who smoked during pregnancy, what is the average number of cigarettes smoked per day? How does this compare with your answer from 2), and why?\n\nFind the average of fatheduc in the sample. Why are only 1,192 observations used to compute this average?\n\nReport the average family income and its standard deviation in dollars.\n\n\n\nSample Solution\nlibrary('wooldridge')\nlibrary('dplyr')\ndata('bwght')\n?bwght\n\n#1)\nnrow(bwght[is.na(bwght$mothereduc), ])\n#there is no missing information so we have 1388 women\n1388-nrow(bwght[(bwght$cigs==0), ])\n\n#2)\nmean(bwght$cigs)\n#No, because there is a lot of non-smokers\n\n#3)\nbwght %>% filter(cigs>0) %>% summarise(mean(cigs))\n\n#4)\nmean(bwght$fatheduc, na.rm = TRUE)\n\n#5)\nmean(bwght$faminc)\nsd(bwght$faminc)"
  },
  {
    "objectID": "statistic/Stat1_Uebung.html#exercise-3",
    "href": "statistic/Stat1_Uebung.html#exercise-3",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "Exercise 3",
    "text": "Exercise 3\nThe data in MEAP01.RAW are for the state of Michigan in the year 2001. Use these data to answer the following questions.\n\nFind the largest and smallest values of math4. Does the range make sense? Explain.\n\nHow many schools have a perfect pass rate on the math test? What percentage is this of the total sample?\n\nHow many schools have math pass rates of exactly 50%?\n\nCompare the average pass rates for the math and reading scores. Which test is harder to pass?\n\nFind the correlation between math4 and read4. What do you conclude?\n\nThe variable exppp is expenditure per pupil. Find the average of exppp along with its standard deviation. Would you say there is wide variation in per pupil spending?\n\nSuppose School A spends $6,000 per student and School B spends $5,500 per student. By what percentage does School A’s spending exceed School B’s?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('meap01')\n?meap01\n\n#1)\nmin(meap01$math4)\nmax(meap01$math4)\n\n#2)\nnrow(meap01[(meap01$math4==100), ])\nnrow(meap01[(meap01$math4==100), ])/1823*100\n\n#3)\nnrow(meap01[(meap01$math4==50), ])\n\n#4)\nmean(meap01$math4)\nmean(meap01$read4)\n\n#5)\ncorr=lm(math4 ~ read4, data = meap01)\nsummary(corr)\n\n#6)\nmean(meap01$exppp)\nsd(meap01$exppp)\n\n#7)\n(6000-5500)/5500*100"
  },
  {
    "objectID": "statistic/Stat1_Uebung.html#exercise-4",
    "href": "statistic/Stat1_Uebung.html#exercise-4",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "Exercise 4",
    "text": "Exercise 4\nThe data in JTRAIN2.RAW come from a job training experiment conducted for low-income men during 1976–1977.\n\nUse the indicator variable train to determine the fraction of men receiving job training.\n\nThe variable re78 is earnings from 1978, measured in thousands of 1982 dollars. Find the averages of re78 for the sample of men receiving job training and the sample not receiving job training. Is the difference economically large?\n\nThe variable unem78 is an indicator of whether a man is unemployed or not in 1978. What fraction of the men who received job training are unemployed? What about for men who did not receive job training? Comment on the difference.\n\nFrom questions 2 and 3, does it appear that the job training program was effective? What would make our conclusions more convincing?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('jtrain2')\n?jtrain2\n\n#1)\nnrow(jtrain2[(jtrain2$train==1), ])/445\n\n#2)\njtrain2 %>% filter(train==0) %>% summarise(mean(re78))\njtrain2 %>% filter(train==1) %>% summarise(mean(re78))\n\n#3)\n(nrow(jtrain2[(jtrain2$train==1), ]) - (jtrain2 %>% filter(train==1) %>% summarise(sum(unem78))))/nrow(jtrain2[(jtrain2$train==1), ])\n\n(nrow(jtrain2[(jtrain2$train==0), ]) - (jtrain2 %>% filter(train==0) %>% summarise(sum(unem78))))/nrow(jtrain2[(jtrain2$train==0), ])\n\n#4)\n#No\n#To make a linear regression to find if there is evidence on the fact that the training does not have any effects on the employement rate"
  },
  {
    "objectID": "statistic/Stat1_Uebung.html#exercise-5",
    "href": "statistic/Stat1_Uebung.html#exercise-5",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "Exercise 5",
    "text": "Exercise 5\nThe data in FERTIL2.DTA were collected on women living in the Republic of Botswana in 1988. The variable children refers to the number of living children. The variable electric is a binary indicator equal to one if the woman’s home has electricity, and zero if not.\n\nFind the smallest and largest values of children in the sample. What is the average of children?\n\nWhat percentage of women have electricity in the home?\n\nCompute the average of children for those without electricity and do the same for those with electricity. Comment on what you find.\n\nFrom question 3), can you infer that having electricity “causes” women to have fewer children? Explain.\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('fertil2')\n?fertil2\n\n#1)\nmin(fertil2$children)\nmax(fertil2$children)\nmean(fertil2$children)\n\n#2)\nnrow(fertil2[(fertil2$electric==1), ])/4361*100\n\n#3)\nfertil2 %>% filter(electric==0) %>% summarise(mean(children))\nfertil2 %>% filter(electric==1) %>% summarise(mean(children))\n\n#4)\n#No"
  },
  {
    "objectID": "statistic/Stat1_Uebung.html#exercise-ii-the-simple-linear-regression-model",
    "href": "statistic/Stat1_Uebung.html#exercise-ii-the-simple-linear-regression-model",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "Exercise II: The Simple Linear Regression Model",
    "text": "Exercise II: The Simple Linear Regression Model"
  },
  {
    "objectID": "statistic/Stat1_Uebung.html#exercise-6",
    "href": "statistic/Stat1_Uebung.html#exercise-6",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "Exercise 6",
    "text": "Exercise 6\nThe data in 401K.RAW are a subset of data analyzed by Papke (1995) to study the relationship between participation in a 401(k) pension plan and the generosity of the plan. The variable prate is the percentage of eligible workers with an active account; this is the variable we would like to explain. The measure of generosity is the plan match rate, mrate. This variable gives the average amount the firm contributes to each worker’s plan for each $1 contribution by the worker. For example, if mrate = 0.50, then a $1 contribution by the worker is matched by a 50¢ contribution by the firm.\n\nFind the average participation rate and the average match rate in the sample of plans.\nNow, estimate the simple regression equation \\(\\widehat{prate} = \\hat{\\beta_0} + \\hat{\\beta_1}mrate,\\) and report the results along with the sample size and R-squared.\nInterpret the intercept in your equation. Interpret the coefficient on mrate.\nFind the predicted prate when mrate = 3.5. Is this a reasonable prediction? Explain what is happening here.\nHow much of the variation in prate is explained by mrate? Is this a lot in your opinion?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('k401k')\n?k401k\n\n#1)\nmean(k401k$prate)\nmean(k401k$mrate)\n\n#2)\nLinMod <- lm(prate ~ mrate, data=k401k)\nsummary(LinMod)\n\n#3)\n#Intercept is 83.0755\n#if mrate increase by 1% then prate increases by 5.86%\n\n#4)\n#No\n\n#5)\n#R-squared=0.075. No"
  },
  {
    "objectID": "statistic/Stat1_Uebung.html#exercise-7",
    "href": "statistic/Stat1_Uebung.html#exercise-7",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "Exercise 7",
    "text": "Exercise 7\nThe data set in CEOSAL2.RAW contains information on chief executive officers for U.S. corporations. The variable salary is annual compensation, in thousands of dollars, and ceoten is prior number of years as company CEO.\n\nFind the average salary and the average tenure in the sample.\nHow many CEOs are in their first year as CEO (that is, ceoten = 0)? What is the longest tenure as a CEO?\nEstimate the simple regression model \\(log(salary) = \\beta_0 + \\beta_1ceoten + u,\\) and report your results in the usual form. What is the (approximate) predicted percentage increase in salary given one more year as a CEO?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('ceosal2')\n?ceosal2\n\n#1)\nmean(ceosal2$salary)\nmean(ceosal2$ceoten)\n\n#2)\nmax(ceosal2$ceoten)\nnrow(ceosal2[(ceosal2$ceoten==0), ])\n\n#3)\nLinMod <- lm(log(salary) ~ ceoten, data=ceosal2)\nsummary(LinMod)"
  },
  {
    "objectID": "statistic/Stat1_Uebung.html#exercise-8",
    "href": "statistic/Stat1_Uebung.html#exercise-8",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "Exercise 8",
    "text": "Exercise 8\nUse the data in SLEEP75.RAW from Biddle and Hamermesh (1990) to study whether there is a tradeoff between the time spent sleeping per week and the time spent in paid work. We could use either variable as the dependent variable. For concreteness, estimate the model \\(sleep = \\beta_0 + \\beta_1totwrk + u,\\) where sleep is minutes spent sleeping at night per week and totwrk is total minutes worked during the week.\n\nReport your results in equation form along with the number of observations and R-squared. What does the intercept in this equation mean?\nIf totwrk increases by 2 hours, by how much is sleep estimated to fall? Do you find this to be a large effect?\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('sleep75')\n?sleep75\n\n#1)\nLinMod <- lm(sleep ~ totwrk, data=sleep75)\nsummary(LinMod)\n\n#2)\n#120*0.15075=18.09 minutes"
  },
  {
    "objectID": "statistic/Stat1_Uebung.html#exercise-9",
    "href": "statistic/Stat1_Uebung.html#exercise-9",
    "title": "Statistics 1: The Basics of Statistics",
    "section": "Exercise 9",
    "text": "Exercise 9\nFor the population of firms in the chemical industry, let rd denote annual expenditures on research and development, and let sales denote annual sales (both are in millions of dollars).\n\nWrite down a model (not an estimated equation) that implies a constant elasticity between rd and sales. Which parameter is the elasticity?\nNow, estimate the model using the data in RDCHEM.RAW. Write out the estimated equation in the usual form. What is the estimated elasticity of rd with respect to sales? Explain in words what this elasticity means.\n\n\n\nSample Solution\nlibrary('wooldridge')\ndata('rdchem')\n?rdchem\n\n#1) Constant elasticity -> log ~ log regression, the elasticity is the slope parameter\n\n#2)\nLinMod <- lm(log(rd) ~ log(sales), data=rdchem)\nsummary(LinMod)\n#when sales increase by 1%, R&D increase by 1.08%"
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-i-estimation-of-multiple-regression-analysis",
    "href": "statistic/Stat2_Uebung.html#exercise-i-estimation-of-multiple-regression-analysis",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise I: Estimation of Multiple Regression Analysis",
    "text": "Exercise I: Estimation of Multiple Regression Analysis"
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-1",
    "href": "statistic/Stat2_Uebung.html#exercise-1",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise 1",
    "text": "Exercise 1\nA problem of interest to health officials (and others) is to determine the effects of smoking during pregnancy on infant health. One measure of infant health is birth weight; a birth weight that is too low can put an infant at risk for contracting various illnesses. Since factors other than cigarette smoking that affect birth weight are likely to be correlated with smoking, we should take those factors into account. For example, higher income generally results in access to better prenatal care, as well as better nutrition forthe mother. An equation that recognizes this is \\(bwght = \\beta_0 + \\beta_1cigs + \\beta_2 faminc + u.\\)\n\nWhat is the most likely sign for \\beta_2?\nDo you think cigs and faminc are likely to be correlated? Explain why the correlation might be positive or negative.\nNow, estimate the equation with and without faminc, using the data in BWGHT.RAW. Report the results in equation form, including the sample size and R-squared. Discuss your results, focusing on whether adding faminc substantially changes the estimated effect of cigs on bwght."
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-2",
    "href": "statistic/Stat2_Uebung.html#exercise-2",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise 2",
    "text": "Exercise 2\nUse the data in HPRICE1.RAW to estimate the model \\(price = \\beta_0 + \\beta_1sqrft + \\beta_2bdrms + u,\\) where price is the house price measured in thousands of dollars.\n\nWrite out the results in equation form.\nWhat is the estimated increase in price for a house with one more bedroom, holding square footage constant?\nWhat is the estimated increase in price for a house with an additional bedroom that is 140 square feet in size? Compare this to your answer in question 2.\nWhat percentage of the variation in price is explained by square footage and number of bedrooms?\nThe first house in the sample has sqrft = 2,438 and bdrms = 4. Find the predicted selling price for this house from the OLS regression line.\nThe actual selling price of the first house in the sample was $300,000 (so price = 300). Find the residual for this house. Does it suggest that the buyer underpaid or overpaid for the house?"
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-3",
    "href": "statistic/Stat2_Uebung.html#exercise-3",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise 3",
    "text": "Exercise 3\nThe file CEOSAL2.RAW contains data on 177 chief executive officers and can be used to examine the effects of firm performance on CEO salary.\n\nEstimate a model relating annual salary to firm sales and market value. Make the model of the constant elasticity variety for both independent variables. Write the results out in equation form.\nAdd profits to the model from question 1. Why can this variable not be included in logarithmic form? Would you say that these firm performance variables explain most of the variation in CEO salaries?\nAdd the variable ceoten to the model in question 2. What is the estimated percentage return for another year of CEO tenure, holding other factors fixed?\nFind the sample correlation coefficient between the variables log(mktval) and profits. Are these variables highly correlated?"
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-ii-multiple-regression-analysis-inference",
    "href": "statistic/Stat2_Uebung.html#exercise-ii-multiple-regression-analysis-inference",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise II: Multiple Regression Analysis: Inference",
    "text": "Exercise II: Multiple Regression Analysis: Inference"
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-4",
    "href": "statistic/Stat2_Uebung.html#exercise-4",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise 4",
    "text": "Exercise 4\nThe following model can be used to study whether campaign expenditures affect election outcomes: \\(voteA = \\beta_0 + \\beta_1log(expendA) + \\beta_2log(expendB) + \\beta_3 prtystrA + u,\\) where voteA is the percentage of the vote received by Candidate A, expendA and expendB are campaign expenditures by Candidates A and B, and prtystrA is a measure of party strength for Candidate A (the percentage of the most recent presidential vote that went to A’s party).\n\nWhat is the interpretation of \\(\\beta_1\\)?\nIn terms of the parameters, state the null hypothesis that a 1% increase in A’s expenditures is offset by a 1% increase in B’s expenditures.\nEstimate the given model using the data in VOTE1.RAW and report the results in usual form. Do A’s expenditures affect the outcome? What about B’s expenditures? Can you use these results to test the hypothesis in question 2?\nEstimate a model that directly gives the t statistic for testing the hypothesis in question 2. What do you conclude? (Use a two-sided alternative knowing that the 10% critical value against a two-side alternative with 169 df is 1.645)"
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-5",
    "href": "statistic/Stat2_Uebung.html#exercise-5",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise 5",
    "text": "Exercise 5\nUse the data in WAGE2.RAW for this exercise.\n\nConsider the standard wage equation \\(log(wage) = \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 tenure + u\\). State the null hypothesis that another year of general workforce experience has no effect on log(wage).\nTest the null hypothesis in question 1. against a two-sided alternative, at the 5% significance level. What do you conclude?\nState the null hypothesis that another year of general workforce experience has the same effect on log(wage) as another year of tenure with the current employer.\nTest the null hypothesis in question 3. against a two-sided alternative, at the 5% significance level."
  },
  {
    "objectID": "statistic/Stat2_Uebung.html#exercise-6",
    "href": "statistic/Stat2_Uebung.html#exercise-6",
    "title": "Statistics 2: Advanced Topics: Inductive and Multivariate Statistics",
    "section": "Exercise 6",
    "text": "Exercise 6\nThe data set 401KSUBS.RAW contains information on net financial wealth nettfa, age of the survey respondent age, annual family income inc, family size fsize, and participation in certain pension plans for people in the United States. The wealth and income variables are both recorded in thousands of dollars. For this question, use only the data for single-person households (so fsize = 1).\n\nHow many single-person households are there in the data set?\nUse OLS to estimate the model \\(nettfa = \\beta_0 + \\beta_1inc + \\beta_2age + u,\\) and report the results using the usual format. Be sure to use only the single-person households in the sample. Interpret the slope coefficients. Are there any surprises in the slope estimates?\nDoes the intercept from the regression in question 2 have an interesting meaning? Explain.\nFind the p-value for the test \\(H_0: \\beta_2 = 0\\) against a two-side alternative. Do you reject \\(H_0\\) at the 1% significance level?\nIf you do a simple regression of nettfa on inc, is the estimated coefficient on inc much different from the estimate in question 2? Why or why not?"
  },
  {
    "objectID": "statistic/Stat3_Uebung.html#k-means-clustering",
    "href": "statistic/Stat3_Uebung.html#k-means-clustering",
    "title": "Statistics 3: Cluster analysis and data classification approaches",
    "section": "K-means Clustering",
    "text": "K-means Clustering\n\ninstall.packages(\"factoextra\")\ninstall.packages(\"mclust\")\nlibrary(factoextra)\n\nConsider the \\(iris\\) dataset. Suppose we are given the petal and sepal length and widths, but not told which species each iris belongs to.\nOur goal is to find clusters within the data. These are groups of irises that are more similar to each other than to those in other groups (clusters). These clusters may correspond to the Species label (which we aren’t given), or they may not. The goal of cluster analysis is not to predict the species, but simply to group the data into similar clusters.\nLet’s look at finding 3 clusters. We can do this using the kmeans command in R.\n\niris2 <- iris[,1:4]\n# nstart gives the number of random initialisations to try \nset.seed(123)\n(iris.k <- kmeans(iris2, centers = 3, nstart=25))\n\nFrom this output we can read off the final cluster means. Also given is the final within-cluster sum of squares for each cluster.\nWe can visualise the output of K-means using the fviz_cluster command from the factoextra package. This first projects the points into two dimensions using PCA, and then shows the classification in 2D, and so some caution is needed in interpreting these plots.\n\nfviz_cluster(iris.k, data = iris2,\n             geom = \"point\")\n\nFinally, in this case we know that there really are three clusters in the data (the three species). We can compare the clusters found using K-means with the species label to see if they are similar. The easiest way to do this is with the table command.\n\ntable(iris$Species, iris.k$cluster)\n\nIn the iris data, we know there are 3 distinct species. But suppose we didn’t know this. What happens if we try other values for \\(K\\)?\nFor the iris data, we can create an elbow plot using the fviz_nbclust command from the factoextra package.\n\nfviz_nbclust(iris2, kmeans, method = \"wss\")\n\nIn this case, we would probably decide there most likely three natural clusters in the data, as there is a reasonable decrease in W when moving from 2 to 3 clusters, but moving to 3 clusters only yields a minor improvement. Note here the slight increase in W in moving from 9 to 10 clusters. This is due to only using a greed search, rather than an exhaustive one (we know the best 10-group cluster must be better than the best 9-group cluster, we just have found it)."
  },
  {
    "objectID": "statistic/Stat3_Uebung.html#hierarchical-clustering",
    "href": "statistic/Stat3_Uebung.html#hierarchical-clustering",
    "title": "Statistics 3: Cluster analysis and data classification approaches",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nSuppose we are given 5 observations with distance matrix\n\n(D <- as.dist(matrix(c(0,0,0,0,0,\n                      2,0,0,0,0,\n                      11,9,0,0,0,\n                      15,13,10,0,0,\n                      7,5,4,8,0), nr=5, byrow=T)))\n\nThe hclust command does agglomerative clustering: we just have to specify the method to use.\n\nD.sl <-hclust(D, method=\"single\")\n\nTo display the dendrogram\n\nplot(D.sl)\n\nChanging the method\n\nplot(hclust(D, method=\"complete\"))\n\nGroup average clustering produces the same hierarchy of clusters as single linkage, but the nodes (points where clusters join) are at different heights in the dendrogram.\n\nD.ga <- hclust(D, method=\"average\")\nplot(D.ga)"
  },
  {
    "objectID": "statistic/Stat3_Uebung.html#model-based-clustering",
    "href": "statistic/Stat3_Uebung.html#model-based-clustering",
    "title": "Statistics 3: Cluster analysis and data classification approaches",
    "section": "Model-Based Clustering",
    "text": "Model-Based Clustering\nModel-based clustering is similar to K-means clustering, in that we want to allocate each case to a cluster. The difference is that we will now assume a probability distribution for the observations within each cluster.\nThe mclust library can be used to perform model-based clustering with Gaussian clusters. We just have to specify the number of required clusters.\n\nlibrary(mclust)\niris.m <- Mclust(iris2,G=3)\n\nPairs plots of the classification of each point can easily be obtained, as can the estimated probability density of each cluster.\n\nplot(iris.m, what = c(\"classification\"))\n\nChanging the representation\n\nplot(iris.m, what = c(\"density\"))"
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "Kovic, Marko. 2014. “Je Weniger Ausländer, Desto Mehr Ja-Stimmen?\nWirklich?” Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualize, and Model\nData. 2nd Edition. O’Reilly. https://r4ds.hadley.nz/."
  }
]